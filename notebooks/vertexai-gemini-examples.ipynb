{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2s1IpH6OGZQ"
      },
      "source": [
        "# Haystack üíô Google Gemini\n",
        "\n",
        "*by Tuana Celik: [Twitter](https://twitter.com/tuanacelik), [LinkedIn](https://www.linkedin.com/in/tuanacelik/), Tilde Thurium: [Twitter](https://twitter.com/annthurium), [LinkedIn](https://www.linkedin.com/in/annthurium/) and Silvano Cerza: [LinkedIn](https://www.linkedin.com/in/silvanocerza/)*\n",
        "\n",
        "**üìö Check out the [Gemini Models with Google Vertex AI Integration for Haystack](https://haystack.deepset.ai/blog/gemini-models-with-google-vertex-for-haystack) article for a detailed run through of this example.**\n",
        "\n",
        "This is a notebook showing how you can use Gemini with Haystack.\n",
        "\n",
        "Gemini is Google's newest model. You can read more about its capabilities [here](https://deepmind.google/technologies/gemini/#capabilities).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XromVwB1nQ76"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "As a prerequisite, you need to have a Google Cloud Project set up that has access to Gemini. Following that, you'll only need to authenticate yourself in this Colab.\n",
        "\n",
        "First thing first we need to install our dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxGffegfOGZR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade haystack-ai google-genai-haystack trafilatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHDiEzI2OGZU"
      },
      "source": [
        "To use Gemini you need to have a Google Cloud Platform account and be logged in using Application Default Credentials (ADCs). For more info see the [official documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc).\n",
        "\n",
        "Time to login!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKvKRuRXOGZU"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uw5F0M_OGZT"
      },
      "source": [
        "Remember to set the `project_id` variable to a valid project ID that you have enough authorization to use for Gemini.\n",
        "We're going to use this one throughout the example!\n",
        "\n",
        "To find your project ID you can find it in the [GCP resource manager](https://console.cloud.google.com/cloud-resource-manager) or locally by running `gcloud projects list` in your terminal. For more info on the gcloud CLI see the [official documentation](https://cloud.google.com/cli)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzA_x7iFOGZT"
      },
      "outputs": [],
      "source": [
        "project_id = input(\"Enter your project ID:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08TA9zAQlqy6"
      },
      "source": [
        "## Use `gemini-2.0-flash`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ynAXT5mI1s"
      },
      "source": [
        "### Answer Questions\n",
        "\n",
        "Now that we setup everything we can create an instance of our `GoogleGenAIChatGenerator`. This component supports both Gemini and Vertex AI. For this demo, we will set `api=\"vertex\"`, and pass our project_id as vertex_ai_project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Ql3qSlOGZV"
      },
      "outputs": [],
      "source": [
        "from haystack_integrations.components.generators.googlegenai import GoogleGenAIChatGenerator\n",
        "\n",
        "gemini = GoogleGenAIChatGenerator(model=\"gemini-2.0-flash\", api=\"vertex\", vertex_ai_project=project_id, vertex_ai_location=\"europe-west1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qRBY_hGOGZV"
      },
      "source": [
        "Let's start by asking something simple.\n",
        "\n",
        "This component expects a list of `ChatMessage` as input to the `run()` method. You can pass text or function calls through the messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbqFt4IiOGZV"
      },
      "outputs": [],
      "source": [
        "from haystack.dataclasses import ChatMessage\n",
        "messages = [ChatMessage.from_user(\"What is the most interesting thing you know?\")]\n",
        "result = gemini.run(messages = messages)\n",
        "for answer in result[\"replies\"]:\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLIIQ4PZmX-H"
      },
      "source": [
        "## Function Calling with `gemini-2.0-flash`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCiSz840mfME"
      },
      "source": [
        "\n",
        "With `gemini-2.0-flash`, we can also use function calling!\n",
        "So let's see how we can do that üëá\n",
        "\n",
        "Let's see if we can build a system that can run a `get_current_weather` function, based on a question asked in natural language.\n",
        "\n",
        "First we create our function definition and tool (learn more about [Tools](https://docs.haystack.deepset.ai/docs/tool) in the docs).\n",
        "\n",
        "For demonstration purposes, we're simply creating a `get_current_weather` function that returns an object which will _always_ tell us it's 'Sunny, and 21.8 degrees'... If it's Celsius, that's a good day! ‚òÄÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wa_IoDDNg9V"
      },
      "outputs": [],
      "source": [
        "from haystack.components.tools import ToolInvoker\n",
        "from haystack.tools import create_tool_from_function\n",
        "from typing import Annotated\n",
        "\n",
        "def get_current_weather(\n",
        "    location: Annotated[str, \"The city for which to get the weather, e.g. 'San Francisco'\"] = \"Munich\",\n",
        "    unit: Annotated[str, \"The unit for the temperature, e.g. 'celsius'\"] = \"celsius\",\n",
        "):\n",
        "  return {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": unit}\n",
        "\n",
        "weather_tool = create_tool_from_function(get_current_weather)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD4G61Z0OGZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_message = [ChatMessage.from_user(\"What is the temperature in celsius in Berlin?\")]\n",
        "replies = gemini.run(messages=user_message)[\"replies\"]\n",
        "print(replies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbEpArmfOGZX"
      },
      "source": [
        "Look at that! We go a message with some interesting information now.\n",
        "We can use that information to call a real function locally.\n",
        "\n",
        "Let's do exactly that and pass the result back to Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-DurWKOOSOk"
      },
      "outputs": [],
      "source": [
        "tool_invoker = ToolInvoker(tools=[weather_tool])\n",
        "tool_messages = tool_invoker.run(messages=replies)[\"tool_messages\"]\n",
        "\n",
        "messages = user_message + replies + tool_messages\n",
        "print(messages)\n",
        "\n",
        "res = gemini_chat.run(messages = messages)\n",
        "res[\"replies\"][0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsiwN7dbOwSW"
      },
      "source": [
        "Seems like the weather is nice and sunny, remember to put on your sunglasses. üòé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi37EVlDenPw"
      },
      "source": [
        "## Build a full Retrieval-Augmented Generation Pipeline with `gemini-2.0-flash`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQz9_N46hniU"
      },
      "source": [
        "As a final exercise, let's add the `GoogleGenAIChatGenerator` to a full RAG pipeline. In the example below, we are building a RAG pipeline that does question answering on the web, using `gemini-2.0-flash`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43QF7y1PhRQn"
      },
      "outputs": [],
      "source": [
        "from haystack.components.fetchers.link_content import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "from haystack.components.rankers import TransformersSimilarityRanker\n",
        "from haystack.components.builders.chat_prompt_builder import ChatPromptBuilder\n",
        "from haystack import Pipeline\n",
        "\n",
        "fetcher = LinkContentFetcher()\n",
        "converter = HTMLToDocument()\n",
        "document_splitter = DocumentSplitter(split_by=\"word\", split_length=50)\n",
        "similarity_ranker = TransformersSimilarityRanker(top_k=3)\n",
        "gemini = GoogleGenAIChatGenerator(model=\"gemini-2.0-flash\", api=\"vertex\", vertex_ai_project=project_id, vertex_ai_location=\"europe-west1\")\n",
        "\n",
        "prompt_template = [ChatMessage.from_user(\"\"\"\n",
        "According to these documents:\n",
        "\n",
        "{% for doc in documents %}\n",
        "  {{ doc.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Answer the given question: {{question}}\n",
        "Answer:\n",
        "\"\"\")]\n",
        "prompt_builder = ChatPromptBuilder(template=prompt_template)\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(\"fetcher\", fetcher)\n",
        "pipeline.add_component(\"converter\", converter)\n",
        "pipeline.add_component(\"splitter\", document_splitter)\n",
        "pipeline.add_component(\"ranker\", similarity_ranker)\n",
        "pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipeline.add_component(\"gemini\", gemini)\n",
        "\n",
        "pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "pipeline.connect(\"converter.documents\", \"splitter.documents\")\n",
        "pipeline.connect(\"splitter.documents\", \"ranker.documents\")\n",
        "pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
        "pipeline.connect(\"prompt_builder.prompt\", \"gemini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGqt94B0fZi1"
      },
      "source": [
        "Let's try asking Gemini to tell us about Haystack and how to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhEx8xO7jMf9"
      },
      "outputs": [],
      "source": [
        "question = \"What do graphs have to do with Haystack?\"\n",
        "result = pipeline.run({\"prompt_builder\": {\"question\": question},\n",
        "                   \"ranker\": {\"query\": question},\n",
        "                   \"fetcher\": {\"urls\": [\"https://haystack.deepset.ai/blog/introducing-haystack-2-beta-and-advent\"]}})\n",
        "\n",
        "for answer in result[\"gemini\"][\"replies\"]:\n",
        "  print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2SAGT_l25K"
      },
      "source": [
        "Now you've seen some of what Gemini can do, as well as how to integrate it with Haystack. If you want to learn more:\n",
        "- check out the Haystack [docs](https://docs.haystack.deepset.ai/docs) or [tutorials](https://haystack.deepset.ai/tutorials)\n",
        "- Try out the [Gemini quickstart colab from Google](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb#scrollTo=IqFXdgDFRvlU)\n",
        "- Participate in the [Advent of Haystack](https://haystack.deepset.ai/advent-of-haystack)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
