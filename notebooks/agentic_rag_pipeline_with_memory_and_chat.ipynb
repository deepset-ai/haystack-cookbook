{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt3tjDtpWX-A"
      },
      "source": [
        "## Agentic RAG Pipeline with memory and chat\n",
        "\n",
        "_notebook by Stefano Fiorucci ([Twitter](https://x.com/theanakin87), [LI](https://www.linkedin.com/in/stefano-fiorucci/)), Bilge Yucel ([Twitter](https://x.com/bilgeycl), [LI](https://www.linkedin.com/in/bilge-yucel/)),  and Tilde Thurium ([Twitter](https://x.com/annthurium), [LI](https://www.linkedin.com/in/annthurium/))_\n",
        "\n",
        "Here is an agentic RAG pipelines that can remember previous chat messages. The most helpful assistants don't forget the thing you just said. üìì\n",
        "\n",
        "This pipeline takes advantage of Haystack's flexible looping capabilities, running and saving previous interactions until you exit.\n",
        "\n",
        "Components used:\n",
        "- [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore)\n",
        "- [`InMemoryBM25Retriever`](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever)\n",
        "- [`ChatPromptBuilder`](https://docs.haystack.deepset.ai/docs/chatpromptbuilder)\n",
        "- [`FilterRetriever`](https://docs.haystack.deepset.ai/docs/filterretriever)\n",
        "- [`DocumentWriter`](https://docs.haystack.deepset.ai/docs/documentwriter)\n",
        "- [`OutputAdapter`](https://docs.haystack.deepset.ai/docs/outputadapter)\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "You'll need an [OpenAI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key), although this code could be adapted to use [any model Haystack supports](https://docs.haystack.deepset.ai/docs/generators)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "aKuneM4lWtfv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQbU8GUfO-qZ",
        "outputId": "4f761281-e309-40ae-8fff-ed63a67887bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.2.1-py3-none-any.whl (345 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 345.2/345.2 kB 4.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from haystack-ai)\n",
            "  Downloading openai-1.32.0-py3-none-any.whl (325 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 325.2/325.2 kB 9.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.0.3)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 41.3/41.3 kB 4.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.12.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75.6/75.6 kB 4.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 77.9/77.9 kB 5.5 MB/s eta 0:00:00\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 58.3/58.3 kB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.18.4)\n",
            "Installing collected packages: monotonic, lazy-imports, h11, backoff, posthog, httpcore, httpx, openai, haystack-ai\n",
            "Successfully installed backoff-2.2.1 h11-0.14.0 haystack-ai-2.2.1 httpcore-1.0.5 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.32.0 posthog-3.5.0\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorization"
      ],
      "metadata": {
        "id": "Qpf00rOFW2a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnaLI90LIhu2",
        "outputId": "e06aef44-0a2b-4da6-dcf3-0a79ef7de649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the document store\n",
        "\n",
        "Write some documents to the `DocumentStore`. You'll ask the agent to refer to them later."
      ],
      "metadata": {
        "id": "0-lHLCeVZqcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3q0vLO1GLNL",
        "outputId": "23f71fbc-8d57-4f3d-9c17-54f22425ee40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from haystack import Document, Pipeline\n",
        "from haystack.components.builders.chat_prompt_builder import ChatPromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "documents = [Document(content=\"There are over 7,000 languages spoken around the world today.\"),\n",
        "             Document(content=\"Chinese language boasts the highest number of native speakers.\"),\n",
        "\t\t\t       Document(content=\"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"),\n",
        "\t\t\t       Document(content=\"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\")]\n",
        "document_store.write_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve previous chat messages\n",
        "\n",
        "`FilterRetriever` is a special retriever that retrieves documents based on the passed-in `filters` parameter. Here, it is used to retrieve documents that were previously stored in the `InMemoryDocumentStore`.\n",
        "\n",
        "Note that no filters are passed in to the `FilterRetriever`. If we had a huge list of chat messages, this could get quite slow! In a production scenario, you'd want to create a session ID for each chat session and pass that in as a filter."
      ],
      "metadata": {
        "id": "Er_i5ClSaCb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.retrievers import FilterRetriever\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack import Document\n",
        "from haystack.components.converters import OutputAdapter\n",
        "from typing import List\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "\n",
        "memory_store = InMemoryDocumentStore()\n",
        "memory_retriever = FilterRetriever(memory_store)\n",
        "# The same ChatMessage can't be stored multiple times in memory_store\n",
        "memory_writer = DocumentWriter(memory_store, policy=DuplicatePolicy.SKIP)"
      ],
      "metadata": {
        "id": "KxLgfi11Z3BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save `ChatMessage` objects\n",
        "\n",
        "As the pipeline runs through multiple loops, you'll need to save previous chat messages to the document store.\n",
        "\n",
        "This is a utility function that takes a list of `ChatMessage` objects and turns them into Haystack documents. The `OutputAdapter` component makes the `chat_messages_to_docs` function fit smoothly into our pipeline, passing the input from the component at the end of the previous loop iteration into the next."
      ],
      "metadata": {
        "id": "jF4lvfzdbkvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_messages_to_docs(chat_messages: List[ChatMessage]):\n",
        "    return [Document(content=message.content) for message in chat_messages]\n",
        "\n",
        "output_adapter = OutputAdapter(template=\"{{ chat_messages | chat_messages_to_docs }}\", output_type=List[Document], custom_filters={\"chat_messages_to_docs\": chat_messages_to_docs})"
      ],
      "metadata": {
        "id": "7hq8rZMvbrBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the prompt\n",
        "\n",
        "The system message tells the LLM how to act.\n",
        "\n",
        "We need 2 loops here in our prompt: one to run through the memories, one to run through any relevant documents that were returned."
      ],
      "metadata": {
        "id": "PAREbRszj-29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, this pipelines uses the `InMemoryBM25Retriever` to do keyword-based retrieval. In a production scenario where you're searching a vast number of documents, an `EmbeddingRetriever` would be faster and more accurate.\n",
        "\n",
        "The `PromptBuilder` will take 3 arguments when the pipeline is run:\n",
        "- _query_, the chat message from the current loop iteration\n",
        "- _documents_, which were written when we initialized the `DocumentStore`\n",
        "- _memories_, or previous chat messages"
      ],
      "metadata": {
        "id": "a2Pam3kdfbF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = ChatMessage.from_system(\"You are a helpful assistant.\")\n",
        "user_message_template =\"\"\"Given the previous messages and the provided documents, answer the question. Use your memory.\n",
        "    Memory:\n",
        "    {% for memory in memories %}\n",
        "        {{ memory.content }}\n",
        "    {% endfor %}\n",
        "\n",
        "    Documents:\n",
        "    {% for doc in documents %}\n",
        "        {{ doc.content }}\n",
        "    {% endfor %}\n",
        "\n",
        "    \\nQuestion: {{query}}\n",
        "    \\nAnswer:\n",
        "\"\"\"\n",
        "user_message = ChatMessage.from_user(user_message_template)"
      ],
      "metadata": {
        "id": "aJW16ERZj3KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline"
      ],
      "metadata": {
        "id": "_xvET3_Md0B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# components for RAG\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
        "pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(template=[system_message, user_message]))\n",
        "pipeline.add_component(\"llm\", OpenAIChatGenerator())\n",
        "\n",
        "# components for memory\n",
        "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
        "pipeline.add_component(\"memory_writer\", memory_writer)\n",
        "pipeline.add_component(\"output_adapter\", output_adapter)\n",
        "\n",
        "# connections for RAG\n",
        "pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
        "\n",
        "# connections for memory\n",
        "pipeline.connect(\"memory_retriever\", \"prompt_builder.memories\")\n",
        "pipeline.connect(\"llm.replies\", \"output_adapter.chat_messages\")\n",
        "pipeline.connect(\"output_adapter\", \"memory_writer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znc3HnwbdzTk",
        "outputId": "4514e7cd-f1fe-448d-eda1-5e0d86c1a81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x788dd4100c40>\n",
              "üöÖ Components\n",
              "  - retriever: InMemoryBM25Retriever\n",
              "  - prompt_builder: ChatPromptBuilder\n",
              "  - llm: OpenAIChatGenerator\n",
              "  - memory_retriever: FilterRetriever\n",
              "  - memory_writer: DocumentWriter\n",
              "  - output_adapter: OutputAdapter\n",
              "üõ§Ô∏è Connections\n",
              "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
              "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
              "  - llm.replies -> output_adapter.chat_messages (List[ChatMessage])\n",
              "  - memory_retriever.documents -> prompt_builder.memories (List[Document])\n",
              "  - output_adapter.output -> memory_writer.documents (List[Document])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the pipeline\n",
        "\n",
        "If you want to see a diagram of the pipeline, running this cell will create a file locally. https://docs.haystack.deepset.ai/docs/visualizing-pipelines"
      ],
      "metadata": {
        "id": "zJ49TG4OxZ3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "fgZhV4-4yB1j",
        "outputId": "012dd8ce-b35a-47c8-c65c-b080d863c2de",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-49bbe8939f1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bring it all together\n",
        "\n",
        "What follows is glue code to run the pipeline in a loop, provide input instructions, and break when the user enters `Q` to quit.\n",
        "\n",
        "Here are some example questions to get you started:\n",
        "- _How many languages are there?_\n",
        "- _What is the one with most native speakers_\n",
        "- _Do you remember the two answers you gave to me before?_"
      ],
      "metadata": {
        "id": "uFwkDCbAk-P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question or Q to exit. Example: How many languages are there?\\nüîÆ \")\n",
        "    if question==\"Q\":\n",
        "        break\n",
        "\n",
        "    res = pipeline.run(data={\"retriever\": {\"query\": question},\n",
        "                             \"prompt_builder\": {\"template_variables\": {\"query\": question}}}, include_outputs_from={\"llm\", \"memory_retriever\"})\n",
        "    print(f\"res: {res}\")\n",
        "    assistant_resp = res['llm']['replies'][0]\n",
        "    print(f\"ü§ñ {assistant_resp.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk1ScpLGk-hS",
        "outputId": "bdb73299-d415-4365-aac8-4f702769e94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "üîÆ How many languages are there?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.in_memory.document_store:ID 'cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94' already exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "res: {'memory_writer': {'documents_written': 0}, 'llm': {'replies': [ChatMessage(content='There are over 7,000 languages spoken around the world today.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 14, 'prompt_tokens': 160, 'total_tokens': 174}})]}, 'memory_retriever': {'documents': [Document(id=cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94, content: 'There are over 7,000 languages spoken around the world today.')]}}\n",
            "ü§ñ There are over 7,000 languages spoken around the world today.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "üîÆ Which one is spoken the most?\n",
            "res: {'memory_writer': {'documents_written': 1}, 'llm': {'replies': [ChatMessage(content='Chinese language, as it boasts the highest number of native speakers.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 13, 'prompt_tokens': 173, 'total_tokens': 186}})]}, 'memory_retriever': {'documents': [Document(id=cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94, content: 'There are over 7,000 languages spoken around the world today.'), Document(id=109d86f4eef4a30f2d7a0a05137344049b2d93df7d9d0d3251243161f8292546, content: 'Chinese language boasts the highest number of native speakers.')]}}\n",
            "ü§ñ Chinese language, as it boasts the highest number of native speakers.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "üîÆ Do you remember the two answers you gave to me before?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.in_memory.document_store:ID '12e046a102ad7a258d8e73c3b313a6b79fcdfc6d86c0eefc6e7e9f2c7db8e68a' already exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "res: {'memory_writer': {'documents_written': 0}, 'llm': {'replies': [ChatMessage(content='Yes, the two answers I provided to you before were about bioluminescent waves being witnessed in certain parts of the world like the Maldives, Puerto Rico, and San Diego, and elephants showing high levels of self-awareness by recognizing themselves in mirrors.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 52, 'prompt_tokens': 247, 'total_tokens': 299}})]}, 'memory_retriever': {'documents': [Document(id=cfe93bc1c274908801e6670440bf2bbba54fad792770d57421f85ffa2a4fcc94, content: 'There are over 7,000 languages spoken around the world today.'), Document(id=109d86f4eef4a30f2d7a0a05137344049b2d93df7d9d0d3251243161f8292546, content: 'Chinese language boasts the highest number of native speakers.'), Document(id=3629b04367e85b2998123be0eb30dc543d0c38778fa8a1afac4e21c5026619e7, content: 'Chinese language, as it boasts the highest number of native speakers.'), Document(id=12e046a102ad7a258d8e73c3b313a6b79fcdfc6d86c0eefc6e7e9f2c7db8e68a, content: 'Yes, the two answers I provided to you before were about bioluminescent waves being witnessed in cer...')]}}\n",
            "ü§ñ Yes, the two answers I provided to you before were about bioluminescent waves being witnessed in certain parts of the world like the Maldives, Puerto Rico, and San Diego, and elephants showing high levels of self-awareness by recognizing themselves in mirrors.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "üîÆ Q\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}