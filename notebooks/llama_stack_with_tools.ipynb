{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üõ†Ô∏èü¶ô Build with Llama Stack and Haystack\n",
        "\n",
        "\n",
        "This notebook demonstrates how to use the `LlamaStackChatGenerator` component with tools to enable function calling capabilities. We'll create a simple weather tool that the model can call to provide dynamic, up-to-date information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Installation\n",
        "%%bash\n",
        "\n",
        "pip install llama-stack-haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Before running this example, you need to:\n",
        "\n",
        "1. Set up Llama Stack Server through an inference provider\n",
        "2. Have a model available (e.g., `llama3.2:3b`)\n",
        "\n",
        "For a quick start on how to setup server with Ollama, see the [Llama Stack documentation](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
        "\n",
        "Once you have the server running, it will typically be available at `http://localhost:8321/v1/openai/v1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Importing Components\n",
        "from haystack.components.tools import ToolInvoker\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.tools import Tool\n",
        "from haystack_integrations.components.generators.llama_stack import LlamaStackChatGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a Tool\n",
        "\n",
        "Tools in Haystack allow models to call functions to get real-time information or perform actions. Let's create a simple weather tool that the model can use to provide weather information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a tool that models can call\n",
        "def weather(city: str):\n",
        "    \"\"\"Return mock weather info for the given city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny and 32¬∞C\"\n",
        "\n",
        "# Define the tool parameters schema\n",
        "tool_parameters = {\n",
        "    \"type\": \"object\", \n",
        "    \"properties\": {\n",
        "        \"city\": {\"type\": \"string\"}\n",
        "    }, \n",
        "    \"required\": [\"city\"]\n",
        "}\n",
        "\n",
        "# Create the weather tool\n",
        "weather_tool = Tool(\n",
        "    name=\"weather\",\n",
        "    description=\"Useful for getting the weather in a specific city\",\n",
        "    parameters=tool_parameters,\n",
        "    function=weather,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Components\n",
        "\n",
        "Now let's create the `ToolInvoker` and `LlamaStackChatGenerator` components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tool invoker with the weather tool\n",
        "tool_invoker = ToolInvoker(tools=[weather_tool])\n",
        "\n",
        "# Create the LlamaStackChatGenerator\n",
        "chat_generator = LlamaStackChatGenerator(\n",
        "    model=\"ollama/llama3.2:3b\",  # model name varies depending on the inference provider used for the Llama Stack Server\n",
        "    api_base_url=\"http://localhost:8321/v1/openai/v1\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Tools with the Chat Generator\n",
        "\n",
        "Now lets ask `LlamaStackChatGenerator` some questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant message: ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text=''), ToolCall(tool_name='weather', arguments={'city': 'Tokyo'}, id='call_52l1vdot')], _name=None, _meta={'model': 'llama3.2:3b', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 17, 'prompt_tokens': 162, 'total_tokens': 179, 'completion_tokens_details': None, 'prompt_tokens_details': None}})\n",
            "Tool calls: [ToolCall(tool_name='weather', arguments={'city': 'Tokyo'}, id='call_52l1vdot')]\n"
          ]
        }
      ],
      "source": [
        "# Create a message asking about the weather\n",
        "messages = [ChatMessage.from_user(\"What's the weather in Tokyo?\")]\n",
        "\n",
        "# Generate a response from the model with access to tools\n",
        "response = chat_generator.run(messages=messages, tools=[weather_tool])[\"replies\"]\n",
        "\n",
        "print(f\"Assistant message: {response[0]}\")\n",
        "print(f\"Tool calls: {response[0].tool_calls}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üõ†Ô∏è Executing tool call...\n",
            "Tool results: [ChatMessage(_role=<ChatRole.TOOL: 'tool'>, _content=[ToolCallResult(result='The weather in Tokyo is sunny and 32¬∞C', origin=ToolCall(tool_name='weather', arguments={'city': 'Tokyo'}, id='call_52l1vdot'), error=False)], _name=None, _meta={})]\n",
            "\n",
            "Final response: According to the weather data, the temperature in Tokyo is currently 32¬∞C (0¬∞F), with lots of sunshine. It's a beautiful day to explore the city! Would you like more information on the current weather conditions or forecasts for Tokyo?\n"
          ]
        }
      ],
      "source": [
        "# If the assistant message contains a tool call, run the tool invoker\n",
        "if response[0].tool_calls:\n",
        "    print(\"\\nüõ†Ô∏è Executing tool call...\")\n",
        "    tool_messages = tool_invoker.run(messages=response)[\"tool_messages\"]\n",
        "    print(f\"Tool results: {tool_messages}\")\n",
        "    \n",
        "    # Continue the conversation with the tool results\n",
        "    all_messages = messages + response + tool_messages\n",
        "    final_response = chat_generator.run(messages=all_messages, tools=[weather_tool])[\"replies\"]\n",
        "    print(f\"\\nFinal response: {final_response[0].text}\")\n",
        "else:\n",
        "    print(\"No tool calls were made.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets create a simple mechanism to chat with `LlamaStackChatGenerator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ The main character in The Witcher series, also known as the eponymous figure, is Geralt of Rivia, a monster hunter with supernatural abilities and mutations that allow him to control the elements. He was created by Polish author_and_polish_video_game_development_company_(CD Projekt).\n",
            "ü§ñ One of the most fascinating aspects of dolphin behavior is their ability to produce complex, context-dependent vocalizations that are unique to each individual, similar to human language. They also exhibit advanced social behaviors, such as cooperation, empathy, and self-awareness.\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "\n",
        "while True:\n",
        "  msg = input(\"Enter your message or Q to exit\\nüßë \")\n",
        "  if msg==\"Q\":\n",
        "    break\n",
        "  messages.append(ChatMessage.from_user(msg))\n",
        "  response = chat_generator.run(messages=messages)\n",
        "  assistant_resp = response['replies'][0]\n",
        "  print(\"ü§ñ \"+assistant_resp.text)\n",
        "  messages.append(assistant_resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to switch your model provider, you can reuse the same `LlamaStackChatGenerator` code with different providers. Simply run the desired inference provider on the Llama Stack Server and update the model name during the initialization of `LlamaStackChatGenerator`.\n",
        "\n",
        "For more details on available inference providers, see (Llama Stack docs)[https://llama-stack.readthedocs.io/en/latest/providers/inference/index.html]."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
