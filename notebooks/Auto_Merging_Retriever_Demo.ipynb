{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Retrieval with Hierarchical Document Retrieval\n",
        "\n",
        "This notebook shows how to use experimetnal components for Haystack: `AutoMergingRetriever` and `HierarchicalDocumentSplitter`.\n",
        "\n",
        "- ğŸ™ Please [join the discussion for these experimental components](https://github.com/deepset-ai/haystack-experimental/discussions/78)\n",
        "- Documentation for [`AutoMergingRetriever`](https://docs.haystack.deepset.ai/reference/auto-merge-retriever)\n",
        "- Documentation for [`HierarchicalDocumentSplitter`](https://docs.haystack.deepset.ai/reference/hierarchical-document-splitter)\n",
        "- This is material part of the blog post on the Auto-Merging Retriever - add link here"
      ],
      "metadata": {
        "id": "3hGCrW6Ue-7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Improving Retrieval with Hierarchical Document Retrieval](#scrollTo=3hGCrW6Ue-7v)\n",
        "\n",
        ">>[Setting up](#scrollTo=TeOAvHZsBXhf)\n",
        "\n",
        ">>[Let's get a dataset to index and explore](#scrollTo=2bgDKD-GgX4l)\n",
        "\n",
        ">>[Let's convert the raw data into Haystack Documents](#scrollTo=4gm-olXqhyoE)\n",
        "\n",
        ">>[Document Splitting and Indexing](#scrollTo=jJKxxkysiacd)\n",
        "\n",
        ">>[Retrieving Documents with Auto-Merging](#scrollTo=VDMahtVkpMAT)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "u0joRVe-BbhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up"
      ],
      "metadata": {
        "id": "TeOAvHZsBXhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaJsFx4P1o_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b29fa2-6d74-4ccf-e732-77c8a4f68491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepset-ai/haystack-experimental.git@main\n",
            "  Cloning https://github.com/deepset-ai/haystack-experimental.git (to revision main) to /tmp/pip-req-build-q1phkjza\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack-experimental.git /tmp/pip-req-build-q1phkjza\n",
            "  Resolved https://github.com/deepset-ai/haystack-experimental.git to commit 677d90c5997ba8c4c3bcdba563a86f22eef5088b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting haystack-ai (from haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading haystack_ai-2.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (10.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (3.3)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (1.26.4)\n",
            "Collecting openai>=1.1.0 (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading openai-1.43.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.1.4)\n",
            "Collecting posthog (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading posthog-3.6.3-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2024.8.30)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai->haystack-experimental==0.1.2.dev21+g677d90c) (2.20.1)\n",
            "Downloading haystack_ai-2.5.0-py3-none-any.whl (351 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.5/351.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.43.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Downloading posthog-3.6.3-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: haystack-experimental\n",
            "  Building wheel for haystack-experimental (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for haystack-experimental: filename=haystack_experimental-0.1.2.dev21+g677d90c-py3-none-any.whl size=66409 sha256=5a7f583ab69ee571515f0db646bde4162f3f3c34f85ecc57ff66b827d427ea28\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ds41qxxt/wheels/2a/30/f5/628cacec8f3a806b406470fe9fdbc8a12dfb44630132e98546\n",
            "Successfully built haystack-experimental\n",
            "Installing collected packages: monotonic, lazy-imports, jiter, h11, backoff, posthog, httpcore, httpx, openai, haystack-ai, haystack-experimental\n",
            "Successfully installed backoff-2.2.1 h11-0.14.0 haystack-ai-2.5.0 haystack-experimental-0.1.2.dev21+g677d90c httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.43.1 posthog-3.6.3\n",
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: haystack-experimental in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (0.1.2.dev21+g677d90c)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.4)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.43.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.1.4)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.6.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.8.30)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/deepset-ai/haystack-experimental.git@main\n",
        "!pip install haystack-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get a dataset to index and explore\n",
        "\n",
        "- We will use a dataset containing 2225 new articles part of the paper by \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006. by D. Greene and P. Cunningham.\n",
        "\n",
        "- The original dataset is available at http://mlg.ucd.ie/datasets/bbc.html, but we will instead use a CSV processed version available here: https://raw.githubusercontent.com/amankharwal/Website-data/master/bbc-news-data.csv"
      ],
      "metadata": {
        "id": "2bgDKD-GgX4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amankharwal/Website-data/master/bbc-news-data.csv"
      ],
      "metadata": {
        "id": "cpMYVx1VY7Z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521dbe20-c6dc-4897-c4d7-764b6b82cea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-06 09:41:04--  https://raw.githubusercontent.com/amankharwal/Website-data/master/bbc-news-data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5080260 (4.8M) [text/plain]\n",
            "Saving to: â€˜bbc-news-data.csvâ€™\n",
            "\n",
            "bbc-news-data.csv   100%[===================>]   4.84M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-09-06 09:41:05 (56.4 MB/s) - â€˜bbc-news-data.csvâ€™ saved [5080260/5080260]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Let's convert the raw data into Haystack Documents"
      ],
      "metadata": {
        "id": "4gm-olXqhyoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from typing import List\n",
        "from haystack import Document\n",
        "\n",
        "def read_documents() -> List[Document]:\n",
        "    with open(\"bbc-news-data.csv\", \"r\") as file:\n",
        "        reader = csv.reader(file, delimiter=\"\\t\")\n",
        "        next(reader, None)  # skip the headers\n",
        "        documents = []\n",
        "        for row in reader:\n",
        "            category = row[0].strip()\n",
        "            title = row[2].strip()\n",
        "            text = row[3].strip()\n",
        "            documents.append(Document(content=text, meta={\"category\": category, \"title\": title}))\n",
        "\n",
        "    return documents"
      ],
      "metadata": {
        "id": "BNfHo3tN_8mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = read_documents()"
      ],
      "metadata": {
        "id": "1ThlSJguh7ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OSZntiEh7zs",
        "outputId": "3af289d5-b829-4717-c1b3-ac7a1fe2ad27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=8b0eec9b4039d3c21eed119c9cbf1022a172f6b96661a391c76ee9a00b388334, content: 'Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to...', meta: {'category': 'business', 'title': 'Ad sales boost Time Warner profit'}),\n",
              " Document(id=0b20edb280b3c492d81751d97aa67f008759b242f2596d56c6816bacb5ea0c08, content: 'The dollar has hit its highest level against the euro in almost three months after the Federal Reser...', meta: {'category': 'business', 'title': 'Dollar gains on Greenspan speech'}),\n",
              " Document(id=9465b0a3c9e81843db56beb8cb3183b14810e8fc7b3195bd37718296f3a13e31, content: 'The owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit t...', meta: {'category': 'business', 'title': 'Yukos unit buyer faces loan claim'}),\n",
              " Document(id=151d64ed92b61b1b9e58c52a90e7ab4be964c0e47aaf1a233dfb93110986d9cd, content: 'British Airways has blamed high fuel prices for a 40% drop in profits.  Reporting its results for th...', meta: {'category': 'business', 'title': \"High fuel prices hit BA's profits\"}),\n",
              " Document(id=4355d611f770b814f9e7d33959ad9d16b69048650ed0eaf24f1bce3e8ab5bf4c, content: 'Shares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the targe...', meta: {'category': 'business', 'title': 'Pernod takeover talk lifts Domecq'})]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we have successfully created Documents.\n",
        "\n",
        "## Document Splitting and Indexing\n",
        "\n",
        "Now we split each document into smaller ones creating an hierarchical document structure connecting each smaller child documents with the corresponding parent document.\n",
        "\n",
        "We also create two document stores, one for the leaf documents and the other for the parent documents."
      ],
      "metadata": {
        "id": "jJKxxkysiacd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "\n",
        "from haystack_experimental.components.splitters import HierarchicalDocumentSplitter\n",
        "\n",
        "def indexing(documents: List[Document]) -> Tuple[InMemoryDocumentStore, InMemoryDocumentStore]:\n",
        "    splitter = HierarchicalDocumentSplitter(block_sizes={10, 3}, split_overlap=0, split_by=\"word\")\n",
        "    docs = splitter.run(documents)\n",
        "\n",
        "    # Store the leaf documents in one document store\n",
        "    leaf_documents = [doc for doc in docs[\"documents\"] if doc.meta[\"__level\"] == 1]\n",
        "    leaf_doc_store = InMemoryDocumentStore()\n",
        "    leaf_doc_store.write_documents(leaf_documents, policy=DuplicatePolicy.OVERWRITE)\n",
        "\n",
        "    # Store the parent documents in another document store\n",
        "    parent_documents = [doc for doc in docs[\"documents\"] if doc.meta[\"__level\"] == 0]\n",
        "    parent_doc_store = InMemoryDocumentStore()\n",
        "    parent_doc_store.write_documents(parent_documents, policy=DuplicatePolicy.OVERWRITE)\n",
        "\n",
        "    return leaf_doc_store, parent_doc_store"
      ],
      "metadata": {
        "id": "7KV0CNqJAVM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaf_doc_store, parent_doc_store = indexing(docs)"
      ],
      "metadata": {
        "id": "VJDoInqQnQaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving Documents with Auto-Merging\n",
        "\n",
        "We are now ready to query the document store using the `AutoMergingRetriever`. Let's build a pipeline that uses the `BM25Retriever` to handle the user queries, and we connect it to the `AutoMergingRetriever`, which, based on the documents retrieved and the hierarchical structure, decides whether the leaf documents or the parent document is returned."
      ],
      "metadata": {
        "id": "VDMahtVkpMAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack_experimental.components.retrievers import AutoMergingRetriever\n",
        "\n",
        "def querying_pipeline(leaf_doc_store: InMemoryDocumentStore, parent_doc_store: InMemoryDocumentStore, threshold: float = 0.6):\n",
        "    pipeline = Pipeline()\n",
        "    bm25_retriever = InMemoryBM25Retriever(document_store=leaf_doc_store)\n",
        "    auto_merge_retriever = AutoMergingRetriever(parent_doc_store, threshold=threshold)\n",
        "    pipeline.add_component(instance=bm25_retriever, name=\"BM25Retriever\")\n",
        "    pipeline.add_component(instance=auto_merge_retriever, name=\"AutoMergingRetriever\")\n",
        "    pipeline.connect(\"BM25Retriever.documents\", \"AutoMergingRetriever.matched_leaf_documents\")\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "ysiW52XcAWRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create this pipeline by setting the threshold for the `AutoMergingRetriever` at 0.6"
      ],
      "metadata": {
        "id": "CCf73zK6sdZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = querying_pipeline(leaf_doc_store, parent_doc_store, threshold=0.6)"
      ],
      "metadata": {
        "id": "PTNb-HZCp3fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now query the pipeline for document store for articles related to cybersecurity. Let's also make use of the pipeline parameter `include_outputs_from` to also get the outputs from the `BM25Retriever` component.  \n"
      ],
      "metadata": {
        "id": "BzlnybXQstXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.run(data={'query': 'phishing attacks spoof websites spam e-mails spyware'},  include_outputs_from={'BM25Retriever'})"
      ],
      "metadata": {
        "id": "Gem7W5JmsY1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['AutoMergingRetriever']['documents'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S62-2ScPs0l0",
        "outputId": "3e4e6b52-70e6-4da0-aecb-078b46996764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['BM25Retriever']['documents'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB0XGbCgs9jO",
        "outputId": "48fdbcbb-9add-46db-fa44-67cc4b576fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_doc_titles_bm25 = sorted([d.meta['title'] for d in result['BM25Retriever']['documents']])"
      ],
      "metadata": {
        "id": "m60y5n2Qs-8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_doc_titles_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88VzuL25tAPr",
        "outputId": "5e339156-1026-49eb-e1f3-6849b1a2284f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bad e-mail habits sustains spam',\n",
              " 'Cyber criminals step up the pace',\n",
              " 'Cyber criminals step up the pace',\n",
              " 'More women turn to net security',\n",
              " 'Rich pickings for hi-tech thieves',\n",
              " 'Screensaver tackles spam websites',\n",
              " 'Security scares spark browser fix',\n",
              " 'Solutions to net security fears',\n",
              " 'Solutions to net security fears',\n",
              " 'Spam e-mails tempt net shoppers']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_doc_titles_automerging = sorted([d.meta['title'] for d in result['AutoMergingRetriever']['documents']])"
      ],
      "metadata": {
        "id": "PfmhKk-_tBX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_doc_titles_automerging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMKXEjOmtDCf",
        "outputId": "6f17ddff-b08b-4671-dedf-8358506ff725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bad e-mail habits sustains spam',\n",
              " 'Cyber criminals step up the pace',\n",
              " 'Cyber criminals step up the pace',\n",
              " 'More women turn to net security',\n",
              " 'Rich pickings for hi-tech thieves',\n",
              " 'Screensaver tackles spam websites',\n",
              " 'Security scares spark browser fix',\n",
              " 'Solutions to net security fears',\n",
              " 'Solutions to net security fears',\n",
              " 'Spam e-mails tempt net shoppers']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPXqAv0OtED2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}