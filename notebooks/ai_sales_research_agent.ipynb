{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Build Your Own AI Sales Research Agent\n",
    "\n",
    "## LinkedIn, Company Intelligence & Lead Enrichment with Haystack, MongoDB Atlas, and Bright Data\n",
    "\n",
    "### The AI Sales Research Revolution\n",
    "\n",
    "**The Problem with Traditional Lead Research:**\n",
    "\n",
    "Sales teams today face a perfect storm of challenges:\n",
    "\n",
    "- **LinkedIn Crackdowns**: Recent lawsuits and aggressive anti-scraping measures have killed most LinkedIn data tools, leaving sales teams in the dark\n",
    "- **Expensive SaaS Tools**: Traditional lead intelligence platforms like ZoomInfo, Apollo.io, and Seamless.AI cost $500-2,000+ per month per seat\n",
    "- **Manual Research is Slow**: Researching a single prospect manually takes 30-45 minutes - time most sales teams don't have\n",
    "- **Data Gets Stale Fast**: By the time you buy a lead list, key decision makers may have changed roles\n",
    "- **Compliance Concerns**: Many scraping tools operate in legal gray areas, putting your company at risk\n",
    "\n",
    "**The Compliant, Cost-Effective Alternative:**\n",
    "\n",
    "This cookbook demonstrates how to build your own AI-powered sales research agent that:\n",
    "\n",
    "‚úÖ **Extracts live data** from LinkedIn, Crunchbase, news sources, and job postings  \n",
    "‚úÖ **Stays compliant** using Bright Data's ethical, robots.txt-respecting infrastructure  \n",
    "‚úÖ **Costs ~$0.50 per company** researched (vs $50-100/month per lead in traditional tools)  \n",
    "‚úÖ **Provides deeper insights** by combining multiple data sources with AI analysis  \n",
    "‚úÖ **Answers complex questions** like \"What pain points is this company facing?\" and \"Generate a personalized outreach angle\"  \n",
    "\n",
    "**The Tech Stack:**\n",
    "\n",
    "- **üåê Bright Data**: Compliant web scraping for 45+ data sources (LinkedIn, Crunchbase, news, job boards)\n",
    "- **üçÉ MongoDB Atlas**: Vector database for semantic search + structured metadata filtering\n",
    "- **üîß Haystack**: Open-source LLM framework for building RAG pipelines and AI agents\n",
    "- **ü§ñ Google Gemini 2.0**: Generate actionable sales intelligence from raw data\n",
    "\n",
    "**What You'll Build:**\n",
    "\n",
    "By the end of this cookbook, you'll have a working sales research agent that can:\n",
    "\n",
    "1. **Find companies** matching your Ideal Customer Profile (ICP) criteria\n",
    "2. **Identify decision makers** and research their backgrounds\n",
    "3. **Extract pain points** from job postings, news articles, and company data\n",
    "4. **Generate personalized outreach** angles based on comprehensive company intelligence\n",
    "\n",
    "**Time to ROI:**\n",
    "\n",
    "| Metric | Manual Research | Traditional SaaS | This Agent |\n",
    "|--------|----------------|------------------|------------|\n",
    "| Time per company | 30-45 min | 5-10 min | 2-3 min |\n",
    "| Cost per 100 companies | $0 (your time) | $500-2,000/mo | ~$50 total |\n",
    "| Data freshness | Real-time | Days/weeks old | Real-time |\n",
    "| Customization | Full control | Limited | Full control |\n",
    "| Compliance risk | High (DIY scraping) | Medium | Low (Bright Data) |\n",
    "\n",
    "Let's get started! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Architecture Overview\n",
    "\n",
    "### How the Sales Research Agent Works\n",
    "\n",
    "Our AI agent combines three powerful technologies to deliver comprehensive lead intelligence:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   User Query    ‚îÇ  \"Find AI startups in NYC with Series A funding\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   HAYSTACK PIPELINE                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ   Embedder   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Retriever   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Prompt Builder‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                              ‚îÇ                    ‚îÇ          ‚îÇ\n",
    "‚îÇ                              ‚ñº                    ‚ñº          ‚îÇ\n",
    "‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ                    ‚îÇ  MongoDB Atlas   ‚îÇ  ‚îÇ Gemini 2.0   ‚îÇ  ‚îÇ\n",
    "‚îÇ                    ‚îÇ Vector Search +  ‚îÇ  ‚îÇ  Generator   ‚îÇ  ‚îÇ\n",
    "‚îÇ                    ‚îÇ Metadata Filter  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  INDEXING LAYER   ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ   BRIGHT DATA     ‚îÇ\n",
    "                    ‚îÇ  Web Scraping API ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ               ‚îÇ               ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇLinkedIn ‚îÇ    ‚îÇ Crunchbase‚îÇ  ‚îÇGoogle SERP‚îÇ\n",
    "         ‚îÇProfiles ‚îÇ    ‚îÇ Companies ‚îÇ  ‚îÇ   News    ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Component Breakdown\n",
    "\n",
    "#### 1. **Bright Data Layer** (Data Collection)\n",
    "- **Web Scraper API**: Extracts structured data from 45+ sources\n",
    "  - `linkedin_company_profile`: Company size, industry, description, location\n",
    "  - `linkedin_person_profile`: Decision maker titles, backgrounds, experience\n",
    "  - `crunchbase_company`: Funding rounds, investors, employee count\n",
    "- **SERP API**: Real-time search results from Google/Bing\n",
    "  - Company news and press releases\n",
    "  - Job postings (signal for pain points)\n",
    "  - Industry trends and mentions\n",
    "- **Compliance Built-in**: Respects robots.txt, handles CAPTCHAs, rotates IPs automatically\n",
    "\n",
    "#### 2. **MongoDB Atlas** (Storage & Retrieval)\n",
    "- **Vector Search**: Semantic similarity matching on embedded company/person descriptions\n",
    "- **Metadata Filtering**: Hybrid search combining vectors with structured filters\n",
    "  - Filter by: industry, funding stage, location, company size, job titles\n",
    "- **Document Storage**: Stores raw scraped data + embeddings + metadata\n",
    "- **Scalable**: Handles millions of leads with sub-second query times\n",
    "\n",
    "#### 3. **Haystack Pipeline** (Orchestration)\n",
    "- **Embedder**: Converts queries and documents to vector representations using Google's text-embedding-004\n",
    "- **Retriever**: Finds most relevant leads from MongoDB based on semantic + metadata match\n",
    "- **Prompt Builder**: Constructs context-rich prompts with retrieved lead data\n",
    "- **LLM Generator**: Gemini 2.0 Flash synthesizes insights and generates actionable intelligence\n",
    "\n",
    "### Agent Capabilities\n",
    "\n",
    "This architecture enables four key workflows:\n",
    "\n",
    "**1. Company Discovery**\n",
    "- Input: ICP criteria (industry, funding stage, location, size)\n",
    "- Process: Scrape Crunchbase/LinkedIn ‚Üí Index in MongoDB ‚Üí Semantic search\n",
    "- Output: Ranked list of companies matching criteria\n",
    "\n",
    "**2. Decision Maker Identification**\n",
    "- Input: Company name or URL\n",
    "- Process: Scrape LinkedIn company page ‚Üí Extract employee profiles ‚Üí Identify key roles\n",
    "- Output: List of decision makers with titles, backgrounds, and contact hints\n",
    "\n",
    "**3. Pain Point Analysis**\n",
    "- Input: Company name\n",
    "- Process: SERP search for job postings + news ‚Üí Analyze requirements and challenges\n",
    "- Output: Inferred pain points, hiring priorities, growth signals\n",
    "\n",
    "**4. Personalized Outreach Generation**\n",
    "- Input: Prospect name/company + context from above\n",
    "- Process: RAG retrieval of all data ‚Üí Gemini synthesis with sales prompts\n",
    "- Output: Personalized email/message angle with specific talking points\n",
    "\n",
    "### Data Flow Example\n",
    "\n",
    "**Query**: *\"Find AI startups in NYC that raised Series A in the last 6 months\"*\n",
    "\n",
    "1. **Scraping**: Bright Data queries Crunchbase for AI companies in NYC with recent Series A funding\n",
    "2. **Indexing**: Companies are converted to Documents with embeddings and metadata (industry=AI, location=NYC, funding_stage=Series A)\n",
    "3. **Retrieval**: Query embedding matches semantically similar companies + metadata filters enforce ICP criteria\n",
    "4. **Generation**: Gemini 2.0 receives top 10 matching companies and synthesizes a detailed report with key insights\n",
    "\n",
    "Now let's build it! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to install the required dependencies for our sales research agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in ./venv/lib/python3.11/site-packages (2.22.0)\n",
      "Requirement already satisfied: haystack-brightdata in ./venv/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: mongodb-atlas-haystack in ./venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: google-genai-haystack in ./venv/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: docstring-parser in ./venv/lib/python3.11/site-packages (from haystack-ai) (0.17.0)\n",
      "Requirement already satisfied: filetype in ./venv/lib/python3.11/site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: haystack-experimental in ./venv/lib/python3.11/site-packages (from haystack-ai) (0.16.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in ./venv/lib/python3.11/site-packages (from haystack-ai) (4.26.0)\n",
      "Requirement already satisfied: lazy-imports in ./venv/lib/python3.11/site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: more-itertools in ./venv/lib/python3.11/site-packages (from haystack-ai) (10.8.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from haystack-ai) (3.6.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from haystack-ai) (2.4.1)\n",
      "Requirement already satisfied: openai>=1.99.2 in ./venv/lib/python3.11/site-packages (from haystack-ai) (2.15.0)\n",
      "Requirement already satisfied: posthog!=3.12.0 in ./venv/lib/python3.11/site-packages (from haystack-ai) (7.5.1)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.11/site-packages (from haystack-ai) (2.12.5)\n",
      "Requirement already satisfied: python-dateutil in ./venv/lib/python3.11/site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.11/site-packages (from haystack-ai) (6.0.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from haystack-ai) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in ./venv/lib/python3.11/site-packages (from haystack-ai) (9.1.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.11/site-packages (from haystack-ai) (4.15.0)\n",
      "Requirement already satisfied: aiohttp>=3.8.0 in ./venv/lib/python3.11/site-packages (from haystack-brightdata) (3.13.2)\n",
      "Requirement already satisfied: pymongo>=4.13.0 in ./venv/lib/python3.11/site-packages (from pymongo[srv]>=4.13.0->mongodb-atlas-haystack) (4.16.0)\n",
      "Requirement already satisfied: google-genai>=1.51.0 in ./venv/lib/python3.11/site-packages (from google-genai[aiohttp]>=1.51.0->google-genai-haystack) (1.59.0)\n",
      "Requirement already satisfied: jsonref>=1.0.0 in ./venv/lib/python3.11/site-packages (from google-genai-haystack) (1.1.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.11/site-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.8.0->haystack-brightdata) (1.22.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in ./venv/lib/python3.11/site-packages (from google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (4.12.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in ./venv/lib/python3.11/site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (2.47.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in ./venv/lib/python3.11/site-packages (from google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (0.28.1)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./venv/lib/python3.11/site-packages (from google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.2->haystack-ai) (0.12.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.11/site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.11/site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.11/site-packages (from pydantic->haystack-ai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.11/site-packages (from pydantic->haystack-ai) (0.4.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in ./venv/lib/python3.11/site-packages (from pymongo>=4.13.0->pymongo[srv]>=4.13.0->mongodb-atlas-haystack) (2.8.0)\n",
      "\u001b[33mWARNING: pymongo 4.16.0 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->haystack-ai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->haystack-ai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->haystack-ai) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->haystack-ai) (2026.1.4)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.11/site-packages (from haystack-experimental->haystack-ai) (14.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->haystack-ai) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema->haystack-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema->haystack-ai) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.11/site-packages (from jsonschema->haystack-ai) (0.30.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (4.9.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (0.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.11/site-packages (from rich->haystack-experimental->haystack-ai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich->haystack-experimental->haystack-ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->haystack-experimental->haystack-ai) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai>=1.51.0->google-genai[aiohttp]>=1.51.0->google-genai-haystack) (0.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install haystack-ai haystack-brightdata mongodb-atlas-haystack google-genai-haystack dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "\n",
    "Next, we'll configure the API keys needed for our sales research agent. You'll need:\n",
    "\n",
    "1. **Bright Data API Key**: Get yours from the [Bright Data Dashboard](https://brightdata.com/cp/setting/users)\n",
    "2. **MongoDB Connection String**: From your [MongoDB Atlas cluster](https://www.mongodb.com/docs/atlas/getting-started/)\n",
    "3. **Google API Key**: For Gemini access from [Google AI Studio](https://aistudio.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All environment variables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Map GOOGLE_AI_API_KEY to GOOGLE_API_KEY if needed\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\") and os.environ.get(\"GOOGLE_AI_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GOOGLE_AI_API_KEY\"]\n",
    "\n",
    "# Verify all required keys are loaded\n",
    "required_keys = [\"BRIGHT_DATA_API_KEY\", \"MONGO_CONNECTION_STRING\", \"GOOGLE_API_KEY\"]\n",
    "missing_keys = [key for key in required_keys if not os.environ.get(key)]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"‚ùå Missing keys: {', '.join(missing_keys)}\")\n",
    "    raise ValueError(f\"Please add {', '.join(missing_keys)} to your .env file\")\n",
    "else:\n",
    "    print(\"‚úÖ All environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bright Data Datasets Overview\n",
    "\n",
    "Bright Data provides 45+ pre-built datasets for extracting structured data from major platforms. For our sales research agent, we'll use the following datasets:\n",
    "\n",
    "### üè¢ Company Intelligence Datasets\n",
    "\n",
    "**1. `crunchbase_company`**\n",
    "- Extracts: Funding rounds, investors, total funding, employee count, headquarters location, founding year\n",
    "- Use case: Identify companies by funding stage and investment activity\n",
    "- Example URL: `https://www.crunchbase.com/organization/example-company`\n",
    "\n",
    "**2. `linkedin_company_profile`**\n",
    "- Extracts: Company description, industry, size, location, website, follower count\n",
    "- Use case: Get current company information and industry classification\n",
    "- Example URL: `https://www.linkedin.com/company/example-company/`\n",
    "\n",
    "### üë• People Intelligence Datasets\n",
    "\n",
    "**3. `linkedin_person_profile`**\n",
    "- Extracts: Name, title, company, location, summary, experience, education\n",
    "- Use case: Identify decision makers and research their backgrounds\n",
    "- Example URL: `https://www.linkedin.com/in/example-profile/`\n",
    "\n",
    "### üì∞ Market Signals (via SERP API)\n",
    "\n",
    "**4. `google_serp` (SERP API)**\n",
    "- Extracts: Search results, news articles, job postings\n",
    "- Use case: Find recent company news, hiring signals, and pain points\n",
    "- Query examples: \n",
    "  - `\"Company Name\" news funding` (recent announcements)\n",
    "  - `\"Company Name\" jobs hiring site:linkedin.com` (hiring signals)\n",
    "\n",
    "You can explore all available datasets using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available datasets: 43\n",
      "\n",
      "Sales research relevant datasets:\n",
      "--------------------------------------------------\n",
      "üìä linkedin_person_profile\n",
      "   Extract structured LinkedIn person profile data. Requires a valid LinkedIn profile URL.\n",
      "\n",
      "üìä linkedin_company_profile\n",
      "   Extract structured LinkedIn company profile data. Requires a valid LinkedIn company URL.\n",
      "\n",
      "üìä linkedin_job_listings\n",
      "   Extract structured LinkedIn job listings data. Requires a valid LinkedIn job URL.\n",
      "\n",
      "üìä linkedin_posts\n",
      "   Extract structured LinkedIn posts data. Requires a valid LinkedIn post URL.\n",
      "\n",
      "üìä linkedin_people_search\n",
      "   Extract structured LinkedIn people search data. Requires URL, first_name, and last_name.\n",
      "\n",
      "üìä crunchbase_company\n",
      "   Extract structured Crunchbase company data. Requires a valid Crunchbase company URL.\n",
      "\n",
      "üìä zoominfo_company_profile\n",
      "   Extract structured ZoomInfo company profile data. Requires a valid ZoomInfo company URL.\n",
      "\n",
      "üìä instagram_profiles\n",
      "   Extract structured Instagram profile data. Requires a valid Instagram profile URL.\n",
      "\n",
      "üìä facebook_company_reviews\n",
      "   Extract structured Facebook company reviews. Requires a valid Facebook company URL and num_of_reviews.\n",
      "\n",
      "üìä tiktok_profiles\n",
      "   Extract structured TikTok profile data. Requires a valid TikTok profile URL.\n",
      "\n",
      "üìä youtube_profiles\n",
      "   Extract structured YouTube channel profile data. Requires a valid YouTube channel URL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack_brightdata import BrightDataWebScraper\n",
    "\n",
    "# List all supported datasets\n",
    "datasets = BrightDataWebScraper.get_supported_datasets()\n",
    "\n",
    "print(f\"Total available datasets: {len(datasets)}\\n\")\n",
    "print(\"Sales research relevant datasets:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Filter for relevant datasets\n",
    "relevant_keywords = [\"linkedin\", \"crunchbase\", \"company\", \"profile\"]\n",
    "for dataset in datasets:\n",
    "    if any(keyword in dataset['id'].lower() for keyword in relevant_keywords):\n",
    "        print(f\"üìä {dataset['id']}\")\n",
    "        print(f\"   {dataset['description']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Atlas Setup\n",
    "\n",
    "MongoDB Atlas will serve as our vector database, storing both the embedded representations of our lead data and structured metadata for hybrid search.\n",
    "\n",
    "### Why MongoDB Atlas for Sales Intelligence?\n",
    "\n",
    "- **Vector Search**: Semantic similarity matching to find leads based on meaning, not just keywords\n",
    "- **Metadata Filtering**: Combine semantic search with structured filters (industry, funding stage, location, etc.)\n",
    "- **Scalability**: Handle millions of leads with sub-second query times\n",
    "- **Flexible Schema**: Store varying data structures from different sources (Crunchbase, LinkedIn, news)\n",
    "\n",
    "### Setup Requirements\n",
    "\n",
    "**1. Create a MongoDB Atlas Cluster**\n",
    "\n",
    "If you don't have one yet, follow the [Get Started with Atlas](https://www.mongodb.com/docs/atlas/getting-started/) guide to:\n",
    "- Create a free cluster (M0 tier is sufficient for testing)\n",
    "- Set up database access credentials\n",
    "- Configure network access (allow your IP or use 0.0.0.0/0 for testing)\n",
    "- Get your connection string\n",
    "\n",
    "**2. Create Search Indexes**\n",
    "\n",
    "MongoDB Atlas requires two types of search indexes:\n",
    "\n",
    "**A. Vector Search Index** (for semantic search)\n",
    "\n",
    "1. Go to your cluster in the Atlas UI\n",
    "2. Click on the \"Search\" tab ‚Üí \"Create Search Index\"\n",
    "3. Choose \"Atlas Vector Search\" ‚Üí \"JSON Editor\"\n",
    "4. Use this configuration:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"lead_vector_index\",\n",
    "  \"type\": \"vectorSearch\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 768,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**B. Full-Text Search Index** (for keyword filtering)\n",
    "\n",
    "1. Click \"Create Search Index\" again\n",
    "2. Choose \"Atlas Search\" ‚Üí \"JSON Editor\"\n",
    "3. Use this configuration:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"lead_fulltext_index\",\n",
    "  \"mappings\": {\n",
    "    \"dynamic\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Note**: We're using 768 dimensions for vector search because that's the output size of Google's `text-embedding-004` model.\n",
    "\n",
    "### Data Schema Design\n",
    "\n",
    "Our documents will have the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"content\": \"Company: Example Inc\\nDescription: AI-powered...\\nIndustry: AI/ML...\",\n",
    "  \"embedding\": [0.123, -0.456, ...],  # 768-dimensional vector\n",
    "  \"meta\": {\n",
    "    \"source_url\": \"https://www.crunchbase.com/organization/example\",\n",
    "    \"dataset_type\": \"crunchbase_company\",\n",
    "    \"company_name\": \"Example Inc\",\n",
    "    \"industry\": \"AI/ML\",\n",
    "    \"funding_stage\": \"Series A\",\n",
    "    \"location\": \"New York, NY\",\n",
    "    \"scraped_date\": \"2026-01-19\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This structure allows us to:\n",
    "- Search semantically via the `embedding` field\n",
    "- Filter by metadata (industry, funding stage, location)\n",
    "- Display rich context from the `content` field\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Critical Step: Create the Vector Search Index in Atlas UI\n",
    "\n",
    "\n",
    "The `MongoDBAtlasDocumentStore` expects a pre-configured vector search index. If you skip this step, your queries will return empty results even though documents are stored in the database.\n",
    "\n",
    "1. **Go to MongoDB Atlas UI**:\n",
    "   - Navigate to your cluster \n",
    "   - Click the **\"Search\"** tab (not \"Browse Collections\")\n",
    "\n",
    "2. **Create Vector Search Index**:\n",
    "   - Click \"Create Search Index\"\n",
    "   - Select \"Atlas Vector Search\" ‚Üí \"JSON Editor\"\n",
    "   - Index name: `lead_vector_index`\n",
    "   - Database: `sales_intelligence` (must match your `database_name` parameter)\n",
    "   - Collection: `leads` (must match your `collection_name` parameter)\n",
    "\n",
    "3. **Index Configuration** (paste in JSON editor):\n",
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 768,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "4. **Wait for Index to Build**: The index status should change from \"Building\" to \"Active\" before running queries.\n",
    "\n",
    "**This is how it should look like after setup**: \n",
    "\n",
    "![image.png](../data/ai_sales_research_agent_assets/mongo_setup.png)\n",
    "\n",
    "**Troubleshooting**: If retrieval returns 0 documents:\n",
    "- Verify the index name matches exactly: `lead_vector_index`\n",
    "- Confirm the database and collection names match your code\n",
    "- Check that the index status is \"Active\" (not \"Building\" or \"Failed\")\n",
    "- Ensure `numDimensions: 768` matches the embedding model output\n",
    "\n",
    "Let's initialize the document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MongoDB Atlas DocumentStore initialized\n",
      "   Database: sales_intelligence\n",
      "   Collection: leads\n",
      "   Vector Search Index: lead_vector_index\n",
      "   Full-Text Search Index: lead_fulltext_index\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.document_stores.mongodb_atlas import MongoDBAtlasDocumentStore\n",
    "\n",
    "# Initialize MongoDB Atlas Document Store\n",
    "# Note: It automatically reads from MONGO_CONNECTION_STRING environment variable\n",
    "document_store = MongoDBAtlasDocumentStore(\n",
    "    database_name=\"sales_intelligence\",\n",
    "    collection_name=\"leads\",\n",
    "    vector_search_index=\"lead_vector_index\",\n",
    "    full_text_search_index=\"lead_fulltext_index\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MongoDB Atlas DocumentStore initialized\")\n",
    "print(f\"   Database: sales_intelligence\")\n",
    "print(f\"   Collection: leads\")\n",
    "print(f\"   Vector Search Index: lead_vector_index\")\n",
    "print(f\"   Full-Text Search Index: lead_fulltext_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedders initialized\n",
      "   Document Embedder: text-embedding-004 (768 dimensions)\n",
      "   Text Embedder: text-embedding-004 (768 dimensions)\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.components.embedders.google_genai import GoogleGenAIDocumentEmbedder, GoogleGenAITextEmbedder\n",
    "\n",
    "# Initialize embedders using Google's text-embedding-004 model\n",
    "docs_embedder = GoogleGenAIDocumentEmbedder(model=\"text-embedding-004\")\n",
    "text_embedder = GoogleGenAITextEmbedder(model=\"text-embedding-004\")\n",
    "\n",
    "print(\"‚úÖ Embedders initialized\")\n",
    "print(f\"   Document Embedder: text-embedding-004 (768 dimensions)\")\n",
    "print(f\"   Text Embedder: text-embedding-004 (768 dimensions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model Design\n",
    "\n",
    "Our lead intelligence database uses a flexible schema that accommodates data from multiple sources while enabling powerful hybrid search capabilities.\n",
    "\n",
    "### Document Structure\n",
    "\n",
    "Each document in MongoDB will have three main components:\n",
    "\n",
    "**1. Content Field** (Human-readable text)\n",
    "- Concatenated string containing all relevant information about a company or person\n",
    "- Used for LLM context and display purposes\n",
    "- Example for a company:\n",
    "  ```\n",
    "  Company: Acme AI Inc\n",
    "  Description: Building next-generation machine learning infrastructure\n",
    "  Industry: Artificial Intelligence\n",
    "  Funding Stage: Series A\n",
    "  Total Funding: $15M\n",
    "  Location: San Francisco, CA\n",
    "  Employee Count: 50-100\n",
    "  ```\n",
    "\n",
    "**2. Embedding Field** (768-dimensional vector)\n",
    "- Generated by Google's text-embedding-004 model\n",
    "- Enables semantic similarity search\n",
    "- Captures the meaning and context of the content\n",
    "\n",
    "**3. Metadata Fields** (Structured data for filtering)\n",
    "\n",
    "Our metadata schema supports filtering by multiple dimensions:\n",
    "\n",
    "| Field | Type | Description | Example Values |\n",
    "|-------|------|-------------|----------------|\n",
    "| `source_url` | string | Original source URL | `https://www.crunchbase.com/organization/acme-ai` |\n",
    "| `dataset_type` | string | Type of data source | `crunchbase_company`, `linkedin_person`, `news` |\n",
    "| `company_name` | string | Company name | `Acme AI Inc` |\n",
    "| `industry` | string | Industry/sector | `AI/ML`, `FinTech`, `Healthcare` |\n",
    "| `funding_stage` | string | Investment stage | `Seed`, `Series A`, `Series B`, `Public` |\n",
    "| `location` | string | Geographic location | `New York, NY`, `London, UK` |\n",
    "| `person_name` | string | Person's full name | `John Doe` |\n",
    "| `person_title` | string | Job title | `VP of Engineering`, `CEO` |\n",
    "| `scraped_date` | string | When data was collected | `2026-01-19` |\n",
    "\n",
    "### Hybrid Search Strategy\n",
    "\n",
    "This structure enables three search modes:\n",
    "\n",
    "1. **Semantic Search**: Find similar companies/people based on meaning\n",
    "   - Query: \"AI startups focused on enterprise automation\"\n",
    "   - Matches: Companies with similar descriptions, even if wording differs\n",
    "\n",
    "2. **Metadata Filtering**: Exact match on structured fields\n",
    "   - Filter: `funding_stage = \"Series A\" AND location = \"New York, NY\"`\n",
    "   - Returns: Only companies meeting exact criteria\n",
    "\n",
    "3. **Hybrid Search**: Combine both approaches\n",
    "   - Semantic query: \"Companies building developer tools\"\n",
    "   - + Filters: `funding_stage = \"Series A\"` AND `location = \"San Francisco, CA\"`\n",
    "   - Result: Semantically relevant companies that also match exact criteria\n",
    "\n",
    "### Example Documents\n",
    "\n",
    "**Company Document (Crunchbase):**\n",
    "```python\n",
    "{\n",
    "  \"content\": \"Company: Acme AI\\nIndustry: Artificial Intelligence\\nFunding: $15M Series A...\",\n",
    "  \"embedding\": [0.123, -0.456, ...],  # 768 dimensions\n",
    "  \"meta\": {\n",
    "    \"source_url\": \"https://www.crunchbase.com/organization/acme-ai\",\n",
    "    \"dataset_type\": \"crunchbase_company\",\n",
    "    \"company_name\": \"Acme AI\",\n",
    "    \"industry\": \"AI/ML\",\n",
    "    \"funding_stage\": \"Series A\",\n",
    "    \"location\": \"San Francisco, CA\",\n",
    "    \"scraped_date\": \"2026-01-19\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Person Document (LinkedIn):**\n",
    "```python\n",
    "{\n",
    "  \"content\": \"Name: Jane Smith\\nTitle: VP of Engineering\\nCompany: Acme AI\\nExperience: 10+ years...\",\n",
    "  \"embedding\": [0.234, -0.567, ...],  # 768 dimensions\n",
    "  \"meta\": {\n",
    "    \"source_url\": \"https://www.linkedin.com/in/janesmith\",\n",
    "    \"dataset_type\": \"linkedin_person\",\n",
    "    \"person_name\": \"Jane Smith\",\n",
    "    \"person_title\": \"VP of Engineering\",\n",
    "    \"company\": \"Acme AI\",\n",
    "    \"location\": \"San Francisco, CA\",\n",
    "    \"scraped_date\": \"2026-01-19\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**News Signal Document (SERP):**\n",
    "```python\n",
    "{\n",
    "  \"content\": \"News: Acme AI raises $15M Series A\\nSource: TechCrunch\\nSnippet: AI startup...\",\n",
    "  \"embedding\": [0.345, -0.678, ...],  # 768 dimensions\n",
    "  \"meta\": {\n",
    "    \"source_url\": \"https://techcrunch.com/...\",\n",
    "    \"dataset_type\": \"news\",\n",
    "    \"company_name\": \"Acme AI\",\n",
    "    \"scraped_date\": \"2026-01-19\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This flexible schema allows us to enrich lead profiles with multiple data sources while maintaining fast, accurate search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MongoDB Atlas Retriever initialized\n",
      "   Connected to: leads\n",
      "   Using vector index: lead_vector_index\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.components.retrievers.mongodb_atlas import MongoDBAtlasEmbeddingRetriever\n",
    "\n",
    "# Initialize the retriever for vector search\n",
    "retriever = MongoDBAtlasEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "print(\"‚úÖ MongoDB Atlas Retriever initialized\")\n",
    "print(f\"   Connected to: {document_store.collection_name}\")\n",
    "print(f\"   Using vector index: {document_store.vector_search_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bright Data Scraping Layer\n",
    "\n",
    "Now that we have our database configured, let's set up the data collection layer using Bright Data's web scraping infrastructure.\n",
    "\n",
    "### Why Bright Data?\n",
    "\n",
    "Traditional web scraping approaches face significant challenges:\n",
    "- **IP Blocks**: Websites detect and block scrapers\n",
    "- **CAPTCHAs**: Anti-bot measures prevent automated access\n",
    "- **Geo-Restrictions**: Content varies by location\n",
    "- **Legal Risks**: Non-compliant scraping violates terms of service\n",
    "- **Maintenance**: Website structure changes break scrapers\n",
    "\n",
    "Bright Data solves these problems with:\n",
    "\n",
    "‚úÖ **Compliant Infrastructure**: Respects robots.txt and website policies  \n",
    "‚úÖ **Automatic CAPTCHA Solving**: Handles anti-bot challenges automatically  \n",
    "‚úÖ **Geo-Targeting**: Access content from specific countries/regions  \n",
    "‚úÖ **IP Rotation**: Built-in proxy network prevents blocks  \n",
    "‚úÖ **Structured Data**: Returns clean JSON, not raw HTML  \n",
    "‚úÖ **Maintained Scrapers**: Updates when websites change  \n",
    "\n",
    "### Data Sources for Sales Intelligence\n",
    "\n",
    "For our sales research agent, we'll use three Bright Data capabilities:\n",
    "\n",
    "**1. Web Scraper API** (Structured Data Extraction)\n",
    "- Pre-built scrapers for 45+ platforms\n",
    "- Returns structured JSON data\n",
    "- Datasets we'll use:\n",
    "  - `crunchbase_company`: Funding, investors, employee count\n",
    "  - `linkedin_company_profile`: Industry, size, description\n",
    "  - `linkedin_person_profile`: Decision maker profiles\n",
    "\n",
    "**2. SERP API** (Search Engine Results)\n",
    "- Real-time Google/Bing search results\n",
    "- Geo-targeted queries\n",
    "- Use cases:\n",
    "  - Recent company news and press releases\n",
    "  - Job postings (hiring signals)\n",
    "  - Industry trends and mentions\n",
    "\n",
    "**3. Web Unlocker** (Optional - for custom scraping)\n",
    "- Access any public website\n",
    "- Bypasses bot detection\n",
    "- Returns HTML, markdown, or screenshots\n",
    "- Useful for company blogs, career pages, etc.\n",
    "\n",
    "### Compliance & Ethics\n",
    "\n",
    "Bright Data operates within legal boundaries:\n",
    "- ‚úÖ Only accesses publicly available data\n",
    "- ‚úÖ Respects robots.txt directives\n",
    "- ‚úÖ Complies with GDPR, CCPA regulations\n",
    "- ‚úÖ Does not circumvent authentication or paywalls\n",
    "- ‚úÖ Transparent infrastructure audited by third parties\n",
    "\n",
    "This is especially important given recent LinkedIn lawsuits against scraping services. Bright Data's compliant approach provides legal protection for your sales operations.\n",
    "\n",
    "Let's start scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bright Data Web Scraper initialized\n",
      "   API Key configured: 2dceb1aa0cda2fc6f7f7...\n",
      "   Ready to scrape from 45+ supported datasets\n"
     ]
    }
   ],
   "source": [
    "from haystack_brightdata import BrightDataWebScraper\n",
    "\n",
    "# Initialize the Web Scraper\n",
    "# Note: Automatically uses BRIGHT_DATA_API_KEY from environment\n",
    "scraper = BrightDataWebScraper()\n",
    "\n",
    "print(\"‚úÖ Bright Data Web Scraper initialized\")\n",
    "print(f\"   API Key configured: {os.environ.get('BRIGHT_DATA_API_KEY')[:20]}...\")\n",
    "print(f\"   Ready to scrape from 45+ supported datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Scraping Crunchbase Company Data\n",
    "\n",
    "Let's start by extracting company intelligence from Crunchbase. This gives us funding information, investors, employee count, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Crunchbase data for: https://www.crunchbase.com/organization/openai\n",
      "\n",
      "‚úÖ Successfully scraped company data!\n",
      "\n",
      "üìä Key Information:\n",
      "   Company: OpenAI\n",
      "   Overview: OpenAI is an AI research and deployment company that develops advanced AI models, including ChatGPT.\n",
      "   Industries: Agentic AI, Artificial Intelligence (AI), Foundational AI, Generative AI, Machine Learning, Natural Language Processing, SaaS\n",
      "   Operating Status: active\n",
      "   Website: https://www.openai.com\n",
      "   Employees: 5001-10000\n",
      "   Phone: +1 800-242-8478\n",
      "   Active Tech Count: 79\n",
      "   Tech (sample): DNSSEC, SSL by Default, HSTS, U.S. Server Location, Mobile Non Scaleable Content\n",
      "   Latest News Date: 2026-01-19\n",
      "\n",
      "üìÑ Full data structure (first 500 chars):\n",
      "{\n",
      "  \"name\": \"OpenAI\",\n",
      "  \"url\": \"https://www.crunchbase.com/organization/openai\",\n",
      "  \"id\": \"openai\",\n",
      "  \"cb_rank\": 1,\n",
      "  \"region\": \"California\",\n",
      "  \"about\": \"OpenAI is an AI research and deployment company that develops advanced AI models, including ChatGPT.\",\n",
      "  \"industries\": [\n",
      "    {\n",
      "      \"id\": \"agentic-ai-17fa\",\n",
      "      \"value\": \"Agentic AI\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"artificial-intelligence\",\n",
      "      \"value\": \"Artificial Intelligence (AI)\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"foundational-ai\",\n",
      "      \"value\": \"Fou...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example: Scrape company data from Crunchbase\n",
    "# Replace with an actual Crunchbase company URL you want to research\n",
    "company_url = \"https://www.crunchbase.com/organization/openai\"\n",
    "\n",
    "print(\"Scraping Crunchbase data for: {}\".format(company_url))\n",
    "print()\n",
    "\n",
    "def coalesce(data, *keys, default=\"N/A\"):\n",
    "    for key in keys:\n",
    "        value = data.get(key)\n",
    "        if value not in (None, \"\", [], {}):\n",
    "            return value\n",
    "    return default\n",
    "\n",
    "def format_industries(industries):\n",
    "    if not industries:\n",
    "        return \"N/A\"\n",
    "    if isinstance(industries, list):\n",
    "        values = []\n",
    "        for item in industries:\n",
    "            if isinstance(item, dict):\n",
    "                value = item.get(\"value\") or item.get(\"name\") or item.get(\"id\")\n",
    "                if value:\n",
    "                    values.append(value)\n",
    "            else:\n",
    "                values.append(str(item))\n",
    "        return \", \".join(values) if values else \"N/A\"\n",
    "    return industries\n",
    "\n",
    "def parse_company(result):\n",
    "    raw = result.get(\"data\", result)\n",
    "    if isinstance(raw, str):\n",
    "        raw = json.loads(raw)\n",
    "    if isinstance(raw, list):\n",
    "        return raw[0] if raw else {}\n",
    "    if isinstance(raw, dict):\n",
    "        return raw\n",
    "    return {}\n",
    "\n",
    "try:\n",
    "    result = scraper.run(\n",
    "        dataset=\"crunchbase_company\",\n",
    "        url=company_url\n",
    "    )\n",
    "\n",
    "    company_data = parse_company(result)\n",
    "\n",
    "    industries = format_industries(company_data.get(\"industries\"))\n",
    "    tech_list = company_data.get(\"builtwith_tech\") or company_data.get(\"built_with_tech\") or []\n",
    "    tech_names = [\n",
    "        item.get(\"name\")\n",
    "        for item in tech_list\n",
    "        if isinstance(item, dict) and item.get(\"name\")\n",
    "    ]\n",
    "    tech_preview = \", \".join(tech_names[:5]) if tech_names else \"N/A\"\n",
    "\n",
    "    news_items = company_data.get(\"news\") or []\n",
    "    news_dates = [\n",
    "        item.get(\"date\")\n",
    "        for item in news_items\n",
    "        if isinstance(item, dict) and item.get(\"date\")\n",
    "    ]\n",
    "    latest_news_date = max(news_dates) if news_dates else \"N/A\"\n",
    "\n",
    "    print(\"‚úÖ Successfully scraped company data!\")\n",
    "    print()\n",
    "    print(\"üìä Key Information:\")\n",
    "    print(\"   Company: {}\".format(coalesce(company_data, \"name\", \"legal_name\")))\n",
    "    print(\"   Overview: {}\".format(coalesce(company_data, \"about\", \"company_overview\")))\n",
    "    print(\"   Industries: {}\".format(industries))\n",
    "    print(\"   Operating Status: {}\".format(coalesce(company_data, \"operating_status\")))\n",
    "    print(\"   Website: {}\".format(coalesce(company_data, \"website\", \"url\")))\n",
    "    print(\"   Employees: {}\".format(coalesce(company_data, \"num_employees\", \"number_of_employee_profiles\")))\n",
    "    print(\"   Phone: {}\".format(coalesce(company_data, \"contact_phone\", \"phone_number\")))\n",
    "    print(\n",
    "        \"   Active Tech Count: {}\".format(\n",
    "            coalesce(\n",
    "                company_data,\n",
    "                \"active_tech_count\",\n",
    "                \"builtwith_num_technologies_used\",\n",
    "                \"built_with_num_technologies_used\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"   Tech (sample): {}\".format(tech_preview))\n",
    "    print(\"   Latest News Date: {}\".format(latest_news_date))\n",
    "\n",
    "    print()\n",
    "    print(\"üìÑ Full data structure (first 500 chars):\")\n",
    "    print(json.dumps(company_data, indent=2)[:500] + \"...\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error scraping data: {}\".format(e))\n",
    "    print(\"   This might be due to invalid URL or rate limiting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Scraping Linkedin Company Data\n",
    "\n",
    "Now we will extract company data from Linkedin. This gives us broader information about the requested company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping LinkedIn company data for: https://www.linkedin.com/company/openai/\n",
      "\n",
      "‚úÖ Successfully scraped LinkedIn company data!\n",
      "\n",
      "üìä Key Information:\n",
      "   Company: OpenAI\n",
      "   Description: OpenAI | 9,725,495 followers on LinkedIn. OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. AI is an extremel...\n",
      "   Industry: N/A\n",
      "   Company Size: 201-500 employees\n",
      "   Headquarters: San Francisco, CA\n",
      "   Website: https://openai.com/\n",
      "   Followers: N/A\n",
      "   Specialties: a, r, t, i, f\n",
      "\n",
      "üìÑ Full data structure (first 500 chars):\n",
      "{\n",
      "  \"id\": \"openai\",\n",
      "  \"name\": \"OpenAI\",\n",
      "  \"country_code\": \"US\",\n",
      "  \"locations\": [\n",
      "    \"San Francisco, CA 94110, US\"\n",
      "  ],\n",
      "  \"followers\": 9725495,\n",
      "  \"employees_in_linkedin\": 6891,\n",
      "  \"about\": \"OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. AI is an extremely powerful tool that must be created with safety and human needs at its core. OpenAI is dedicated to putting that alignment of interests first \\u2014 ahe...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example: Scrape LinkedIn company profile\n",
    "# Replace with an actual LinkedIn company URL you want to research\n",
    "linkedin_url = \"https://www.linkedin.com/company/openai/\"\n",
    "\n",
    "print(f\"Scraping LinkedIn company data for: {linkedin_url}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    result = scraper.run(\n",
    "        dataset=\"linkedin_company_profile\",\n",
    "        url=linkedin_url\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    if isinstance(result[\"data\"], str):\n",
    "        company_data = json.loads(result[\"data\"])\n",
    "    else:\n",
    "        company_data = result[\"data\"]\n",
    "    \n",
    "    # Handle list response\n",
    "    if isinstance(company_data, list):\n",
    "        company_data = company_data[0] if company_data else {}\n",
    "    \n",
    "    print(\"‚úÖ Successfully scraped LinkedIn company data!\")\n",
    "    print(\"\\nüìä Key Information:\")\n",
    "    print(f\"   Company: {company_data.get('name', 'N/A')}\")\n",
    "    print(f\"   Description: {company_data.get('description', 'N/A')[:200]}...\")\n",
    "    print(f\"   Industry: {company_data.get('industry', 'N/A')}\")\n",
    "    print(f\"   Company Size: {company_data.get('company_size', 'N/A')}\")\n",
    "    print(f\"   Headquarters: {company_data.get('headquarters', 'N/A')}\")\n",
    "    print(f\"   Website: {company_data.get('website', 'N/A')}\")\n",
    "    print(f\"   Followers: {company_data.get('follower_count', 'N/A')}\")\n",
    "    print(f\"   Specialties: {', '.join(company_data.get('specialties', [])[:5]) if company_data.get('specialties') else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\nüìÑ Full data structure (first 500 chars):\")\n",
    "    print(json.dumps(company_data, indent=2)[:500] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error scraping data: {e}\")\n",
    "    print(\"   This might be due to invalid URL, rate limiting, or authentication requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Scraping LinkedIn Person Profile\n",
    "\n",
    "Now let's extract decision maker profiles from LinkedIn. This helps identify key contacts, their backgrounds, and experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping LinkedIn person profile for: https://www.linkedin.com/in/satyanadella/\n",
      "\n",
      "‚úÖ Successfully scraped LinkedIn person profile!\n",
      "\n",
      "üìä Key Information:\n",
      "   Name: Satya Nadella\n",
      "   Position: Chairman and CEO at Microsoft\n",
      "   Location: Redmond, Washington, United States, US\n",
      "   Current Company: Microsoft\n",
      "   Followers: 11805325\n",
      "   Connections: 500\n",
      "\n",
      "   About: As chairman and CEO of Microsoft, I define my mission and that of my company as empowering every person and every organization on the planet to achieve more....\n",
      "\n",
      "   Experience (5 roles):\n",
      "      1. Chairman and CEO at Microsoft (N/A)\n",
      "      2. Member Board Of Trustees at University of Chicago (N/A)\n",
      "      3. Board Member at Starbucks (N/A)\n",
      "\n",
      "   Education (3 entries):\n",
      "      1. The University of Chicago Booth School of Business (1994-1996)\n",
      "      2. Manipal Institute of Technology, Manipal (-)\n",
      "\n",
      "üìÑ Full data structure (first 500 chars):\n",
      "{\n",
      "  \"id\": \"satyanadella\",\n",
      "  \"name\": \"Satya Nadella\",\n",
      "  \"city\": \"Redmond, Washington, United States\",\n",
      "  \"country_code\": \"US\",\n",
      "  \"position\": \"Chairman and CEO at Microsoft\",\n",
      "  \"about\": \"As chairman and CEO of Microsoft, I define my mission and that of my company as empowering every person and every organization on the planet to achieve more.\",\n",
      "  \"posts\": [\n",
      "    {\n",
      "      \"title\": \"A Positive-Sum Future\",\n",
      "      \"attribution\": \"I\\u2019ve been thinking a lot about what the net benefit of the AI platform...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example: Scrape LinkedIn person profile\n",
    "person_url = \"https://www.linkedin.com/in/satyanadella/\"\n",
    "\n",
    "print(f\"Scraping LinkedIn person profile for: {person_url}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    result = scraper.run(\n",
    "        dataset=\"linkedin_person_profile\",\n",
    "        url=person_url\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    if isinstance(result[\"data\"], str):\n",
    "        person_data = json.loads(result[\"data\"])\n",
    "    else:\n",
    "        person_data = result[\"data\"]\n",
    "    \n",
    "    # Handle list response - LinkedIn returns a list with one person object\n",
    "    if isinstance(person_data, list):\n",
    "        person_data = person_data[0] if person_data else {}\n",
    "    \n",
    "    print(\"‚úÖ Successfully scraped LinkedIn person profile!\")\n",
    "    print(\"\\nüìä Key Information:\")\n",
    "    print(f\"   Name: {person_data.get('name', 'N/A')}\")\n",
    "    print(f\"   Position: {person_data.get('position', 'N/A')}\")\n",
    "    print(f\"   Location: {person_data.get('city', 'N/A')}, {person_data.get('country_code', 'N/A')}\")\n",
    "    \n",
    "    # Current company\n",
    "    current_company = person_data.get('current_company', {})\n",
    "    if current_company:\n",
    "        print(f\"   Current Company: {current_company.get('name', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"   Current Company: N/A\")\n",
    "    \n",
    "    print(f\"   Followers: {person_data.get('followers', 'N/A')}\")\n",
    "    print(f\"   Connections: {person_data.get('connections', 'N/A')}\")\n",
    "    \n",
    "    # About section\n",
    "    about = person_data.get('about')\n",
    "    if about:\n",
    "        print(f\"\\n   About: {about[:200]}...\")\n",
    "    \n",
    "    # Experience\n",
    "    experience = person_data.get('experience', [])\n",
    "    if experience:\n",
    "        print(f\"\\n   Experience ({len(experience)} roles):\")\n",
    "        for i, exp in enumerate(experience[:3]):  # Show first 3 roles\n",
    "            company = exp.get('company', 'N/A')\n",
    "            title = exp.get('title', 'N/A')\n",
    "            duration = exp.get('duration', 'N/A')\n",
    "            print(f\"      {i+1}. {title} at {company} ({duration})\")\n",
    "    \n",
    "    # Education\n",
    "    education = person_data.get('education', [])\n",
    "    if education:\n",
    "        print(f\"\\n   Education ({len(education)} entries):\")\n",
    "        for i, edu in enumerate(education[:2]):  # Show first 2 education entries\n",
    "            title = edu.get('title', 'N/A')\n",
    "            years = f\"{edu.get('start_year', '')}-{edu.get('end_year', '')}\"\n",
    "            print(f\"      {i+1}. {title} ({years})\")\n",
    "    \n",
    "    print(\"\\nüìÑ Full data structure (first 500 chars):\")\n",
    "    print(json.dumps(person_data, indent=2)[:500] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error scraping data: {e}\")\n",
    "    print(\"   This might be due to invalid URL, rate limiting, or authentication requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERP API for Market Signals\n",
    "\n",
    "Beyond direct profile scraping, Bright Data's SERP (Search Engine Results Page) API enables us to gather **market signals** and **company intelligence** through search results.\n",
    "\n",
    "### Why SERP for Sales Intelligence?\n",
    "\n",
    "Traditional profile data (Crunchbase, LinkedIn) gives you a snapshot of a company's current state. But to identify **buying signals** and **pain points**, you need real-time indicators:\n",
    "\n",
    "**1. Hiring Signals** üéØ\n",
    "- Job postings reveal priorities and challenges\n",
    "- \"Hiring VP of Engineering\" ‚Üí scaling technical team\n",
    "- \"Looking for Sales Ops Manager\" ‚Üí experiencing sales friction\n",
    "- Tech stack in job descriptions ‚Üí current tools and potential replacements\n",
    "\n",
    "**2. News & Announcements** üì∞\n",
    "- Funding rounds (expansion mode = budget availability)\n",
    "- Product launches (integration opportunities)\n",
    "- Leadership changes (new decision makers)\n",
    "- Company expansion (new offices, acquisitions)\n",
    "\n",
    "**3. Pain Point Discovery** üîç\n",
    "- Blog posts about challenges\n",
    "- Forum discussions mentioning the company\n",
    "- Press releases about problems solved\n",
    "- Customer reviews and feedback\n",
    "\n",
    "### SERP API vs Web Scraper API\n",
    "\n",
    "| Feature | Web Scraper API | SERP API |\n",
    "|---------|-----------------|----------|\n",
    "| **Use Case** | Extract structured data from specific pages | Find relevant pages and extract snippets |\n",
    "| **Input** | Direct URL to profile/page | Search query |\n",
    "| **Output** | Full structured profile | Search results with titles, URLs, snippets |\n",
    "| **Best For** | Known companies/people | Discovery, monitoring, signals |\n",
    "\n",
    "### Example SERP Queries for Sales Research\n",
    "\n",
    "```python\n",
    "# Hiring signals\n",
    "query = \"site:linkedin.com/jobs \\\"Company Name\\\" engineering\"\n",
    "\n",
    "# Funding news\n",
    "query = \"\\\"Company Name\\\" funding Series A announcement\"\n",
    "\n",
    "# Pain points from job postings\n",
    "query = \"\\\"Company Name\\\" jobs \\\"seeking\\\" OR \\\"looking for\\\" OR \\\"hiring\\\"\"\n",
    "\n",
    "# Technology stack\n",
    "query = \"\\\"Company Name\\\" uses OR powered by OR built with\"\n",
    "\n",
    "# Recent news\n",
    "query = \"\\\"Company Name\\\" news (2024 OR 2025)\"\n",
    "```\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "SERP API returns search results in this format:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"title\": \"Company raises $50M Series B...\",\n",
    "      \"url\": \"https://techcrunch.com/...\",\n",
    "      \"snippet\": \"AI startup Company announced today a $50M Series B...\",\n",
    "      \"date\": \"2025-01-15\"\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This data can be:\n",
    "1. Stored in MongoDB with embeddings for semantic search\n",
    "2. Fed to Gemini for analysis and summarization\n",
    "3. Used to trigger sales alerts (e.g., \"Company just raised funding!\")\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bright Data SERP API initialized\n",
      "   API Key configured: 2dceb1aa0cda2fc6f7f7...\n",
      "   Ready to search Google/Bing for market signals\n"
     ]
    }
   ],
   "source": [
    "from haystack_brightdata import BrightDataSERP\n",
    "\n",
    "# Initialize the SERP API component\n",
    "# Note: Automatically uses BRIGHT_DATA_API_KEY from environment\n",
    "serp = BrightDataSERP()\n",
    "\n",
    "print(\"‚úÖ Bright Data SERP API initialized\")\n",
    "print(f\"   API Key configured: {os.environ.get('BRIGHT_DATA_API_KEY')[:20]}...\")\n",
    "print(f\"   Ready to search Google/Bing for market signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using SERP API to Find Company News\n",
    "\n",
    "Let's use SERP to discover recent news and signals about a company. This is perfect for identifying buying signals like funding announcements, product launches, or hiring initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for recent news about: OpenAI\n",
      "Query: \"OpenAI\" news funding OR announcement OR launch 2025 OR 2026\n",
      "\n",
      "‚úÖ Found 9 results\n",
      "\n",
      "üì∞ Recent News & Signals:\n",
      "\n",
      "1. OpenAI News\n",
      "   URL: https://openai.com/news/\n",
      "   Snippet: Stay up to speed on the rapid advancement of AI technology and the benefits it offers to humanity....\n",
      "\n",
      "2. OpenAI chip deal with Cerebras adds to roster of Nvidia, ...\n",
      "   URL: https://www.cnbc.com/2026/01/16/openai-chip-deal-with-cerebras-adds-to-roster-of-nvidia-amd-broadcom.html\n",
      "   Snippet: In 2025, the ChatGPT maker inked agreements with Nvidia, AMD and Broadcom. Here are the major chip deals that OpenAI has announced as of January ...Re...\n",
      "\n",
      "3. Funding grants for new research into AI and mental health\n",
      "   URL: https://openai.com/index/ai-mental-health-research-grants/\n",
      "   Snippet: As part of our broader safety investments, we are opening a call for research submissions to support independent researchers outside of OpenAI, ...Rea...\n",
      "\n",
      "4. This Is What Convinced Me OpenAI Will Run Out of Money\n",
      "   URL: https://www.nytimes.com/2026/01/13/opinion/openai-ai-bubble-financing.html\n",
      "   Snippet: Companies such as OpenAI are likely to run out of cash before their tantalizing new technology produces big profits. Since the release of ...Read more...\n",
      "\n",
      "5. OpenAI and SoftBank Group partner with SB Energy\n",
      "   URL: https://openai.com/index/stargate-sb-energy-partnership/\n",
      "   Snippet: To support the partnership and as demand for AI compute accelerates, OpenAI and SoftBank Group are each investing $500 million into SB Energy.Read mor...\n",
      "\n",
      "\n",
      "üí° Sales Intelligence Use Cases:\n",
      "   ‚Ä¢ Store these results in MongoDB with embeddings\n",
      "   ‚Ä¢ Use Gemini to summarize key developments\n",
      "   ‚Ä¢ Set up alerts for specific keywords (funding, hiring, launch)\n",
      "   ‚Ä¢ Identify warm leads (companies announcing growth)\n",
      "\n",
      "üìÑ Full data structure (first 500 chars):\n",
      "{\n",
      "  \"general\": {\n",
      "    \"search_engine\": \"google\",\n",
      "    \"query\": \"\\\"OpenAI\\\" news funding OR announcement OR launch 2025 OR 2026\",\n",
      "    \"language\": \"en\",\n",
      "    \"mobile\": false,\n",
      "    \"basic_view\": false,\n",
      "    \"search_type\": \"text\",\n",
      "    \"page_title\": \"\\\"OpenAI\\\" news funding OR announcement OR launch 2025 OR 2026 - Google Search\",\n",
      "    \"timestamp\": \"2026-01-19T17:12:24.103Z\"\n",
      "  },\n",
      "  \"input\": {\n",
      "    \"original_url\": \"https://www.google.com/search?q=%22OpenAI%22+news+funding+OR+announcement+OR+launch+2025+OR+202...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example: Search for recent company news and announcements\n",
    "company_name = \"OpenAI\"\n",
    "search_query = f'\"{company_name}\" news funding OR announcement OR launch 2025 OR 2026'\n",
    "\n",
    "print(f\"Searching for recent news about: {company_name}\")\n",
    "print(f\"Query: {search_query}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    result = serp.run(\n",
    "        query=search_query,\n",
    "        num_results=10\n",
    "    )\n",
    "    \n",
    "    # Parse the results\n",
    "    if isinstance(result[\"results\"], str):\n",
    "        serp_data = json.loads(result[\"results\"])\n",
    "    else:\n",
    "        serp_data = result[\"results\"]\n",
    "    \n",
    "    # Extract organic results (may be at root level or nested)\n",
    "    organic_results = serp_data.get(\"organic\", [])\n",
    "    if not organic_results and \"results\" in serp_data:\n",
    "        organic_results = serp_data.get(\"results\", [])\n",
    "    \n",
    "    if not organic_results:\n",
    "        print(\"‚ö†Ô∏è No results found\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(organic_results)} results\")\n",
    "        print(\"\\nüì∞ Recent News & Signals:\\n\")\n",
    "        \n",
    "        for i, item in enumerate(organic_results[:5], 1):  # Show top 5 results\n",
    "            title = item.get(\"title\", \"N/A\")\n",
    "            link = item.get(\"link\", item.get(\"url\", \"N/A\"))\n",
    "            snippet = item.get(\"snippet\", item.get(\"description\", \"N/A\"))\n",
    "            \n",
    "            print(f\"{i}. {title}\")\n",
    "            print(f\"   URL: {link}\")\n",
    "            print(f\"   Snippet: {snippet[:150]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(\"\\nüí° Sales Intelligence Use Cases:\")\n",
    "        print(\"   ‚Ä¢ Store these results in MongoDB with embeddings\")\n",
    "        print(\"   ‚Ä¢ Use Gemini to summarize key developments\")\n",
    "        print(\"   ‚Ä¢ Set up alerts for specific keywords (funding, hiring, launch)\")\n",
    "        print(\"   ‚Ä¢ Identify warm leads (companies announcing growth)\")\n",
    "    \n",
    "    print(\"\\nüìÑ Full data structure (first 500 chars):\")\n",
    "    print(json.dumps(serp_data, indent=2)[:500] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error searching: {e}\")\n",
    "    print(\"   This might be due to rate limiting or API issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Indexing Pipeline\n",
    "\n",
    "Now that we can scrape data from multiple sources (Crunchbase, LinkedIn, SERP), we need to **process** and **index** it into MongoDB Atlas for semantic search.\n",
    "\n",
    "### The Indexing Pipeline Flow\n",
    "\n",
    "```\n",
    "Raw Scraped Data ‚Üí Document Creation ‚Üí Embedding Generation ‚Üí MongoDB Storage\n",
    "     (JSON)            (Haystack)         (Gemini 768d)         (Vector DB)\n",
    "```\n",
    "\n",
    "### Why Index Data?\n",
    "\n",
    "**Without Indexing:**\n",
    "- Data sits as raw JSON\n",
    "- No semantic search capability\n",
    "- Can't answer questions like \"Find AI startups in NYC\"\n",
    "- No RAG pipeline possible\n",
    "\n",
    "**With Indexing:**\n",
    "- ‚úÖ Semantic search across all companies/people\n",
    "- ‚úÖ Hybrid filtering (vector similarity + metadata)\n",
    "- ‚úÖ Feed relevant context to Gemini for intelligent answers\n",
    "- ‚úÖ Build RAG pipeline for sales intelligence\n",
    "\n",
    "### Document Structure Recap\n",
    "\n",
    "Each document in MongoDB has three components:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"content\": \"Human-readable text about company/person\",\n",
    "  \"embedding\": [0.123, -0.456, ...],  # 768-dimensional vector\n",
    "  \"meta\": {\n",
    "    \"source_url\": \"...\",\n",
    "    \"dataset_type\": \"crunchbase_company\",\n",
    "    \"company_name\": \"...\",\n",
    "    \"industry\": \"...\",\n",
    "    \"funding_stage\": \"...\",\n",
    "    \"location\": \"...\",\n",
    "    \"scraped_date\": \"2026-01-19\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Processing Steps\n",
    "\n",
    "**Step 1: Extract & Transform** üì¶\n",
    "- Take raw JSON from scraper\n",
    "- Extract key fields (name, industry, funding, etc.)\n",
    "- Create human-readable content string\n",
    "- Structure metadata for filtering\n",
    "\n",
    "**Step 2: Generate Embeddings** üß†\n",
    "- Pass content through Google's text-embedding-004\n",
    "- Get 768-dimensional vector representation\n",
    "- Captures semantic meaning for similarity search\n",
    "\n",
    "**Step 3: Store in MongoDB** üíæ\n",
    "- Write document with content, embedding, and metadata\n",
    "- Automatically indexed by vector search index\n",
    "- Ready for hybrid search queries\n",
    "\n",
    "### Indexing Pipeline Component\n",
    "\n",
    "Haystack provides an **indexing pipeline** that automates this flow:\n",
    "\n",
    "```python\n",
    "from haystack import Pipeline\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"embedder\", docs_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", document_store)\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "```\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions: Transform Scraped Data into Haystack Documents\n",
    "\n",
    "Before we can index data, we need to transform raw scraper responses into Haystack `Document` objects. Let's create helper functions for each data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper function created: create_company_documents()\n",
      "   Supports: crunchbase_company, linkedin_company_profile\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from haystack import Document\n",
    "\n",
    "def create_company_documents(scraper_result, source_url, dataset_type):\n",
    "    \"\"\"\n",
    "    Transform company data from Crunchbase or LinkedIn into Haystack Documents.\n",
    "    \n",
    "    Args:\n",
    "        scraper_result: Raw result from BrightDataWebScraper.run()\n",
    "        source_url: Original URL that was scraped\n",
    "        dataset_type: \"crunchbase_company\" or \"linkedin_company_profile\"\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects ready for indexing\n",
    "    \"\"\"\n",
    "    # Parse the JSON response\n",
    "    if isinstance(scraper_result[\"data\"], str):\n",
    "        data = json.loads(scraper_result[\"data\"])\n",
    "    else:\n",
    "        data = scraper_result[\"data\"]\n",
    "    \n",
    "    # Handle both list and single object responses\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    documents = []\n",
    "    scraped_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for item in data:\n",
    "        # Create content string based on dataset type\n",
    "        if dataset_type == \"crunchbase_company\":\n",
    "            content = f\"\"\"Company: {item.get('name', 'N/A')}\n",
    "Overview: {item.get('about', 'N/A')}\n",
    "Industries: {item.get('industries', 'N/A')}\n",
    "Operating Status: {item.get('operating_status', 'N/A')}\n",
    "Location: {item.get('headquarters', 'N/A')}\n",
    "Founded: {item.get('founded_year') or item.get('founded_date', 'N/A')}\n",
    "Employees: {item.get('num_employees', 'N/A')}\n",
    "Website: {item.get('website', 'N/A')}\"\"\"\n",
    "        \n",
    "        elif dataset_type == \"linkedin_company_profile\":\n",
    "            content = f\"\"\"Company: {item.get('name', 'N/A')}\n",
    "About: {item.get('about') or item.get('description', 'N/A')}\n",
    "Industries: {item.get('industries', 'N/A')}\n",
    "Company Size: {item.get('company_size', 'N/A')}\n",
    "Headquarters: {item.get('headquarters', 'N/A')}\n",
    "Founded: {item.get('founded', 'N/A')}\n",
    "Website: {item.get('website', 'N/A')}\n",
    "Followers: {item.get('followers', 'N/A')}\n",
    "Employees on LinkedIn: {item.get('employees_in_linkedin', 'N/A')}\"\"\"\n",
    "        \n",
    "        else:\n",
    "            content = f\"Company: {item.get('name', 'N/A')}\"\n",
    "        \n",
    "        # Extract industry - handle both string and list formats\n",
    "        industries = item.get('industries', item.get('industry', ''))\n",
    "        if isinstance(industries, list):\n",
    "            industries = ', '.join([\n",
    "                ind.get('value', ind) if isinstance(ind, dict) else str(ind)\n",
    "                for ind in industries\n",
    "            ])\n",
    "        \n",
    "        # Create Document with metadata\n",
    "        documents.append(Document(\n",
    "            content=content,\n",
    "            meta={\n",
    "                \"source_url\": source_url,\n",
    "                \"dataset_type\": dataset_type,\n",
    "                \"company_name\": item.get('name', ''),\n",
    "                \"industry\": industries,\n",
    "                \"location\": item.get('headquarters') or item.get('location', ''),\n",
    "                \"scraped_date\": scraped_date\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"‚úÖ Helper function created: create_company_documents()\")\n",
    "print(\"   Supports: crunchbase_company, linkedin_company_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper function created: create_person_documents()\n",
      "   Supports: linkedin_person_profile\n"
     ]
    }
   ],
   "source": [
    "def create_person_documents(scraper_result, source_url):\n",
    "    \"\"\"\n",
    "    Transform LinkedIn person profile data into Haystack Documents.\n",
    "    \n",
    "    Args:\n",
    "        scraper_result: Raw result from BrightDataWebScraper.run()\n",
    "        source_url: Original LinkedIn profile URL\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects ready for indexing\n",
    "    \"\"\"\n",
    "    # Parse the JSON response\n",
    "    if isinstance(scraper_result[\"data\"], str):\n",
    "        data = json.loads(scraper_result[\"data\"])\n",
    "    else:\n",
    "        data = scraper_result[\"data\"]\n",
    "    \n",
    "    # Handle both list and single object responses\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    documents = []\n",
    "    scraped_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for person in data:\n",
    "        # Extract experience summary (first 3 roles)\n",
    "        experience = person.get('experience', [])\n",
    "        experience_summary = []\n",
    "        for i, exp in enumerate(experience[:3]):\n",
    "            company = exp.get('company', 'N/A')\n",
    "            title = exp.get('title', 'N/A')\n",
    "            duration = exp.get('duration', 'N/A')\n",
    "            experience_summary.append(f\"{title} at {company} ({duration})\")\n",
    "        experience_text = '\\n'.join(experience_summary) if experience_summary else 'N/A'\n",
    "        \n",
    "        # Extract education summary\n",
    "        education = person.get('education', [])\n",
    "        education_summary = []\n",
    "        for edu in education[:2]:\n",
    "            title = edu.get('title', 'N/A')\n",
    "            years = f\"{edu.get('start_year', '')}-{edu.get('end_year', '')}\"\n",
    "            education_summary.append(f\"{title} ({years})\")\n",
    "        education_text = '\\n'.join(education_summary) if education_summary else 'N/A'\n",
    "        \n",
    "        # Get current company info\n",
    "        current_company = person.get('current_company', {})\n",
    "        current_company_name = current_company.get('name', 'N/A') if current_company else 'N/A'\n",
    "        \n",
    "        # Create content string\n",
    "        content = f\"\"\"Name: {person.get('name', 'N/A')}\n",
    "Position: {person.get('position', 'N/A')}\n",
    "Current Company: {current_company_name}\n",
    "Location: {person.get('city', 'N/A')}, {person.get('country_code', 'N/A')}\n",
    "About: {person.get('about', 'N/A')}\n",
    "Followers: {person.get('followers', 'N/A')}\n",
    "Connections: {person.get('connections', 'N/A')}\n",
    "\n",
    "Recent Experience:\n",
    "{experience_text}\n",
    "\n",
    "Education:\n",
    "{education_text}\"\"\"\n",
    "        \n",
    "        # Create Document with metadata\n",
    "        documents.append(Document(\n",
    "            content=content,\n",
    "            meta={\n",
    "                \"source_url\": source_url,\n",
    "                \"dataset_type\": \"linkedin_person_profile\",\n",
    "                \"person_name\": person.get('name', ''),\n",
    "                \"person_title\": person.get('position', ''),\n",
    "                \"company\": current_company_name,\n",
    "                \"location\": f\"{person.get('city', '')}, {person.get('country_code', '')}\",\n",
    "                \"scraped_date\": scraped_date\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"‚úÖ Helper function created: create_person_documents()\")\n",
    "print(\"   Supports: linkedin_person_profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Indexing Pipeline\n",
    "\n",
    "Now let's create a Haystack pipeline that automatically:\n",
    "1. Takes Document objects\n",
    "2. Generates embeddings using Gemini\n",
    "3. Writes to MongoDB Atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexing pipeline created\n",
      "\n",
      "Pipeline structure:\n",
      "   Documents ‚Üí Embedder (Gemini text-embedding-004) ‚Üí Writer (MongoDB)\n",
      "\n",
      "Components:\n",
      "   ‚Ä¢ Embedder: GoogleGenAIDocumentEmbedder (768 dimensions)\n",
      "   ‚Ä¢ Writer: MongoDB Atlas (leads)\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.components.embedders.google_genai import GoogleGenAIDocumentEmbedder\n",
    "\n",
    "# Create the indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add components - create a fresh embedder instance for this pipeline\n",
    "indexing_pipeline.add_component(\"embedder\", GoogleGenAIDocumentEmbedder(model=\"text-embedding-004\"))\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "# Connect components\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "print(\"‚úÖ Indexing pipeline created\")\n",
    "print(\"\\nPipeline structure:\")\n",
    "print(\"   Documents ‚Üí Embedder (Gemini text-embedding-004) ‚Üí Writer (MongoDB)\")\n",
    "print(\"\\nComponents:\")\n",
    "print(f\"   ‚Ä¢ Embedder: GoogleGenAIDocumentEmbedder (768 dimensions)\")\n",
    "print(f\"   ‚Ä¢ Writer: MongoDB Atlas ({document_store.collection_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Index Sample Companies\n",
    "\n",
    "Let's test the complete indexing flow by scraping a company and indexing it into MongoDB Atlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collection 'leads' already exists\n",
      "   Current document count: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize the collection in MongoDB if it doesn't exist\n",
    "# This creates the collection and ensures it's ready for indexing\n",
    "\n",
    "try:\n",
    "    # Get the MongoDB client and database\n",
    "    from pymongo import MongoClient\n",
    "    \n",
    "    client = MongoClient(os.environ.get(\"MONGO_CONNECTION_STRING\"))\n",
    "    db = client[document_store.database_name]\n",
    "    \n",
    "    # Create the collection if it doesn't exist\n",
    "    if document_store.collection_name not in db.list_collection_names():\n",
    "        db.create_collection(document_store.collection_name)\n",
    "        print(f\"‚úÖ Created collection '{document_store.collection_name}' in database '{document_store.database_name}'\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Collection '{document_store.collection_name}' already exists\")\n",
    "    \n",
    "    # Count existing documents\n",
    "    collection = db[document_store.collection_name]\n",
    "    doc_count = collection.count_documents({})\n",
    "    print(f\"   Current document count: {doc_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing collection: {e}\")\n",
    "    print(\"   You may need to create the collection manually in MongoDB Atlas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Before indexing, we need to ensure the MongoDB collection exists. Let's initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping company data from https://www.crunchbase.com/organization/openai\n",
      "------------------------------------------------------------\n",
      "‚úÖ Scraping complete\n",
      "\n",
      "Step 2: Transforming into Haystack Documents\n",
      "------------------------------------------------------------\n",
      "‚úÖ Created 1 document(s)\n",
      "\n",
      "Document preview:\n",
      "   Content (first 200 chars): Company: OpenAI\n",
      "Overview: OpenAI is an AI research and deployment company that develops advanced AI models, including ChatGPT.\n",
      "Industries: [{'id': 'agentic-ai-17fa', 'value': 'Agentic AI'}, {'id': 'ar...\n",
      "   Metadata: {'source_url': 'https://www.crunchbase.com/organization/openai', 'dataset_type': 'crunchbase_company', 'company_name': 'OpenAI', 'industry': 'Agentic AI, Artificial Intelligence (AI), Foundational AI, Generative AI, Machine Learning, Natural Language Processing, SaaS', 'location': [{'name': 'San Francisco', 'permalink': 'san-francisco-california'}, {'name': 'California', 'permalink': 'california-united-states'}, {'name': 'United States', 'permalink': 'united-states'}, {'name': 'North America', 'permalink': 'north-america'}], 'scraped_date': '2026-01-19'}\n",
      "\n",
      "Step 3: Generating embeddings and indexing into MongoDB\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexed 1 document(s) into MongoDB\n",
      "\n",
      "üéâ Complete! The company is now searchable in your vector database\n",
      "   ‚Ä¢ Semantic search: Find similar companies\n",
      "   ‚Ä¢ Metadata filters: Filter by industry, location, etc.\n",
      "   ‚Ä¢ RAG pipeline: Answer questions about this company\n"
     ]
    }
   ],
   "source": [
    "# Example: Scrape and index a company from Crunchbase\n",
    "company_url = \"https://www.crunchbase.com/organization/openai\"\n",
    "\n",
    "print(f\"Step 1: Scraping company data from {company_url}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Scrape the company\n",
    "scraper_result = scraper.run(\n",
    "    dataset=\"crunchbase_company\",\n",
    "    url=company_url\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Scraping complete\")\n",
    "\n",
    "# Transform into Haystack Documents\n",
    "print(\"\\nStep 2: Transforming into Haystack Documents\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "documents = create_company_documents(\n",
    "    scraper_result=scraper_result,\n",
    "    source_url=company_url,\n",
    "    dataset_type=\"crunchbase_company\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} document(s)\")\n",
    "print(f\"\\nDocument preview:\")\n",
    "print(f\"   Content (first 200 chars): {documents[0].content[:200]}...\")\n",
    "print(f\"   Metadata: {documents[0].meta}\")\n",
    "\n",
    "# Index into MongoDB\n",
    "print(\"\\nStep 3: Generating embeddings and indexing into MongoDB\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = indexing_pipeline.run({\"embedder\": {\"documents\": documents}})\n",
    "\n",
    "print(f\"‚úÖ Indexed {result['writer']['documents_written']} document(s) into MongoDB\")\n",
    "print(f\"\\nüéâ Complete! The company is now searchable in your vector database\")\n",
    "print(f\"   ‚Ä¢ Semantic search: Find similar companies\")\n",
    "print(f\"   ‚Ä¢ Metadata filters: Filter by industry, location, etc.\")\n",
    "print(f\"   ‚Ä¢ RAG pipeline: Answer questions about this company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline for Sales Intelligence\n",
    "\n",
    "Now that we have company and person data indexed in MongoDB, let's build a **Retrieval-Augmented Generation (RAG)** pipeline to answer sales intelligence questions.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG combines **retrieval** (finding relevant documents) with **generation** (using an LLM to synthesize answers). This enables the AI to answer questions based on your specific data, not just general knowledge.\n",
    "\n",
    "**Without RAG:**\n",
    "- ‚ùå LLM only knows general information\n",
    "- ‚ùå Can't answer \"What companies in my database are in fintech?\"\n",
    "- ‚ùå No personalized sales insights\n",
    "\n",
    "**With RAG:**\n",
    "- ‚úÖ LLM accesses your scraped company/person data\n",
    "- ‚úÖ Answers questions based on real, indexed information\n",
    "- ‚úÖ Provides personalized sales intelligence\n",
    "\n",
    "### RAG Pipeline Architecture\n",
    "\n",
    "```\n",
    "User Question ‚Üí Text Embedder ‚Üí Retriever ‚Üí Prompt Builder ‚Üí LLM Generator ‚Üí Answer\n",
    "    (str)         (768d vec)     (MongoDB)    (Context)       (Gemini)      (str)\n",
    "```\n",
    "\n",
    "**Step-by-step flow:**\n",
    "\n",
    "1. **User Question**: \"Find AI startups in NYC with Series A funding\"\n",
    "\n",
    "2. **Text Embedder**: Converts question to 768-dimensional embedding\n",
    "   - Uses same model as indexing (text-embedding-004)\n",
    "   - Enables semantic similarity matching\n",
    "\n",
    "3. **Retriever**: Searches MongoDB Atlas\n",
    "   - Vector search: Find semantically similar companies\n",
    "   - Metadata filters: Apply exact criteria (location=NYC, funding_stage=Series A)\n",
    "   - Returns top-k most relevant documents\n",
    "\n",
    "4. **Prompt Builder**: Constructs LLM prompt\n",
    "   - Combines user question with retrieved context\n",
    "   - Template: \"Based on these companies: {context}, answer: {question}\"\n",
    "\n",
    "5. **LLM Generator**: Gemini 2.0 synthesizes answer\n",
    "   - Reads retrieved company data\n",
    "   - Generates natural language response\n",
    "   - Cites specific companies and details\n",
    "\n",
    "6. **Answer**: \"I found 3 AI startups in NYC with Series A funding: Company X (raised $10M), Company Y...\"\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**1. Text Embedder** (`GoogleGenAITextEmbedder`)\n",
    "- Input: User's question (string)\n",
    "- Output: 768-dimensional vector\n",
    "- Purpose: Convert query to searchable embedding\n",
    "\n",
    "**2. Retriever** (`MongoDBAtlasEmbeddingRetriever`)\n",
    "- Input: Query embedding + optional filters\n",
    "- Output: Top-k relevant documents\n",
    "- Purpose: Find similar companies/people from vector DB\n",
    "\n",
    "**3. Prompt Builder** (`PromptBuilder`)\n",
    "- Input: Retrieved documents + user question\n",
    "- Output: Formatted prompt for LLM\n",
    "- Purpose: Structure context for generation\n",
    "\n",
    "**4. Generator** (`GoogleGenAIChatGenerator`)\n",
    "- Input: Prompt with context\n",
    "- Output: Natural language answer\n",
    "- Purpose: Synthesize insights from retrieved data\n",
    "\n",
    "### Example Queries\n",
    "\n",
    "Once built, our RAG pipeline can answer:\n",
    "\n",
    "- **Company Discovery**: \"Find AI companies in San Francisco with 50-100 employees\"\n",
    "- **Decision Maker Search**: \"Who are the VPs of Engineering at fintech companies?\"\n",
    "- **Pain Point Analysis**: \"What companies are hiring for data engineering roles?\"\n",
    "- **Personalized Outreach**: \"Generate a sales pitch for Company X based on their profile\"\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini chat generator initialized\n",
      "   Model: gemini-2.0-flash-exp\n",
      "   Purpose: Generate sales intelligence answers from retrieved context\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.components.generators.google_genai import GoogleGenAIChatGenerator\n",
    "\n",
    "# Initialize Gemini chat generator for RAG pipeline\n",
    "# Using gemini-2.0-flash-exp for fast, cost-effective generation\n",
    "generator = GoogleGenAIChatGenerator(model=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "print(\"‚úÖ Gemini chat generator initialized\")\n",
    "print(f\"   Model: gemini-2.0-flash-exp\")\n",
    "print(f\"   Purpose: Generate sales intelligence answers from retrieved context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the RAG Pipeline\n",
    "\n",
    "Now let's assemble all components into a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChatPromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG pipeline created\n",
      "\n",
      "Pipeline structure:\n",
      "   Question ‚Üí Text Embedder ‚Üí Retriever ‚Üí Prompt Builder ‚Üí Generator ‚Üí Answer\n",
      "\n",
      "Components:\n",
      "   ‚Ä¢ Text Embedder: text-embedding-004 (768d)\n",
      "   ‚Ä¢ Retriever: MongoDB Atlas (top_k=5)\n",
      "   ‚Ä¢ Prompt Builder: Sales intelligence template\n",
      "   ‚Ä¢ Generator: gemini-2.0-flash-exp\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_integrations.components.embedders.google_genai import GoogleGenAITextEmbedder\n",
    "\n",
    "# Define the prompt template for sales intelligence\n",
    "system_message = ChatMessage.from_system(\"\"\"\n",
    "You are a sales intelligence assistant. Your role is to analyze company and people data to provide actionable sales intelligence.\n",
    "\n",
    "When answering queries:\n",
    "- Cite specific company names and details from the data\n",
    "- Provide insights relevant for sales outreach\n",
    "- Highlight key information like funding, company size, location, recent news\n",
    "- Suggest talking points for personalized outreach\n",
    "\"\"\")\n",
    "\n",
    "user_template = \"\"\"\n",
    "Based on the following company/person data, answer the user's question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "{{ document.content }}\n",
    "---\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ question }}\n",
    "\n",
    "Provide a detailed, actionable answer based on the retrieved data.\n",
    "\"\"\"\n",
    "\n",
    "user_message = ChatMessage.from_user(user_template)\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "rag_pipeline.add_component(\"text_embedder\", GoogleGenAITextEmbedder(model=\"text-embedding-004\"))\n",
    "rag_pipeline.add_component(\"retriever\", MongoDBAtlasEmbeddingRetriever(document_store=document_store, top_k=5))\n",
    "rag_pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(template=[system_message, user_message]))\n",
    "rag_pipeline.add_component(\"generator\", GoogleGenAIChatGenerator(model=\"gemini-2.0-flash-exp\"))\n",
    "\n",
    "# Connect components\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"generator.messages\")\n",
    "\n",
    "print(\"‚úÖ RAG pipeline created\")\n",
    "print(\"\\nPipeline structure:\")\n",
    "print(\"   Question ‚Üí Text Embedder ‚Üí Retriever ‚Üí Prompt Builder ‚Üí Generator ‚Üí Answer\")\n",
    "print(\"\\nComponents:\")\n",
    "print(\"   ‚Ä¢ Text Embedder: text-embedding-004 (768d)\")\n",
    "print(\"   ‚Ä¢ Retriever: MongoDB Atlas (top_k=5)\")\n",
    "print(\"   ‚Ä¢ Prompt Builder: Sales intelligence template\")\n",
    "print(\"   ‚Ä¢ Generator: gemini-2.0-flash-exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Query the RAG Pipeline\n",
    "\n",
    "Let's test our RAG pipeline with a sales intelligence question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What can you tell me about OpenAI? Include details about their industry, products, and any relevant information for sales outreach.\n",
      "\n",
      "================================================================================\n",
      "Processing...\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "Here's what I can tell you about OpenAI based on the provided data, along with actionable insights for sales outreach:\n",
      "\n",
      "**Company Overview:**\n",
      "\n",
      "*   **Name:** OpenAI\n",
      "*   **Industry:** A leader in the Artificial Intelligence (AI) space, specifically focusing on Agentic AI, Foundational AI, Generative AI, Machine Learning, and Natural Language Processing. They also operate as a SaaS company.\n",
      "*   **Products:** Known for developing advanced AI models, most notably ChatGPT.\n",
      "*   **Operating Status:** Active\n",
      "*   **Employee Size:** 5,001-10,000 employees.\n",
      "*   **Website:** https://www.openai.com\n",
      "\n",
      "**Sales Outreach Insights & Talking Points:**\n",
      "\n",
      "*   **Focus on AI Solutions:** Given OpenAI's focus on AI, tailor your sales pitch to highlight how your product/service can enhance their AI capabilities, improve efficiency in AI development, or address challenges in deploying AI models.\n",
      "*   **ChatGPT Integration/Enhancement:** Consider how your offering could integrate with or enhance ChatGPT. Talking points could include:\n",
      "    *   \"I noticed OpenAI is a leader in Generative AI with ChatGPT. We help companies like \\[Competitor] improve the performance of their GenAI tools.\"\n",
      "    *   \"Our solution can help optimize the infrastructure supporting large language models like ChatGPT, reducing latency and improving user experience.\"\n",
      "*   **Large Organization:** With 5,000-10,000 employees, OpenAI likely has specialized teams and complex workflows. Position your product as a solution that can scale and integrate within a large organization.\n",
      "*   **Industry Leadership:** Acknowledge OpenAI's leadership in the AI field and position your product as a way for them to maintain their competitive edge. For example: \"As pioneers in AI, OpenAI faces unique challenges. Our \\[product/service] is designed to address those challenges...\"\n",
      "*   **SaaS Offering:** Given that OpenAI is in the SaaS business, it would be beneficial to talk about their infrastructure and technology stack.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Retrieved 2 relevant documents from MongoDB\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED DOCUMENTS:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "   Company: OpenAI\n",
      "   Source: crunchbase_company\n",
      "   Location: [{'name': 'San Francisco', 'permalink': 'san-francisco-california'}, {'name': 'California', 'permalink': 'california-united-states'}, {'name': 'United States', 'permalink': 'united-states'}, {'name': 'North America', 'permalink': 'north-america'}]\n",
      "   Industry: Agentic AI, Artificial Intelligence (AI), Foundational AI, Generative AI, Machine Learning, Natural Language Processing, SaaS\n",
      "\n",
      "   Content:\n",
      "   Company: OpenAI\n",
      "Overview: OpenAI is an AI research and deployment company that develops advanced AI models, including ChatGPT.\n",
      "Industries: [{'id': 'agentic-ai-17fa', 'value': 'Agentic AI'}, {'id': 'artificial-intelligence', 'value': 'Artificial Intelligence (AI)'}, {'id': 'foundational-ai', 'value':...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "   Company: OpenAI\n",
      "   Source: crunchbase_company\n",
      "   Location: [{'name': 'San Francisco', 'permalink': 'san-francisco-california'}, {'name': 'California', 'permalink': 'california-united-states'}, {'name': 'United States', 'permalink': 'united-states'}, {'name': 'North America', 'permalink': 'north-america'}]\n",
      "   Industry: Agentic AI, Artificial Intelligence (AI), Foundational AI, Generative AI, Machine Learning, Natural Language Processing, SaaS\n",
      "\n",
      "   Content:\n",
      "   Company: OpenAI\n",
      "Overview: OpenAI is an AI research and deployment company that develops advanced AI models, including ChatGPT.\n",
      "Industries: [{'id': 'agentic-ai-17fa', 'value': 'Agentic AI'}, {'id': 'artificial-intelligence', 'value': 'Artificial Intelligence (AI)'}, {'id': 'foundational-ai', 'value':...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example query: Ask about companies in the database\n",
    "question = \"What can you tell me about OpenAI? Include details about their industry, products, and any relevant information for sales outreach.\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    # Run the RAG pipeline with include_outputs_from to get retriever results\n",
    "    result = rag_pipeline.run(\n",
    "        data={\n",
    "            \"text_embedder\": {\"text\": question},\n",
    "            \"prompt_builder\": {\"question\": question}\n",
    "        },\n",
    "        include_outputs_from={\"retriever\"}\n",
    "    )\n",
    "    \n",
    "    # Extract the answer using .text\n",
    "    answer = result[\"generator\"][\"replies\"][0].text\n",
    "    \n",
    "    print(\"Answer:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Show retrieved documents\n",
    "    if \"retriever\" in result:\n",
    "        retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "        print(f\"\\nüìÑ Retrieved {len(retrieved_docs)} relevant documents from MongoDB\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RETRIEVED DOCUMENTS:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\nDocument {i}:\")\n",
    "            print(f\"   Company: {doc.meta.get('company_name', 'N/A')}\")\n",
    "            print(f\"   Source: {doc.meta.get('dataset_type', 'N/A')}\")\n",
    "            print(f\"   Location: {doc.meta.get('location', 'N/A')}\")\n",
    "            print(f\"   Industry: {doc.meta.get('industry', 'N/A')}\")\n",
    "            print(f\"\\n   Content:\")\n",
    "            print(f\"   {doc.content[:300]}...\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Retriever output not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"   1. Indexed at least one company (run the indexing demo cell)\")\n",
    "    print(\"   2. MongoDB collection exists and has data\")\n",
    "    print(\"   3. Vector search index is properly configured\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
