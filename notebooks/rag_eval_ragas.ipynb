{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJBg1TcQksMo"
      },
      "source": [
        "# RAG pipeline evaluation using Ragas\n",
        "\n",
        "[Ragas](https://docs.ragas.io/en/stable/) is an open source framework for model-based evaluation to evaluate your [Retrieval Augmented Generation](https://www.deepset.ai/blog/llms-retrieval-augmentation) (RAG) pipelines and LLM applications.\n",
        "It supports metrics like correctness, tone, hallucination (faithfulness), fluency, and more.\n",
        "\n",
        "For more information about evaluators, supported metrics and usage, check out:\n",
        "\n",
        "* [RagasEvaluator](https://docs.haystack.deepset.ai/docs/ragasevaluator)\n",
        "* [Model based evaluation](https://docs.haystack.deepset.ai/docs/model-based-evaluation)\n",
        "\n",
        "This notebook shows how to use the [Ragas-Haystack](https://haystack.deepset.ai/integrations/ragas) integration to evaluate a RAG pipeline against various metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL7fWRqIQMP_"
      },
      "source": [
        "Notebook by [*Anushree Bannadabhavi*](https://github.com/AnushreeBannadabhavi), [*Siddharth Sahu*](https://github.com/sahusiddharth), [*Julian Risch*](https://github.com/julian-risch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsTU-Z2QjtwZ"
      },
      "source": [
        "## Prerequisites:\n",
        "\n",
        "- **Ragas** uses [OpenAI](https://openai.com/) key for computing some metrics, so we need an OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18vjNnVwGwjH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxMjhSB-10Ws"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7u0pxUg3JHn"
      },
      "outputs": [],
      "source": [
        "!pip install ragas-haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do4z1oOdF-cM"
      },
      "source": [
        "#### Importing Required Libraries"
      ]
    },
    {
      "metadata": {
        "id": "MhzZtNfD7nW9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 3,
      "source": [
        "from haystack import Document, Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import OpenAITextEmbedder, OpenAIDocumentEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import ChatPromptBuilder\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.builders import AnswerBuilder\n",
        "from haystack_integrations.components.evaluators.ragas import RagasEvaluator\n",
        "\n",
        "from ragas.llms import HaystackLLMWrapper\n",
        "from ragas.metrics import AnswerRelevancy, ContextPrecision, Faithfulness"
      ]
    },
    {
      "metadata": {
        "id": "5eo4Ybc07nW9"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creating a Sample Dataset\n",
        "In this section we create a sample dataset containing information about AI companies and their language models. This dataset serves as the context for retrieving relevant data during pipeline execution."
      ]
    },
    {
      "metadata": {
        "id": "C3VuBdDC7nW-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 4,
      "source": [
        "dataset = [\n",
        "    \"OpenAI is one of the most recognized names in the large language model space, known for its GPT series of models. These models excel at generating human-like text and performing tasks like creative writing, answering questions, and summarizing content. GPT-4, their latest release, has set benchmarks in understanding context and delivering detailed responses.\",\n",
        "    \"Anthropic is well-known for its Claude series of language models, designed with a strong focus on safety and ethical AI behavior. Claude is particularly praised for its ability to follow complex instructions and generate text that aligns closely with user intent.\",\n",
        "    \"DeepMind, a division of Google, is recognized for its cutting-edge Gemini models, which are integrated into various Google products like Bard and Workspace tools. These models are renowned for their conversational abilities and their capacity to handle complex, multi-turn dialogues.\",\n",
        "    \"Meta AI is best known for its LLaMA (Large Language Model Meta AI) series, which has been made open-source for researchers and developers. LLaMA models are praised for their ability to support innovation and experimentation due to their accessibility and strong performance.\",\n",
        "    \"Meta AI with it's LLaMA models aims to democratize AI development by making high-quality models available for free, fostering collaboration across industries. Their open-source approach has been a game-changer for researchers without access to expensive resources.\",\n",
        "    \"Microsoft’s Azure AI platform is famous for integrating OpenAI’s GPT models, enabling businesses to use these advanced models in a scalable and secure cloud environment. Azure AI powers applications like Copilot in Office 365, helping users draft emails, generate summaries, and more.\",\n",
        "    \"Amazon’s Bedrock platform is recognized for providing access to various language models, including its own models and third-party ones like Anthropic’s Claude and AI21’s Jurassic. Bedrock is especially valued for its flexibility, allowing users to choose models based on their specific needs.\",\n",
        "    \"Cohere is well-known for its language models tailored for business use, excelling in tasks like search, summarization, and customer support. Their models are recognized for being efficient, cost-effective, and easy to integrate into workflows.\",\n",
        "    \"AI21 Labs is famous for its Jurassic series of language models, which are highly versatile and capable of handling tasks like content creation and code generation. The Jurassic models stand out for their natural language understanding and ability to generate detailed and coherent responses.\",\n",
        "    \"In the rapidly advancing field of artificial intelligence, several companies have made significant contributions with their large language models. Notable players include OpenAI, known for its GPT Series (including GPT-4); Anthropic, which offers the Claude Series; Google DeepMind with its Gemini Models; Meta AI, recognized for its LLaMA Series; Microsoft Azure AI, which integrates OpenAI’s GPT Models; Amazon AWS (Bedrock), providing access to various models including Claude (Anthropic) and Jurassic (AI21 Labs); Cohere, which offers its own models tailored for business use; and AI21 Labs, known for its Jurassic Series. These companies are shaping the landscape of AI by providing powerful models with diverse capabilities.\",\n",
        "]"
      ]
    },
    {
      "metadata": {
        "id": "yN7wi4GE7nW-"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initializing RAG Pipeline Components\n",
        "This section sets up the essential components required to build a Retrieval-Augmented Generation (RAG) pipeline. These components include a Document Store for managing and storing documents, an Embedder for generating embeddings to enable similarity-based retrieval, and a Retriever for fetching relevant documents. Additionally, a Prompt Template is designed to structure the pipeline's input, while a Chat Generator handles response generation. Together, these components form the backbone of the RAG pipeline, ensuring smooth integration between document retrieval and response generation."
      ]
    },
    {
      "metadata": {
        "id": "f5-2BQi67nW-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Sets up an in-memory store to hold documents\n",
        "document_store = InMemoryDocumentStore()\n",
        "docs = [Document(content=doc) for doc in dataset]\n",
        "\n",
        "# Embeds the documents using OpenAI's embedding models to enable similarity search.\n",
        "document_embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
        "text_embedder = OpenAITextEmbedder(model=\"text-embedding-3-small\")\n",
        "\n",
        "docs_with_embeddings = document_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n",
        "# Configures a retriever to fetch relevant documents based on embeddings\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=2)\n",
        "\n",
        "# Defines a template for prompting the LLM with a user query and the retrieved documents\n",
        "template = [\n",
        "    ChatMessage.from_user(\n",
        "        \"\"\"\n",
        "Given the following information, answer the question.\n",
        "\n",
        "Context:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{question}}\n",
        "Answer:\n",
        "\"\"\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Sets up an LLM-based generator to create responses\n",
        "prompt_builder = ChatPromptBuilder(template=template)\n",
        "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "metadata": {
        "id": "f7q8RbcA7nW-"
      },
      "cell_type": "markdown",
      "source": [
        "#### Configuring RagasEvaluator Component\n",
        "\n",
        "Pass all the Ragas metrics you want to use for evaluation, ensuring that all the necessary information to calculate each selected metric is provided.\n",
        "\n",
        "For example:\n",
        "\n",
        "- **AnswerRelevancy**: requires both the **query** and the **response**. It does not consider factuality but instead assigns lower score to cases where the response lacks completeness or contains redundant details.\n",
        "- **ContextPrecision**: requires the **query**, **retrieved documents**, and the **reference**. It evaluates to what extent the retrieved documents contain precisely only what is relevant to answer the query.\n",
        "- **Faithfulness**: requires the **query**, **retrieved documents**, and the **response**. The response is regarded as faithful if all the claims that are made in the response can be inferred from the retrieved documents.\n",
        "\n",
        "Make sure to include all relevant data for each metric to ensure accurate evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WUE6oacg7nW-"
      },
      "outputs": [],
      "source": [
        "llm = OpenAIGenerator(model=\"gpt-4o-mini\")\n",
        "evaluator_llm = HaystackLLMWrapper(llm)\n",
        "\n",
        "ragas_evaluator = RagasEvaluator(\n",
        "    ragas_metrics=[AnswerRelevancy(), ContextPrecision(), Faithfulness()],\n",
        "    evaluator_llm=evaluator_llm,\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "K3UBR1Nn7nW-"
      },
      "cell_type": "markdown",
      "source": [
        "#### Building and Connecting the RAG Pipeline\n",
        "Here we add and connect the initialized components to form a RAG Haystack pipeline."
      ]
    },
    {
      "metadata": {
        "id": "S4EVs74m7nW-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Creating the Pipeline\n",
        "rag_pipeline = Pipeline()\n",
        "\n",
        "# Adding the components\n",
        "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "rag_pipeline.add_component(\"retriever\", retriever)\n",
        "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag_pipeline.add_component(\"llm\", chat_generator)\n",
        "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
        "rag_pipeline.add_component(\"ragas_evaluator\", ragas_evaluator)\n",
        "\n",
        "# Connecting the components\n",
        "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder\")\n",
        "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
        "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"retriever\", \"ragas_evaluator.documents\")\n",
        "rag_pipeline.connect(\"llm.replies\", \"ragas_evaluator.response\")"
      ]
    },
    {
      "metadata": {
        "id": "NwiyoIJq7nW_",
        "outputId": "0973ca10-8d30-423f-8eab-7896ad588500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "e1ea206079ff46b096bd5944d75f815f",
            "7c9fa15ebb81400f9f8885ef5f30347d",
            "0a32898e49d04efda1b9c03c5b418337",
            "17442107c97a4f6594525c61384e687a",
            "850534e0e95f4734a5ec85788ba5ea67",
            "2126c1f0299242afb296a3090fbb7f4c",
            "71f9eb9f20574a27aa6cfc9e39dc00b8",
            "bafb66ef08f34a26a7a0a5403c9ef040",
            "4c6164616b2440a8b56945654b1fb3dc",
            "9de9ffb57a8745eeaa8c7857b3083526",
            "4bceff1d336d4b928c64364ef2d51d82"
          ]
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ea206079ff46b096bd5944d75f815f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta AI’s LLaMA models stand out for several reasons:\n",
            "\n",
            "1. **Open-Source Accessibility**: The LLaMA models are open-source, allowing researchers and developers to use, modify, and experiment with them freely, which promotes innovation.\n",
            "\n",
            "2. **Strong Performance**: LLaMA models are recognized for their high-performance capabilities, enabling effective application across various tasks and industries.\n",
            "\n",
            "3. **Democratization of AI Development**: By providing these advanced models for free, Meta AI aims to democratize access to AI technology, allowing individuals and smaller organizations to engage in AI development without the barrier of high costs.\n",
            "\n",
            "4. **Fostering Collaboration**: The open-source nature encourages collaboration among researchers and industries, creating a community-driven approach to AI advancement.\n",
            "\n",
            "These factors together position LLaMA models as significant contributors to the AI landscape, especially for those lacking access to expensive proprietary models. \n",
            "\n",
            "{'answer_relevancy': 0.9889, 'context_precision': 1.0000, 'faithfulness': 0.6667}\n"
          ]
        }
      ],
      "execution_count": 8,
      "source": [
        "question = \"What makes Meta AI’s LLaMA models stand out?\"\n",
        "\n",
        "reference = \"Meta AI’s LLaMA models stand out for being open-source, supporting innovation and experimentation due to their accessibility and strong performance.\"\n",
        "\n",
        "\n",
        "result = rag_pipeline.run(\n",
        "    {\n",
        "        \"text_embedder\": {\"text\": question},\n",
        "        \"prompt_builder\": {\"question\": question},\n",
        "        \"answer_builder\": {\"query\": question},\n",
        "        \"ragas_evaluator\": {\"query\": question, \"reference\": reference},\n",
        "        # Each metric expects a specific set of parameters as input. Refer to the\n",
        "        # Ragas class' documentation for more details.\n",
        "    }\n",
        ")\n",
        "\n",
        "print(result['answer_builder']['answers'][0].data, '\\n')\n",
        "print(result['ragas_evaluator']['result'])"
      ]
    },
    {
      "metadata": {
        "id": "-zHu2R9S7nW_"
      },
      "cell_type": "markdown",
      "source": [
        "## Standalone Evaluation of the RAG Pipeline\n",
        "\n",
        "This section explores an alternative approach to evaluating a RAG pipeline without using the `RagasEvaluator` component. It emphasizes manual extraction of outputs and organizing them for evaluation.\n",
        "\n",
        "You can use any existing Haystack pipeline for this purpose. For demonstration, we will create a simple RAG pipeline similar to the one described earlier, but without including the `RagasEvaluator` component.\n",
        "\n",
        "#### Setting Up a Basic RAG Pipeline\n",
        "We construct a simple RAG pipeline similar to the approach above but without the RagasEvaluator component."
      ]
    },
    {
      "metadata": {
        "id": "316LMozo7nW_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Initialize components for RAG pipeline\n",
        "document_store = InMemoryDocumentStore()\n",
        "docs = [Document(content=doc) for doc in dataset]\n",
        "\n",
        "document_embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
        "text_embedder = OpenAITextEmbedder(model=\"text-embedding-3-small\")\n",
        "\n",
        "docs_with_embeddings = document_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store, top_k=2)\n",
        "\n",
        "template = [\n",
        "    ChatMessage.from_user(\n",
        "        \"\"\"\n",
        "Given the following information, answer the question.\n",
        "\n",
        "Context:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{question}}\n",
        "Answer:\n",
        "\"\"\"\n",
        "    )\n",
        "]\n",
        "\n",
        "prompt_builder = ChatPromptBuilder(template=template)\n",
        "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Creating the Pipeline\n",
        "rag_pipeline = Pipeline()\n",
        "\n",
        "# Adding the components\n",
        "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "rag_pipeline.add_component(\"retriever\", retriever)\n",
        "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag_pipeline.add_component(\"llm\", chat_generator)\n",
        "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
        "\n",
        "# Connecting the components\n",
        "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder\")\n",
        "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
        "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
      ]
    },
    {
      "metadata": {
        "id": "bbzXtprA7nW_"
      },
      "cell_type": "markdown",
      "source": [
        "#### Extracting Outputs for Evaluation\n",
        "After building the pipeline, we use it to generate the necessary outputs, such as retrieved documents and responses. These outputs are then structured into a dataset for evaluation.\n"
      ]
    },
    {
      "metadata": {
        "id": "dGQp5XbB7nW_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 11,
      "source": [
        "questions = [\n",
        "    \"Who are the major players in the large language model space?\",\n",
        "    \"What is Microsoft’s Azure AI platform known for?\",\n",
        "    \"What kind of models does Cohere provide?\",\n",
        "]\n",
        "\n",
        "references = [\n",
        "    \"The major players include OpenAI (GPT Series), Anthropic (Claude Series), Google DeepMind (Gemini Models), Meta AI (LLaMA Series), Microsoft Azure AI (integrating GPT Models), Amazon AWS (Bedrock with Claude and Jurassic), Cohere (business-focused models), and AI21 Labs (Jurassic Series).\",\n",
        "    \"Microsoft’s Azure AI platform is known for integrating OpenAI’s GPT models, enabling businesses to use these models in a scalable and secure cloud environment.\",\n",
        "    \"Cohere provides language models tailored for business use, excelling in tasks like search, summarization, and customer support.\",\n",
        "]\n",
        "\n",
        "\n",
        "evals_list = []\n",
        "\n",
        "for que_idx in range(len(questions)):\n",
        "\n",
        "    single_turn = {}\n",
        "    single_turn['user_input'] = questions[que_idx]\n",
        "    single_turn['reference'] = references[que_idx]\n",
        "\n",
        "    # Running the pipeline\n",
        "    response = rag_pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": questions[que_idx]},\n",
        "            \"prompt_builder\": {\"question\": questions[que_idx]},\n",
        "            \"answer_builder\": {\"query\": questions[que_idx]},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # the response of the pipeline\n",
        "    single_turn['response'] = response[\"answer_builder\"][\"answers\"][0].data\n",
        "\n",
        "    haystack_documents = response[\"answer_builder\"][\"answers\"][0].documents\n",
        "    # extracting context from haystack documents\n",
        "    # retrieved durring answer generation process\n",
        "    single_turn['retrieved_contexts'] = [doc.content for doc in haystack_documents]\n",
        "\n",
        "    evals_list.append(single_turn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WM9Nzia7nW_"
      },
      "source": [
        "> When constructing the `evals_list`, it is important to align the keys in the single_turn dictionary with the attributes defined in the Ragas [SingleTurnSample](https://docs.ragas.io/en/stable/references/evaluation_schema/#ragas.dataset_schema.SingleTurnSample). This ensures compatibility with the Ragas evaluation framework. Use the retrieved documents and pipeline outputs to populate these fields accurately, as demonstrated in the provided code snippet.\n",
        "\n",
        "#### Evaluating the pipeline using Ragas EvaluationDataset\n",
        "The extracted dataset is converted into a Ragas [EvaluationDataset](https://docs.ragas.io/en/stable/references/evaluation_schema/#ragas.dataset_schema.EvaluationDataset) so that Ragas can process it.\n",
        "We then initialize an LLM evaluator using the HaystackLLMWrapper. Finally, we call Ragas's evaluate() function with our evaluation dataset, three metrics, and the LLM evaluator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "14c49076c4b3459fbcf839e6996d63cf",
            "bb215d7d08ea441697c63eb0b1b1c28a",
            "2e631498e57b4396bbf448045d9f8650",
            "8142eb2d0afd43d3b9e6d44786759aa4",
            "09802868f9b648f68693beb133770ea1",
            "a9e33a6094ff4997bd9ab46d166b71fc",
            "8b60458f423c475d8555683a0ab26d4a",
            "985e12b121e747babe3c8daea9522dcf",
            "1ac66efb04b34210874f40a90e4a250b",
            "13ea43b8a9e04b568de4f9c89fdd9389",
            "7035623ba85a42bc8e896e88f7090005"
          ]
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14c49076c4b3459fbcf839e6996d63cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer_relevancy': 0.9701, 'context_precision': 1.0000, 'faithfulness': 1.0000}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          user_input  \\\n",
              "0  Who are the major players in the large languag...   \n",
              "1   What is Microsoft’s Azure AI platform known for?   \n",
              "2           What kind of models does Cohere provide?   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [In the rapidly advancing field of artificial ...   \n",
              "1  [Microsoft’s Azure AI platform is famous for i...   \n",
              "2  [Cohere is well-known for its language models ...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The major players in the large language model ...   \n",
              "1  Microsoft’s Azure AI platform is known for int...   \n",
              "2  Cohere provides language models tailored for b...   \n",
              "\n",
              "                                           reference  answer_relevancy  \\\n",
              "0  The major players include OpenAI (GPT Series),...          1.000000   \n",
              "1  Microsoft’s Azure AI platform is known for int...          1.000000   \n",
              "2  Cohere provides language models tailored for b...          0.910337   \n",
              "\n",
              "   context_precision  faithfulness  \n",
              "0                1.0           1.0  \n",
              "1                1.0           1.0  \n",
              "2                1.0           1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d82a6ec-3e30-499d-8b65-887b243b2c31\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>faithfulness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who are the major players in the large languag...</td>\n",
              "      <td>[In the rapidly advancing field of artificial ...</td>\n",
              "      <td>The major players in the large language model ...</td>\n",
              "      <td>The major players include OpenAI (GPT Series),...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is Microsoft’s Azure AI platform known for?</td>\n",
              "      <td>[Microsoft’s Azure AI platform is famous for i...</td>\n",
              "      <td>Microsoft’s Azure AI platform is known for int...</td>\n",
              "      <td>Microsoft’s Azure AI platform is known for int...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What kind of models does Cohere provide?</td>\n",
              "      <td>[Cohere is well-known for its language models ...</td>\n",
              "      <td>Cohere provides language models tailored for b...</td>\n",
              "      <td>Cohere provides language models tailored for b...</td>\n",
              "      <td>0.910337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d82a6ec-3e30-499d-8b65-887b243b2c31')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5d82a6ec-3e30-499d-8b65-887b243b2c31 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5d82a6ec-3e30-499d-8b65-887b243b2c31');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00d5629e-9aec-4713-8afa-9d006d1acbfc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00d5629e-9aec-4713-8afa-9d006d1acbfc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00d5629e-9aec-4713-8afa-9d006d1acbfc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Who are the major players in the large language model space?\",\n          \"What is Microsoft\\u2019s Azure AI platform known for?\",\n          \"What kind of models does Cohere provide?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"The major players in the large language model space are:\\n\\n1. OpenAI - Known for its GPT Series, including GPT-4.\\n2. Anthropic - Offers the Claude Series.\\n3. Google DeepMind - Known for its Gemini Models.\\n4. Meta AI - Recognized for its LLaMA Series.\\n5. Microsoft Azure AI - Integrates OpenAI\\u2019s GPT Models.\\n6. Amazon AWS (Bedrock) - Provides access to various models including Claude (Anthropic) and Jurassic (AI21 Labs).\\n7. Cohere - Offers its own models tailored for business use.\\n8. AI21 Labs - Known for its Jurassic Series.\\n\\nThese companies are significantly shaping the AI landscape with their diverse and powerful models.\",\n          \"Microsoft\\u2019s Azure AI platform is known for integrating OpenAI\\u2019s GPT models, allowing businesses to utilize these advanced models in a scalable and secure cloud environment. It powers applications like Copilot in Office 365, assisting users in tasks such as drafting emails and generating summaries.\",\n          \"Cohere provides language models tailored for business use, excelling in tasks such as search, summarization, and customer support. These models are noted for being efficient, cost-effective, and easy to integrate into workflows.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"The major players include OpenAI (GPT Series), Anthropic (Claude Series), Google DeepMind (Gemini Models), Meta AI (LLaMA Series), Microsoft Azure AI (integrating GPT Models), Amazon AWS (Bedrock with Claude and Jurassic), Cohere (business-focused models), and AI21 Labs (Jurassic Series).\",\n          \"Microsoft\\u2019s Azure AI platform is known for integrating OpenAI\\u2019s GPT models, enabling businesses to use these models in a scalable and secure cloud environment.\",\n          \"Cohere provides language models tailored for business use, excelling in tasks like search, summarization, and customer support.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05176703994646551,\n        \"min\": 0.9103368566552751,\n        \"max\": 1.000000000000001,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.000000000000001,\n          1.0000000000000009,\n          0.9103368566552751\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8867515847990063e-11,\n        \"min\": 0.9999999999,\n        \"max\": 0.99999999995,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.99999999995,\n          0.9999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12,
      "source": [
        "from ragas import evaluate\n",
        "from ragas.dataset_schema import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_list(evals_list)\n",
        "\n",
        "llm = OpenAIGenerator(model=\"gpt-4o-mini\")\n",
        "evaluator_llm = HaystackLLMWrapper(llm)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[AnswerRelevancy(), ContextPrecision(), Faithfulness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "print(result)\n",
        "result.to_pandas()"
      ]
    },
    {
      "metadata": {
        "id": "lZK1B27I7nXA"
      },
      "cell_type": "markdown",
      "source": [
        "**Haystack Useful Sources**\n",
        "\n",
        "* [Docs](https://docs.haystack.deepset.ai/docs/intro)\n",
        "* [Tutorials](https://haystack.deepset.ai/tutorials)\n",
        "* [Other Cookbooks](https://github.com/deepset-ai/haystack-cookbook)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1ea206079ff46b096bd5944d75f815f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c9fa15ebb81400f9f8885ef5f30347d",
              "IPY_MODEL_0a32898e49d04efda1b9c03c5b418337",
              "IPY_MODEL_17442107c97a4f6594525c61384e687a"
            ],
            "layout": "IPY_MODEL_850534e0e95f4734a5ec85788ba5ea67"
          }
        },
        "7c9fa15ebb81400f9f8885ef5f30347d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2126c1f0299242afb296a3090fbb7f4c",
            "placeholder": "​",
            "style": "IPY_MODEL_71f9eb9f20574a27aa6cfc9e39dc00b8",
            "value": "Evaluating: 100%"
          }
        },
        "0a32898e49d04efda1b9c03c5b418337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bafb66ef08f34a26a7a0a5403c9ef040",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c6164616b2440a8b56945654b1fb3dc",
            "value": 3
          }
        },
        "17442107c97a4f6594525c61384e687a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9de9ffb57a8745eeaa8c7857b3083526",
            "placeholder": "​",
            "style": "IPY_MODEL_4bceff1d336d4b928c64364ef2d51d82",
            "value": " 3/3 [00:13&lt;00:00,  5.02s/it]"
          }
        },
        "850534e0e95f4734a5ec85788ba5ea67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2126c1f0299242afb296a3090fbb7f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f9eb9f20574a27aa6cfc9e39dc00b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bafb66ef08f34a26a7a0a5403c9ef040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c6164616b2440a8b56945654b1fb3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9de9ffb57a8745eeaa8c7857b3083526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bceff1d336d4b928c64364ef2d51d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14c49076c4b3459fbcf839e6996d63cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb215d7d08ea441697c63eb0b1b1c28a",
              "IPY_MODEL_2e631498e57b4396bbf448045d9f8650",
              "IPY_MODEL_8142eb2d0afd43d3b9e6d44786759aa4"
            ],
            "layout": "IPY_MODEL_09802868f9b648f68693beb133770ea1"
          }
        },
        "bb215d7d08ea441697c63eb0b1b1c28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9e33a6094ff4997bd9ab46d166b71fc",
            "placeholder": "​",
            "style": "IPY_MODEL_8b60458f423c475d8555683a0ab26d4a",
            "value": "Evaluating: 100%"
          }
        },
        "2e631498e57b4396bbf448045d9f8650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_985e12b121e747babe3c8daea9522dcf",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ac66efb04b34210874f40a90e4a250b",
            "value": 9
          }
        },
        "8142eb2d0afd43d3b9e6d44786759aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ea43b8a9e04b568de4f9c89fdd9389",
            "placeholder": "​",
            "style": "IPY_MODEL_7035623ba85a42bc8e896e88f7090005",
            "value": " 9/9 [00:20&lt;00:00,  3.53s/it]"
          }
        },
        "09802868f9b648f68693beb133770ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9e33a6094ff4997bd9ab46d166b71fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b60458f423c475d8555683a0ab26d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "985e12b121e747babe3c8daea9522dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac66efb04b34210874f40a90e4a250b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13ea43b8a9e04b568de4f9c89fdd9389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7035623ba85a42bc8e896e88f7090005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
