{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjlwUPWugM37"
   },
   "source": [
    "# Use ChromaDocumentStore with Haystack\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "135w48jbgRRU"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znSRD-hO2doM"
   },
   "outputs": [],
   "source": [
    "# Install the Chroma integration, Haystack will come as a dependency\n",
    "!pip install -U chroma-haystack \"huggingface_hub>=0.22.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt_XhGXBgU-I"
   },
   "source": [
    "## Indexing Pipeline: preprocess, split and index documents\n",
    "In this section, we will index documents into a Chroma DB collection by building a Haystack indexing pipeline. Here, we are indexing documents from the [VIM User Manuel](https://vimhelp.org/) into the Haystack `ChromaDocumentStore`.\n",
    "\n",
    " We have the `.txt` files for these pages in the examples folder for the `ChromaDocumentStore`, so we are using the [`TextFileToDocument`](https://docs.haystack.deepset.ai/v2.0/docs/textfiletodocument) and [`DocumentWriter`](https://docs.haystack.deepset.ai/v2.0/docs/documentwriter) components to build this indexing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGxsA9C74BWr"
   },
   "outputs": [],
   "source": [
    "# Fetch data files from the Github repo\n",
    "!curl -sL https://github.com/deepset-ai/haystack-core-integrations/tarball/main -o main.tar\n",
    "!mkdir main\n",
    "!tar xf main.tar -C main --strip-components 1\n",
    "!mv main/integrations/chroma/example/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayyBKQIC3jGo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "\n",
    "file_paths = [\"data\" / Path(name) for name in os.listdir(\"data\")]\n",
    "\n",
    "# Chroma is used in-memory so we use the same instances in the two pipelines below\n",
    "document_store = ChromaDocumentStore()\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"converter\", TextFileToDocument())\n",
    "indexing.add_component(\"writer\", DocumentWriter(document_store))\n",
    "indexing.connect(\"converter\", \"writer\")\n",
    "indexing.run({\"converter\": {\"sources\": file_paths}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44cRT55agw2e"
   },
   "source": [
    "## Query Pipeline: build retrieval-augmented generation (RAG) pipelines\n",
    "\n",
    "Once we have documents in the `ChromaDocumentStore`, we can use the accompanying Chroma retrievers to build a query pipeline. The query pipeline below is a simple retrieval-augmented generation (RAG) pipeline that uses Chroma's [query API](https://docs.trychroma.com/usage-guide#querying-a-collection).\n",
    "\n",
    "You can change the idnexing pipeline and query pipelines here for embedding search by using one of the [`Haystack Embedders`](https://docs.haystack.deepset.ai/v2.0/docs/embedders) accompanied by the  `ChromaEmbeddingRetriever`.\n",
    "\n",
    "\n",
    "In this example we are using:\n",
    "- The `HuggingFaceAPIGenerator` with zephyr-7b-beta. (You will need a Hugging Face token to use this model). You can replace this with any of the other [`Generators`](https://docs.haystack.deepset.ai/v2.0/docs/generators)\n",
    "- The `PromptBuilder` which holds the prompt template. You can adjust this to a prompt of your choice\n",
    "- The `ChromaQueryRetriver` which expects a list of queries and retieves the `top_k` most relevant documents from your Chroma collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGGApIR3pllW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter Hugging Face API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the query based on the provided context.\n",
    "If the context does not contain the answer, say 'Answer not found'.\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "{% endfor %}\n",
    "query: {{query}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder = PromptBuilder(template=prompt)\n",
    "\n",
    "llm = HuggingFaceAPIGenerator(api_type=\"serverless_inference_api\",\n",
    "                              api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"})\n",
    "\n",
    "retriever = ChromaQueryTextRetriever(document_store)\n",
    "\n",
    "querying = Pipeline()\n",
    "querying.add_component(\"retriever\", retriever)\n",
    "querying.add_component(\"prompt_builder\", prompt_builder)\n",
    "querying.add_component(\"llm\", llm)\n",
    "\n",
    "querying.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "querying.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8jcmcdqrGu1"
   },
   "outputs": [],
   "source": [
    "query = \"Should I write documentation for my plugin?\"\n",
    "results = querying.run({\"retriever\": {\"query\": query, \"top_k\": 3},\n",
    "                        \"prompt_builder\": {\"query\": query},\n",
    "                        \"llm\":{\"generation_kwargs\": {\"max_new_tokens\": 350}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa7f7EzjtBXw"
   },
   "outputs": [],
   "source": [
    "print(results[\"llm\"][\"replies\"][0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
