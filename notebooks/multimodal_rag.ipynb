{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bdffd7-8ca7-403a-a8b6-eb1b20a9f22d",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Introduction to Multimodal RAG\n",
    "\n",
    "In this notebook, you'll learn how to index and retrieve images using Haystack. By the end, you'll be able to build a Retrieval-Augmented Generation (RAG) pipeline that can answer questions grounded in both images and text. This is useful when working with datasets like scientific papers, diagrams, or screenshots where meaning is spread across modalities.\n",
    "\n",
    "This tutorial uses the following **new components** that enable image indexing:\n",
    "\n",
    "- `SentenceTransformersDocumentImageEmbedder`: Embed image documents with CLIP-based models\n",
    "- `ImageFileToDocument`: Convert image files into Haystack `Document`s\n",
    "- `DocumentTypeRouter`: Route retrieved documents by mime type (e.g., image vs text)\n",
    "- `DocumentToImageContent`: Convert image documents into `ImageContent` to be processed by our ChatGenerator\n",
    "\n",
    "In this notebook, we'll introduce all these features, show an application using **image + text retrieval + multimodal generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddacd09-5725-4979-a8df-13786b7ed2ee",
   "metadata": {},
   "source": [
    "## Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44239d-8a00-4d79-b372-b495c4cdee6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade \"haystack-experimental\" pillow pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfc3365-9e50-4e17-85bc-32c181dffb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from pprint import pp as print\n",
    "\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46249a-ab6a-4a77-b230-b0e3b94d6d99",
   "metadata": {},
   "source": [
    "## Introduction to Embedding Images\n",
    "\n",
    "Let's compare the similarity between a text and two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff5def-ce04-42fd-a375-ae80e91c91f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget \"https://upload.wikimedia.org/wikipedia/commons/2/26/Pink_Lady_Apple_%284107712628%29.jpg\" -O apple.jpg\n",
    "!wget \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Cattle_tyrant_%28Machetornis_rixosa%29_on_Capybara.jpg/960px-Cattle_tyrant_%28Machetornis_rixosa%29_on_Capybara.jpg?download\" -O capybara.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4935a4d-a961-4350-9a6e-096fd9d9db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.components.converters.image import ImageFileToDocument\n",
    "\n",
    "image_file_converter = ImageFileToDocument()\n",
    "image_docs = image_file_converter.run(sources=[\"apple.jpg\", \"capybara.jpg\"])[\"documents\"]\n",
    "print(image_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cca659-c6f9-43ba-a240-5cacac9a7c36",
   "metadata": {},
   "source": [
    "Next we load our embedders. It's important that we use the same CLIP model for both text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1eb686-47d0-4f22-8305-9732b6cbee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.embedders.sentence_transformers_text_embedder import SentenceTransformersTextEmbedder\n",
    "from haystack_experimental.components.embedders.image.sentence_transformers_doc_image_embedder import (\n",
    "    SentenceTransformersDocumentImageEmbedder,\n",
    ")\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/clip-ViT-L-14\", progress_bar=False)\n",
    "image_embedder = SentenceTransformersDocumentImageEmbedder(model=\"sentence-transformers/clip-ViT-L-14\", progress_bar=False)\n",
    "\n",
    "# Warm up the models to load them\n",
    "text_embedder.warm_up()\n",
    "image_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd64407b-9431-41aa-8f7e-d1f4d636d0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e408d256073f4167b7edbf21645e73d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd73e18b57f24cfea2e3fd8b4fd8ff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Similarity with apple.jpg: 0.27'\n",
      "'Similarity with capybara.jpg: 0.07'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "query = \"A red apple on a white background\"\n",
    "text_embedding = text_embedder.run(text=query)[\"embedding\"]\n",
    "image_docs_with_embeddings = image_embedder.run(image_docs)[\"documents\"]\n",
    "\n",
    "# Compare the similarities between the query and two image documents\n",
    "for doc in image_docs_with_embeddings:\n",
    "    similarity = util.cos_sim(torch.tensor(text_embedding), torch.tensor(doc.embedding))\n",
    "    print(f\"Similarity with {doc.meta['file_path'].split('/')[-1]}: {similarity.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76248dd0-d1fc-4cea-a2ad-3a9f0629c278",
   "metadata": {},
   "source": [
    "As we can see the text is most similar to our Apple image!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065b946-a71d-4129-a5cc-232b97c32db3",
   "metadata": {},
   "source": [
    "## Building an Image & Text Indexing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe53645-4453-4868-9886-bafc1f65e6f2",
   "metadata": {},
   "source": [
    "First let's also download a sample PDF file to see how we can retrieve over both text and image based documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d723c01-6fc8-40be-ba19-b2e7f98e8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: missing URL\n",
      "Usage: wget [OPTION]... [URL]...\n",
      "\n",
      "Try `wget --help' for more options.\n"
     ]
    }
   ],
   "source": [
    "!wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d0179-259d-4f2d-8c0a-3c9e2fe93e00",
   "metadata": {},
   "source": [
    "### Manually Embed Text and Image Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d5f03ce-10ee-4ba1-9b4d-dc798feba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from haystack.components.converters.pypdf import PyPDFToDocument\n",
    "from haystack.components.embedders.sentence_transformers_document_embedder import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.writers.document_writer import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d111ab-ec18-48de-848b-7653af9fcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our document store\n",
    "doc_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "# Define our components\n",
    "image_converter = ImageFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "pdf_splitter = DocumentSplitter(split_by=\"page\", split_length=1)\n",
    "text_doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/clip-ViT-L-14\")\n",
    "\n",
    "text_doc_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342960c3-07d5-40df-9c64-e94a257ed07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-07-03T09:44:21.727630Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCould not read sample.pdf. Skipping it. Error: [Errno 2] No such file or directory: 'sample.pdf'\u001b[0m \u001b[36merror\u001b[0m=\u001b[35mFileNotFoundError(2, 'No such file or directory')\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m203\u001b[0m \u001b[36mmodule\u001b[0m=\u001b[35mhaystack.components.converters.pypdf\u001b[0m \u001b[36msource\u001b[0m=\u001b[35msample.pdf\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4991396cb934ae1a25b0bd72e33dd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243caf909377420a91950ee2bbdae484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create our pdf + image documents\n",
    "pdf_docs = pdf_converter.run(sources=[\"sample.pdf\"])[\"documents\"]\n",
    "split_pdf_docs = pdf_splitter.run(documents=pdf_docs)[\"documents\"]\n",
    "image_docs = image_converter.run(sources=[\"apple.jpg\"])[\"documents\"]\n",
    "\n",
    "# Embed our text\n",
    "pdf_docs_with_embeddings = text_doc_embedder.run(split_pdf_docs)['documents']\n",
    "img_docs_with_embeddings = image_embedder.run(image_docs)['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d470bd0f-fbba-45db-bdeb-8c3c8339f97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write our documents to the document store\n",
    "doc_store.write_documents(pdf_docs_with_embeddings + img_docs_with_embeddings, policy=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6770f-50f4-4139-863d-413cd9e2feba",
   "metadata": {},
   "source": [
    "### Create an Indexing Pipeline to Process All Files at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71faa6c6-aaf5-4a4f-82f8-35a0d76e6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from haystack.components.routers.file_type_router import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90957ede-b1a3-4237-9a55-210a74c38469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional component definitions\n",
    "file_type_router = FileTypeRouter(mime_types=[\"application/pdf\", \"image/jpeg\"])\n",
    "final_doc_joiner = DocumentJoiner(sort_by_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17f4eb7a-96c1-4e2a-ab7c-14eeb85deb5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineError",
     "evalue": "Component has already been added in another Pipeline. Components can't be shared between Pipelines. Create a new instance instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m indexing_pipe \u001b[38;5;241m=\u001b[39m Pipeline()\n\u001b[1;32m      5\u001b[0m indexing_pipe\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type_router\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_type_router)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mindexing_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_component\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdf_converter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_converter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m indexing_pipe\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_splitter\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdf_splitter)\n\u001b[1;32m      8\u001b[0m indexing_pipe\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_converter\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_converter)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/haystack/lib/python3.10/site-packages/haystack/core/pipeline/base.py:366\u001b[0m, in \u001b[0;36mPipelineBase.add_component\u001b[0;34m(self, name, instance)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__haystack_added_to_pipeline__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    362\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComponent has already been added in another Pipeline. Components can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be shared between Pipelines. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a new instance instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineError(msg)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__haystack_added_to_pipeline__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__component_name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n",
      "\u001b[0;31mPipelineError\u001b[0m: Component has already been added in another Pipeline. Components can't be shared between Pipelines. Create a new instance instead."
     ]
    }
   ],
   "source": [
    "# Create the Indexing Pipeline\n",
    "from haystack import Pipeline\n",
    "\n",
    "indexing_pipe = Pipeline()\n",
    "indexing_pipe.add_component(\"file_type_router\", file_type_router)\n",
    "indexing_pipe.add_component(\"pdf_converter\", pdf_converter)\n",
    "indexing_pipe.add_component(\"pdf_splitter\", pdf_splitter)\n",
    "indexing_pipe.add_component(\"image_converter\", image_converter)\n",
    "indexing_pipe.add_component(\"text_doc_embedder\", text_doc_embedder)\n",
    "indexing_pipe.add_component(\"image_doc_embedder\", image_embedder)\n",
    "indexing_pipe.add_component(\"final_doc_joiner\", final_doc_joiner)\n",
    "indexing_pipe.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "indexing_pipe.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "indexing_pipe.connect(\"pdf_converter.documents\", \"pdf_splitter.documents\")\n",
    "indexing_pipe.connect(\"pdf_splitter.documents\", \"text_doc_embedder.documents\")\n",
    "indexing_pipe.connect(\"file_type_router.image/jpeg\", \"image_converter.sources\")\n",
    "indexing_pipe.connect(\"image_converter.documents\", \"image_doc_embedder.documents\")\n",
    "indexing_pipe.connect(\"text_doc_embedder.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe.connect(\"image_doc_embedder.documents\", \"final_doc_joiner.documents\")\n",
    "indexing_pipe.connect(\"final_doc_joiner.documents\", \"document_writer.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42687c4e-e64f-4784-998e-cb1f526b5cf6",
   "metadata": {},
   "source": [
    "Visualize the Indexing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef301d8e-bbee-4fdb-91a3-925612d93aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipe.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed368e8-c639-4b32-ad33-464099f19649",
   "metadata": {},
   "source": [
    "Run the indexing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cd7b9-8f3c-4985-a1ec-0ef55a1df4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_result = indexing_pipe.run(\n",
    "    data={\"file_type_router\": {\"sources\": [\"sample.pdf\", \"apple.jpg\"]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607387c4-8a8e-49fe-b2cc-32b291879fbd",
   "metadata": {},
   "source": [
    "Inspect the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fad8e-a8de-4bb8-b7e0-b01957407192",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_documents = document_store.filter_documents()\n",
    "print(f\"Indexed {len(indexed_documents)} documents:\")\n",
    "print(indexed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8cbb2-7b21-4e24-b032-e0da31752cb2",
   "metadata": {},
   "source": [
    "## Retrieval ‚Äì Searching Image + Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f966e35-0916-4f00-90ee-0724d044a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = InMemoryEmbeddingRetriever(document_store=doc_store)\n",
    "results = retriever.run(query=\"An image of an apple\")['documents']\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.content[:100], doc.content_type, doc.meta.get(\"file_path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11f1cb-9e9e-4180-9a27-065fa432927d",
   "metadata": {},
   "source": [
    "## RAG with Image + Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab4e88-060a-419d-b985-33067bde2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Answer this query: {{ query }}\n",
    "\"\"\"\n",
    "\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=doc_store, top_k=3))\n",
    "rag.add_component(\"router\", DocumentTypeRouter())\n",
    "rag.add_component(\"img_convert\", DocumentToImageContent(detail=\"low\"))\n",
    "rag.add_component(\"prompt\", PromptBuilder(template=prompt_template))\n",
    "rag.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "rag.connect(\"retriever.documents\", \"router.documents\")\n",
    "rag.connect(\"router.image\", \"img_convert.documents\")\n",
    "rag.connect(\"retriever.documents\", \"prompt.documents\")\n",
    "rag.connect(\"img_convert.image_contents\", \"prompt.image_contents\")\n",
    "rag.connect(\"prompt.prompt\", \"llm.messages\")\n",
    "\n",
    "response = rag.run({\"query\": \"What does the image of the apple show?\"})\n",
    "print(response[\"llm\"][\"replies\"][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062bb1a-8b8c-4e9a-aa41-c84a9b02b200",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "You can follow the progress of the Multimodal experiment in this [GitHub issue](https://github.com/deepset-ai/haystack/issues/8976).\n",
    "\n",
    "(*Notebook by [Sebastian Husch Lee](https://github.com/sjrl)*)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
