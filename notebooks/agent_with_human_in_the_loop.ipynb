{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ DevOps Support Agent with Human in the Loop\n",
    "\n",
    "This notebook demonstrates how a [Haystack Agent](https://docs.haystack.deepset.ai/docs/agents) can interactively ask for user input when it's uncertain about the next step. We'll build a **Human-in-the-Loop** tool that the Agent can call dynamically. When the Agent encounters ambiguity or incomplete information, it will ask for more input from the human to continue solving the task.\n",
    "\n",
    "For this purpose, we will create a **DevOps Support Agent**.\n",
    "\n",
    "CI/CD pipelines occasionally fail for reasons that can be hard to diagnose including manuallyâ€”broken tests, mis-configured environment variables, flaky integrations, etc. \n",
    "\n",
    "The support Agent is created using Haystack [Tools](https://docs.haystack.deepset.ai/docs/tool) and [Agent](https://docs.haystack.deepset.ai/docs/agent) and will perform the following tasks:\n",
    "\n",
    "- Check for any failed CI workflows\n",
    "- Fetch build logs and status\n",
    "- Lookup and analyze git commits\n",
    "- Suggest next troubleshooting steps\n",
    "- Escalate to humans via Human-in-Loop tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required dependencies\n",
    "!pip install haystack-ai>=2.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we configure our variables. We need to set the `OPENAI_API_KEY` for [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) and \n",
    "a `GITHUB_TOKEN` with appropriate permissions to access repository information, CI logs, and commit history. Make sure your GitHub token has sufficient access rights to the repository you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "if not os.environ.get(\"GITHUB_TOKEN\"):\n",
    "    os.environ[\"GITHUB_TOKEN\"] = getpass(\"Enter your GitHub token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "We start by defining the tools which will be used by our Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`git_list_commits_tool`: Fetches and formats the most recent commits on a given GitHub repository and branch.  \n",
    "  Useful for quickly surfacing the latest changes when diagnosing CI failures or code regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, io\n",
    "from collections import defaultdict\n",
    "from typing import Annotated, List, Dict, Tuple\n",
    "from haystack.tools import tool\n",
    "\n",
    "@tool\n",
    "def git_list_commits_tool(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    branch: Annotated[\n",
    "        str, \n",
    "        \"The branch name to list commits from\"] = \"main\"\n",
    ") -> str:\n",
    "    '''List the most recent commits for a GitHub repository and branch.'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/commits\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"sha\": branch, \"per_page\": 10}\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    commits = resp.json()\n",
    "\n",
    "    lines = []\n",
    "    for c in commits:\n",
    "        sha = c[\"sha\"][:7]\n",
    "        msg = c[\"commit\"][\"message\"].split(\"\\n\")[0]\n",
    "        lines.append(f\"- {sha}: {msg}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`git_diff_tool`: Retrieves the patch (diff) between two commits or branches in a GitHub repository.  \n",
    "  Enables side-by-side inspection of code changes that may have triggered test failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def git_commit_diff_tool(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    base: Annotated[\n",
    "        str, \n",
    "        \"The base commit SHA or branch name\"]\n",
    "    ,\n",
    "    head: Annotated[\n",
    "        str, \n",
    "        \"The head commit SHA or branch name\"]\n",
    ") -> str:\n",
    "    '''Get the diff (patch) between two commits or branches in a GitHub repository.'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/compare/{base}...{head}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data.get(\"files\", [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ci_status_tool`: Checks the most recent GitHub Actions workflow runs for failures, downloads their logs, and extracts the first failed test name and error message. Automates root-cause identification by surfacing the exact test and error that caused a pipeline to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def ci_status_tool(\n",
    "    repo: Annotated[\n",
    "      str,\n",
    "      \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"\n",
    "    ],\n",
    "    per_page: Annotated[\n",
    "      int,\n",
    "      \"How many of the most recent workflow runs to check (default 50)\"\n",
    "    ] = 50\n",
    ") -> str:\n",
    "    '''Check the most recent GitHub Actions workflow runs for a given repository, list any that failed, download their logs, and extract all failures, grouped by test suite (inferred from log file paths).'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN environment variable not set.\"\n",
    "    \n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"per_page\": per_page}\n",
    "    runs_url = f\"https://api.github.com/repos/{repo}/actions/runs\"\n",
    "    \n",
    "    resp = requests.get(runs_url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    runs = resp.json().get(\"workflow_runs\", [])\n",
    "    \n",
    "    failed_runs = [r for r in runs if r.get(\"conclusion\") == \"failure\"]\n",
    "    if not failed_runs:\n",
    "        return f\"No failed runs in the last {per_page} workflow runs for `{repo}`.\"\n",
    "\n",
    "    def extract_all_failures(logs_url: str) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Download and scan logs ZIP for all failure markers.\n",
    "        Returns a list of tuples: (suite, test_name, error_line).\n",
    "        \"\"\"\n",
    "        r = requests.get(logs_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        failures = []\n",
    "        for filepath in z.namelist():\n",
    "            if not filepath.lower().endswith(('.txt', '.log')):\n",
    "                continue\n",
    "            suite = filepath.split('/', 1)[0]  # infer suite as top-level folder or file\n",
    "            with z.open(filepath) as f:\n",
    "                for raw in f:\n",
    "                    try:\n",
    "                        line = raw.decode('utf-8', errors='ignore').strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    if any(marker in line for marker in (\"FAIL\", \"ERROR\", \"Exception\", \"error\")):\n",
    "                        parts = line.split()\n",
    "                        test_name = next(\n",
    "                            (p for p in parts if '.' in p or p.endswith(\"()\")), \n",
    "                            parts[0] if parts else \"\"\n",
    "                        )\n",
    "                        failures.append((suite, test_name, line))\n",
    "        return failures\n",
    "\n",
    "    output = [f\"Found {len(failed_runs)} failed run(s):\"]\n",
    "    for run in failed_runs:\n",
    "        run_id   = run[\"id\"]\n",
    "        branch   = run.get(\"head_branch\")\n",
    "        event    = run.get(\"event\")\n",
    "        created  = run.get(\"created_at\")\n",
    "        logs_url = run.get(\"logs_url\")\n",
    "        html_url = run.get(\"html_url\")\n",
    "\n",
    "        failures = extract_all_failures(logs_url)\n",
    "        if not failures:\n",
    "            detail = \"No individual failures parsed from logs.\"\n",
    "        else:\n",
    "            # Group by suite\n",
    "            by_suite: Dict[str, List[Tuple[str,str]]] = defaultdict(list)\n",
    "            for suite, test, err in failures:\n",
    "                by_suite[suite].append((test, err))\n",
    "            lines = []\n",
    "            for suite, items in by_suite.items():\n",
    "                lines.append(f\"  â–¶ **Suite**: `{suite}`\")\n",
    "                for test, err in items:\n",
    "                    lines.append(f\"    - **{test}**: {err}\")\n",
    "            detail = \"\\n\".join(lines)\n",
    "\n",
    "        output.append(\n",
    "            f\"- **Run ID**: {run_id}\\n\"\n",
    "            f\"  **Branch**: {branch}\\n\"\n",
    "            f\"  **Event**: {event}\\n\"\n",
    "            f\"  **Created At**: {created}\\n\"\n",
    "            f\"  **Failures by Suite**:\\n{detail}\\n\"\n",
    "            f\"  **Logs ZIP**: {logs_url}\\n\"\n",
    "            f\"  **Run URL**: {html_url}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shell_tool`: Executes a local shell command with a configurable timeout, capturing stdout or returning detailed error output. Great for grepping, filtering, or tailing CI log files before passing only the relevant snippets to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "@tool\n",
    "def shell_tool(\n",
    "    command: Annotated[\n",
    "        str,\n",
    "        \"The shell command to execute, e.g. 'grep -E \\\"ERROR|Exception\\\" build.log'\"\n",
    "    ],\n",
    "    timeout: Annotated[\n",
    "        int,\n",
    "        \"Maximum time in seconds to allow the command to run\"\n",
    "    ] = 30\n",
    ") -> str:\n",
    "    '''Executes a shell command with a timeout and returns stdout or an error message.'''\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            timeout=timeout,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        return output\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"âŒ Command failed (exit code {e.returncode}):\\n{e.output}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"âŒ Command timed out after {timeout} seconds.\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Unexpected error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`human_in_loop_tool`: Prompts the user with a clarifying question via `input()` when the Agent encounters ambiguity or needs additional information. Ensures the Agent only interrupts for human feedback when strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_in_loop_tool(\n",
    "    question: Annotated[\n",
    "        str,\n",
    "        \"A clarifying question to prompt the user for more information\"\n",
    "    ]\n",
    ") -> str:\n",
    "    '''Ask the user a clarifying question and return their response via input().'''\n",
    "    user_message = input(f\"[Agent needs your input] {question}\\n> \")\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Configure the Agent\n",
    "Create a Haystack Agent instance and configure its behavior with a system prompt and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=OpenAIChatGenerator(model=\"o4-mini\"),\n",
    "    tools=[git_list_commits_tool, git_commit_diff_tool, ci_status_tool, shell_tool, human_in_loop_tool],\n",
    "    system_prompt=(\n",
    "        \"You are a DevOps support assistant with the following capabilities:\\n\"\n",
    "        \"1. Check CI failures\\n\"\n",
    "        \"2. Fetch commits and diffs from GitHub\\n\"\n",
    "        \"3. Analyze logs\\n\"\n",
    "        \"4. Ask the user for clarification\\n\\n\"\n",
    "        \"Behavior Guidelines:\\n\"\n",
    "        \"- Tool-First: Always attempt to resolve the user's request by using the tools provided.\\n\"\n",
    "        \"- Concise Reasoning: Before calling each tool, think (briefly) about why it's neededâ€”then call it.\\n\"\n",
    "        \"IMPORTANT: Whenever you are unsure about any details or need clarification, \"\n",
    "        \"you MUST use the human_in_loop tool to ask the user questions. For example if\"\n",
    "        \"  * Required parameters are missing or ambiguous (e.g. repo name, run ID, branch)\\n\"\n",
    "        \"  * A tool returns an unexpected error that needs human insight\\n\"\n",
    "        \"- Structured Outputs: After your final tool call, summarize findings in Markdown:\\n\"\n",
    "        \"  * Run ID, Branch, Test, Error\\n\"\n",
    "        \"  * Next Steps (e.g., \\\"Add null-check to line 45\\\", \\\"Rerun tests after env fix\\\")\\n\"\n",
    "        \"- Error Handling: If a tool fails (e.g. 404, timeout), surface the error and decide whether to retry, choose another tool, or ask for clarification.\\n\"\n",
    "        \"EXIT CONDITION: Once you've provided a complete, actionable summary and next steps, exit.\"\n",
    "        \n",
    "    ),\n",
    "    exit_conditions=[\"text\"]\n",
    ")\n",
    "\n",
    "# Run the Agent\n",
    "agent.warm_up()\n",
    "#response = agent.run(messages=[ChatMessage.from_user(\"Did any CI fail deepset-ai/haystack repo?\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[ChatMessage.from_user(\"For the given repo, check for any failed CI workflows. Then debug and analyze the workflow by finding the root cause of the failure.\")]\n",
    "response = agent.run(messages=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s what I found for run 14906963785 on branch v2.13.x:\n",
      "\n",
      "â€¢ Failures  \n",
      "  â€“ TestHuggingFaceAPIChatGenerator::test_run_with_tools  \n",
      "  â€“ TestHuggingFaceAPIChatGenerator::test_run_async_with_tools  \n",
      "  Error  \n",
      "  > TypeError: __init__() got an unexpected keyword argument 'arguments'\n",
      "\n",
      "Root cause  \n",
      "The HuggingFaceAPIChatGenerator class no longer accepts an `arguments` keyword in its constructor, yet these two tests explicitly pass `arguments=â€¦`.\n",
      "\n",
      "Next Steps  \n",
      "1. In `haystack/components/generators/chat/hugging_face_api.py`, update the `__init__` signature:  \n",
      "   - Add an `arguments` parameter (or `**kwargs`)  \n",
      "   - Store it (e.g. `self.arguments = arguments`)  \n",
      "2. Ensure downstream code (e.g. payload construction for the HF API call) uses `self.arguments`.  \n",
      "3. Add a quick unit test or doc example to cover the new `arguments` parameter.  \n",
      "4. (Optional) Address the PyTorch deprecation warning in `sentence_transformers_diversity.py` by replacing any use of `x.T` on >2-D tensors with the recommended `x.mT` or `x.permute(...)`.\n",
      "\n",
      "Once the constructor signature is restored to match the tests, rerun CI to confirm these failures are resolved.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
