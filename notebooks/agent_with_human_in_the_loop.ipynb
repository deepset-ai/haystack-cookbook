{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è DevOps Support Agent with Human in the Loop\n",
    "\n",
    "*Notebook by [Amna Mubashar](https://www.linkedin.com/in/amna-mubashar-3154a814a/)*\n",
    "\n",
    "This notebook demonstrates how a [Haystack Agent](https://docs.haystack.deepset.ai/docs/agents) can interactively ask for user input when it's uncertain about the next step. We'll build a **Human-in-the-Loop** tool that the Agent can call dynamically. When the Agent encounters ambiguity or incomplete information, it will ask for more input from the human to continue solving the task.\n",
    "\n",
    "For this purpose, we will create a **DevOps Support Agent**.\n",
    "\n",
    "CI/CD pipelines occasionally fail for reasons that can be hard to diagnose including manually‚Äîbroken tests, mis-configured environment variables, flaky integrations, etc. \n",
    "\n",
    "The support Agent is created using Haystack [Tools](https://docs.haystack.deepset.ai/docs/tool) and [Agent](https://docs.haystack.deepset.ai/docs/agent) and will perform the following tasks:\n",
    "\n",
    "- Check for any failed CI workflows\n",
    "- Fetch build logs and status\n",
    "- Lookup and analyze git commits\n",
    "- Suggest next troubleshooting steps\n",
    "- Escalate to humans via Human-in-Loop tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"haystack-ai>=2.14.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we configure our variables. We need to set the `OPENAI_API_KEY` for [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) and \n",
    "a `GITHUB_TOKEN` with appropriate permissions to access repository information, CI logs, and commit history. Make sure your GitHub token has sufficient access rights to the repository you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "if not os.environ.get(\"GITHUB_TOKEN\"):\n",
    "    os.environ[\"GITHUB_TOKEN\"] = getpass(\"Enter your GitHub token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "We start by defining the tools which will be used by our Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`git_list_commits_tool`: Fetches and formats the most recent commits on a given GitHub repository and branch.  \n",
    "  Useful for quickly surfacing the latest changes when diagnosing CI failures or code regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, io\n",
    "from collections import defaultdict\n",
    "from typing import Annotated, List, Dict, Tuple\n",
    "from haystack.tools import tool\n",
    "\n",
    "@tool\n",
    "def git_list_commits_tool(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    branch: Annotated[\n",
    "        str, \n",
    "        \"The branch name to list commits from\"] = \"main\"\n",
    ") -> str:\n",
    "    '''List the most recent commits for a GitHub repository and branch.'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/commits\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"sha\": branch, \"per_page\": 10}\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    commits = resp.json()\n",
    "\n",
    "    lines = []\n",
    "    for c in commits:\n",
    "        sha = c[\"sha\"][:7]\n",
    "        msg = c[\"commit\"][\"message\"].split(\"\\n\")[0]\n",
    "        lines.append(f\"- {sha}: {msg}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`git_diff_tool`: Retrieves the patch (diff) between two commits or branches in a GitHub repository.  \n",
    "  Enables side-by-side inspection of code changes that may have triggered test failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def git_commit_diff_tool(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    base: Annotated[\n",
    "        str, \n",
    "        \"The base commit SHA or branch name\"]\n",
    "    ,\n",
    "    head: Annotated[\n",
    "        str, \n",
    "        \"The head commit SHA or branch name\"]\n",
    ") -> str:\n",
    "    '''Get the diff (patch) between two commits or branches in a GitHub repository.'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/compare/{base}...{head}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data.get(\"files\", [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ci_status_tool`: Checks the most recent GitHub Actions workflow runs for failures, downloads their logs, and extracts the first failed test name and error message. Automates root-cause identification by surfacing the exact test and error that caused a pipeline to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def ci_status_tool(\n",
    "    repo: Annotated[\n",
    "      str,\n",
    "      \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"\n",
    "    ],\n",
    "    per_page: Annotated[\n",
    "      int,\n",
    "      \"How many of the most recent workflow runs to check (default 50)\"\n",
    "    ] = 50\n",
    ") -> str:\n",
    "    '''Check the most recent GitHub Actions workflow runs for a given repository, list any that failed, download their logs, and extract all failures, grouped by test suite (inferred from log file paths).'''\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN environment variable not set.\"\n",
    "    \n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"per_page\": per_page}\n",
    "    runs_url = f\"https://api.github.com/repos/{repo}/actions/runs\"\n",
    "    \n",
    "    resp = requests.get(runs_url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    runs = resp.json().get(\"workflow_runs\", [])\n",
    "    \n",
    "    failed_runs = [r for r in runs if r.get(\"conclusion\") == \"failure\"]\n",
    "    if not failed_runs:\n",
    "        return f\"No failed runs in the last {per_page} workflow runs for `{repo}`.\"\n",
    "\n",
    "    def extract_all_failures(logs_url: str) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Download and scan logs ZIP for all failure markers.\n",
    "        Returns a list of tuples: (suite, test_name, error_line).\n",
    "        \"\"\"\n",
    "        r = requests.get(logs_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        failures = []\n",
    "        for filepath in z.namelist():\n",
    "            if not filepath.lower().endswith(('.txt', '.log')):\n",
    "                continue\n",
    "            suite = filepath.split('/', 1)[0]  # infer suite as top-level folder or file\n",
    "            with z.open(filepath) as f:\n",
    "                for raw in f:\n",
    "                    try:\n",
    "                        line = raw.decode('utf-8', errors='ignore').strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    if any(marker in line for marker in (\"FAIL\", \"ERROR\", \"Exception\", \"error\")):\n",
    "                        parts = line.split()\n",
    "                        test_name = next(\n",
    "                            (p for p in parts if '.' in p or p.endswith(\"()\")), \n",
    "                            parts[0] if parts else \"\"\n",
    "                        )\n",
    "                        failures.append((suite, test_name, line))\n",
    "        return failures\n",
    "\n",
    "    output = [f\"Found {len(failed_runs)} failed run(s):\"]\n",
    "    for run in failed_runs:\n",
    "        run_id   = run[\"id\"]\n",
    "        branch   = run.get(\"head_branch\")\n",
    "        event    = run.get(\"event\")\n",
    "        created  = run.get(\"created_at\")\n",
    "        logs_url = run.get(\"logs_url\")\n",
    "        html_url = run.get(\"html_url\")\n",
    "\n",
    "        failures = extract_all_failures(logs_url)\n",
    "        if not failures:\n",
    "            detail = \"No individual failures parsed from logs.\"\n",
    "        else:\n",
    "            # Group by suite\n",
    "            by_suite: Dict[str, List[Tuple[str,str]]] = defaultdict(list)\n",
    "            for suite, test, err in failures:\n",
    "                by_suite[suite].append((test, err))\n",
    "            lines = []\n",
    "            for suite, items in by_suite.items():\n",
    "                lines.append(f\"  ‚ñ∂ **Suite**: `{suite}`\")\n",
    "                for test, err in items:\n",
    "                    lines.append(f\"    - **{test}**: {err}\")\n",
    "            detail = \"\\n\".join(lines)\n",
    "\n",
    "        output.append(\n",
    "            f\"- **Run ID**: {run_id}\\n\"\n",
    "            f\"  **Branch**: {branch}\\n\"\n",
    "            f\"  **Event**: {event}\\n\"\n",
    "            f\"  **Created At**: {created}\\n\"\n",
    "            f\"  **Failures by Suite**:\\n{detail}\\n\"\n",
    "            f\"  **Logs ZIP**: {logs_url}\\n\"\n",
    "            f\"  **Run URL**: {html_url}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shell_tool`: Executes a local shell command with a configurable timeout, capturing stdout or returning detailed error output. Great for grepping, filtering, or tailing CI log files before passing only the relevant snippets to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "@tool\n",
    "def shell_tool(\n",
    "    command: Annotated[\n",
    "        str,\n",
    "        \"The shell command to execute, e.g. 'grep -E \\\"ERROR|Exception\\\" build.log'\"\n",
    "    ],\n",
    "    timeout: Annotated[\n",
    "        int,\n",
    "        \"Maximum time in seconds to allow the command to run\"\n",
    "    ] = 30\n",
    ") -> str:\n",
    "    '''Executes a shell command with a timeout and returns stdout or an error message.'''\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            timeout=timeout,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        return output\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"‚ùå Command failed (exit code {e.returncode}):\\n{e.output}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"‚ùå Command timed out after {timeout} seconds.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Unexpected error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`human_in_loop_tool`: Prompts the user with a clarifying question via `input()` when the Agent encounters ambiguity or needs additional information. Ensures the Agent only interrupts for human feedback when strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_in_loop_tool(\n",
    "    question: Annotated[\n",
    "        str,\n",
    "        \"A clarifying question to prompt the user for more information\"\n",
    "    ]\n",
    ") -> str:\n",
    "    '''Ask the user a clarifying question and return their response via input().'''\n",
    "    user_message = input(f\"[Agent needs your input] {question}\\n> \")\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Configure the Agent\n",
    "Create a Haystack Agent instance and configure its behavior with a system prompt and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=OpenAIChatGenerator(model=\"o4-mini\"),\n",
    "    tools=[git_list_commits_tool, git_commit_diff_tool, ci_status_tool, shell_tool, human_in_loop_tool],\n",
    "    system_prompt=(\n",
    "        \"You are a DevOps support assistant with the following capabilities:\\n\"\n",
    "        \"1. Check CI failures\\n\"\n",
    "        \"2. Fetch commits and diffs from GitHub\\n\"\n",
    "        \"3. Analyze logs\\n\"\n",
    "        \"4. Ask the user for clarification\\n\\n\"\n",
    "        \"Behavior Guidelines:\\n\"\n",
    "        \"- Tool-First: Always attempt to resolve the user's request by using the tools provided.\\n\"\n",
    "        \"- Concise Reasoning: Before calling each tool, think (briefly) about why it's needed‚Äîthen call it.\\n\"\n",
    "        \"IMPORTANT: Whenever you are unsure about any details or need clarification, \"\n",
    "        \"you MUST use the human_in_loop tool to ask the user questions. For example if\"\n",
    "        \"  * Required parameters are missing or ambiguous (e.g. repo name, run ID, branch)\\n\"\n",
    "        \"  * A tool returns an unexpected error that needs human insight\\n\"\n",
    "        \"- Structured Outputs: After your final tool call, summarize findings in Markdown:\\n\"\n",
    "        \"  * Run ID, Branch, Test, Error\\n\"\n",
    "        \"  * Next Steps (e.g., \\\"Add null-check to line 45\\\", \\\"Rerun tests after env fix\\\")\\n\"\n",
    "        \"- Error Handling: If a tool fails (e.g. 404, timeout), surface the error and decide whether to retry, choose another tool, or ask for clarification.\\n\"\n",
    "        \"EXIT CONDITION: Once you've provided a complete, actionable summary and next steps, exit.\"\n",
    "        \n",
    "    ),\n",
    "    exit_conditions=[\"text\"]\n",
    ")\n",
    "\n",
    "# Warm up the Agent\n",
    "agent.warm_up()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Introspection with Streaming\n",
    "Haystack supports streaming of `ToolCalls` and `ToolResults` during Agent execution. By passing a `streaming_callback` to the `Agent`, you can observe the internal reasoning process in real time.\n",
    "\n",
    "This allows you to:\n",
    "- See which tools the Agent decides to use.\n",
    "- Inspect the arguments passed to each tool.\n",
    "- View the results returned from each tool before the final answer is generated.\n",
    "\n",
    "This is especially useful for debugging and transparency in complex multi-step reasoning workflows of Agent.\n",
    "\n",
    "Note: You can pass any `streaming_callback` function. The `print_streaming_chunk` utility function in Haystack is configured to stream tool calls and results by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOOL CALL]\n",
      "Tool: human_in_loop_tool \n",
      "Arguments: {\"question\":\"Please provide the GitHub repository in 'owner/name' format for which you want to check CI failures.\"}\n",
      "\n",
      "[TOOL RESULT]\n",
      "deepset-ai/haystack\n",
      "\n",
      "[TOOL CALL]\n",
      "Tool: ci_status_tool \n",
      "Arguments: {\"repo\":\"deepset-ai/haystack\",\"per_page\":50}\n",
      "\n",
      "[TOOL RESULT]\n",
      "Found 2 failed run(s):\n",
      "\n",
      "- **Run ID**: 14994639315\n",
      "  **Branch**: update-tools-strict-openai\n",
      "  **Event**: pull_request\n",
      "  **Created At**: 2025-05-13T10:45:32Z\n",
      "  **Failures by Suite**:\n",
      "  ‚ñ∂ **Suite**: `Unit  ubuntu-latest`\n",
      "    - **2025-05-13T10:47:25.4802006Z**: 2025-05-13T10:47:25.4802006Z ERROR    haystack.core.serialization:serialization.py:263 Failed to import class 'Nonexisting.DocumentStore'\n",
      "    - **2025-05-13T10:47:26.4669164Z**: 2025-05-13T10:47:26.4669164Z test/components/converters/test_csv_to_document.py::TestCSVToDocument::test_run_error_handling\n",
      "    - **2025-05-13T10:47:26.6024332Z**: 2025-05-13T10:47:26.6024332Z test/components/converters/test_docx_file_to_document.py::TestDOCXToDocument::test_run_error_wrong_file_type\n",
      "\n",
      "    [... additional test failure details omitted for brevity ...]\n",
      "\n",
      "    [... agent analysis and recommendations continue below ...]\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Pass the user query to the Agent\n",
    "message=[ChatMessage.from_user(\"For the given repo, check for any failed CI workflows. Then debug and analyze the workflow by finding the root cause of the failure.\")]\n",
    "\n",
    "# Run the Agent\n",
    "response = agent.run(messages=message, streaming_callback=print_streaming_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs what I found for the most recent failing run:\n",
      "\n",
      "**Run ID**: 14994639315  \n",
      "**Branch**: update-tools-strict-openai  \n",
      "\n",
      "| Test Suite                    | Test / Step                                              | Error                                                                       |\n",
      "|-------------------------------|-----------------------------------------------------------|-----------------------------------------------------------------------------|\n",
      "| 0_lint.txt (‚Üí `lint`)         | `Process completed with exit code 4`                       | Linter returned code 4 (no details in output)                               |\n",
      "| Integration ¬∑ ubuntu-latest   | TestMarkdownToDocument::test_run_error_handling            | ‚ÄúTask exception was never retrieved‚Äù / `RuntimeError('Event loop is closed')` |\n",
      "| Integration ¬∑ windows-latest  | TestMarkdownToDocument::test_run_error_handling            | `RuntimeError('Event loop is closed')` (unraisableexception warnings)       |\n",
      "| Integration ¬∑ macos-latest    | TestMarkdownToDocument::test_run_error_handling            | `RuntimeError('Event loop is closed')`                                      |\n",
      "\n",
      "Next Steps:\n",
      "\n",
      "1. **Fix Lint Exit Code 4**  \n",
      "   - Run the lint job locally (`flake8` or `hatch run lint`) to reproduce the exit-code and capture missing details.  \n",
      "   - Adjust `.flake8`/`pre-commit` config or fix any new style violations so that the lint step exits zero.\n",
      "\n",
      "2. **Resolve ‚ÄúEvent loop is closed‚Äù in Markdown converter tests**  \n",
      "   - In `haystack/components/converters/markdown.py` (or its test), ensure any `asyncio` event loop is properly created/closed.  \n",
      "   - Switch to `pytest-asyncio` fixtures (e.g. use `@pytest.mark.asyncio` and the provided `event_loop` fixture) so that the loop remains open for the duration of `test_run_error_handling`.\n",
      "\n",
      "3. Rerun CI to confirm both the lint and integration failures are resolved.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
