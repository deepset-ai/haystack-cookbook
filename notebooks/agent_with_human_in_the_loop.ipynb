{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Agent with Human in the Loop\n",
    "\n",
    "This notebook demonstrates how a Haystack Agent can interactively ask for user input when it's uncertain about the next step. We'll build a Human-in-the-Loop tool that the Agent can call dynamically. When the Agent encounters ambiguity or incomplete information, it will use a system prompt and trigger human feedback to continue solving the task.\n",
    "\n",
    "For this purpose, we will create a DevOps Support Agent.\n",
    "\n",
    "CI/CD pipelines occasionally fail for reasons that can be hard to diagnose including manuallyâ€”broken tests, mis-configured environment variables, flaky integrations, etc. \n",
    "\n",
    "The support Agent is created using Haystack Tools and Agent and will perform the following tasks:\n",
    "\n",
    "- Check for any failed CI workflows\n",
    "- Fetch build logs and status\n",
    "- Lookup and analyze git commits\n",
    "- Suggest next troubleshooting steps\n",
    "- Escalate to humans via Human-in-Loop tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required dependencies\n",
    "!pip install haystack-ai==2.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure our variables. We need to set API keys for OpenAI and GitHub token to access repository information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "if not os.environ.get(\"GITHUB_TOKEN\"):\n",
    "    os.environ[\"GITHUB_TOKEN\"] = getpass(\"Enter your GitHub token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "We start by defining the tools which will be used by our Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`git_list_commits_tool`: Fetches and formats the most recent commits on a given GitHub repository and branch.  \n",
    "  Useful for quickly surfacing the latest changes when diagnosing CI failures or code regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, io\n",
    "from collections import defaultdict\n",
    "from typing import Annotated, List, Dict, Tuple\n",
    "from haystack.tools import create_tool_from_function\n",
    "\n",
    "def list_commits(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    branch: Annotated[\n",
    "        str, \n",
    "        \"The branch name to list commits from\"] = \"main\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the latest commits for a given GitHub repo and branch.\n",
    "    Returns a formatted string of the 10 most recent commit SHAs and messages.\n",
    "    \"\"\"\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/commits\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"sha\": branch, \"per_page\": 10}\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    commits = resp.json()\n",
    "\n",
    "    lines = []\n",
    "    for c in commits:\n",
    "        sha = c[\"sha\"][:7]\n",
    "        msg = c[\"commit\"][\"message\"].split(\"\\n\")[0]\n",
    "        lines.append(f\"- {sha}: {msg}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "git_list_commits_tool = create_tool_from_function(\n",
    "    list_commits,\n",
    "    name=\"git_list_commits\",\n",
    "    description=\"List the most recent commits for a GitHub repository and branch.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`git_get_diff_tool`: Retrieves the patch (diff) between two commits or branches in a GitHub repository.  \n",
    "  Enables side-by-side inspection of code changes that may have triggered test failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(\n",
    "    repo: Annotated[\n",
    "        str, \n",
    "        \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"]\n",
    "    ,\n",
    "    base: Annotated[\n",
    "        str, \n",
    "        \"The base commit SHA or branch name\"]\n",
    "    ,\n",
    "    head: Annotated[\n",
    "        str, \n",
    "        \"The head commit SHA or branch name\"]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the diff between two commits (or branches) in a GitHub repo.\n",
    "    Returns the patch text (up to the GitHub API's size limit).\n",
    "    \"\"\"\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN not set in environment.\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo}/compare/{base}...{head}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data.get(\"files\", [])\n",
    "\n",
    "git_diff_tool = create_tool_from_function(\n",
    "    get_diff,\n",
    "    name=\"git_get_diff\",\n",
    "    description=\"Get the diff (patch) between two commits or branches in a GitHub repository.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ci_status_tool`: Checks the most recent GitHub Actions workflow runs for failures, downloads their logs, and extracts the first failed test name and error message. Automates root-cause identification by surfacing the exact test and error that caused a pipeline to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_failed_ci_runs_categorized(\n",
    "    repo: Annotated[\n",
    "      str,\n",
    "      \"The GitHub repository in 'owner/name' format, e.g. 'my-org/my-repo'\"\n",
    "    ],\n",
    "    per_page: Annotated[\n",
    "      int,\n",
    "      \"How many of the most recent workflow runs to check (default 50)\"\n",
    "    ] = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Lists recent GitHub Actions workflow runs for a repo, finds failures,\n",
    "    downloads their logs, and extracts all failures, grouped by suite (inferred\n",
    "    from the log file path).\n",
    "    \"\"\"\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Error: GITHUB_TOKEN environment variable not set.\"\n",
    "    \n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    params = {\"per_page\": per_page}\n",
    "    runs_url = f\"https://api.github.com/repos/{repo}/actions/runs\"\n",
    "    \n",
    "    resp = requests.get(runs_url, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    runs = resp.json().get(\"workflow_runs\", [])\n",
    "    \n",
    "    failed_runs = [r for r in runs if r.get(\"conclusion\") == \"failure\"]\n",
    "    if not failed_runs:\n",
    "        return f\"No failed runs in the last {per_page} workflow runs for `{repo}`.\"\n",
    "\n",
    "    def extract_all_failures(logs_url: str) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Download and scan logs ZIP for all failure markers.\n",
    "        Returns a list of tuples: (suite, test_name, error_line).\n",
    "        \"\"\"\n",
    "        r = requests.get(logs_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        failures = []\n",
    "        for filepath in z.namelist():\n",
    "            if not filepath.lower().endswith(('.txt', '.log')):\n",
    "                continue\n",
    "            suite = filepath.split('/', 1)[0]  # infer suite as top-level folder or file\n",
    "            with z.open(filepath) as f:\n",
    "                for raw in f:\n",
    "                    try:\n",
    "                        line = raw.decode('utf-8', errors='ignore').strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    if any(marker in line for marker in (\"FAIL\", \"ERROR\", \"Exception\", \"error\")):\n",
    "                        parts = line.split()\n",
    "                        test_name = next(\n",
    "                            (p for p in parts if '.' in p or p.endswith(\"()\")), \n",
    "                            parts[0] if parts else \"\"\n",
    "                        )\n",
    "                        failures.append((suite, test_name, line))\n",
    "        return failures\n",
    "\n",
    "    output = [f\"Found {len(failed_runs)} failed run(s):\"]\n",
    "    for run in failed_runs:\n",
    "        run_id   = run[\"id\"]\n",
    "        branch   = run.get(\"head_branch\")\n",
    "        event    = run.get(\"event\")\n",
    "        created  = run.get(\"created_at\")\n",
    "        logs_url = run.get(\"logs_url\")\n",
    "        html_url = run.get(\"html_url\")\n",
    "\n",
    "        failures = extract_all_failures(logs_url)\n",
    "        if not failures:\n",
    "            detail = \"No individual failures parsed from logs.\"\n",
    "        else:\n",
    "            # Group by suite\n",
    "            by_suite: Dict[str, List[Tuple[str,str]]] = defaultdict(list)\n",
    "            for suite, test, err in failures:\n",
    "                by_suite[suite].append((test, err))\n",
    "            lines = []\n",
    "            for suite, items in by_suite.items():\n",
    "                lines.append(f\"  â–¶ **Suite**: `{suite}`\")\n",
    "                for test, err in items:\n",
    "                    lines.append(f\"    - **{test}**: {err}\")\n",
    "            detail = \"\\n\".join(lines)\n",
    "\n",
    "        output.append(\n",
    "            f\"- **Run ID**: {run_id}\\n\"\n",
    "            f\"  **Branch**: {branch}\\n\"\n",
    "            f\"  **Event**: {event}\\n\"\n",
    "            f\"  **Created At**: {created}\\n\"\n",
    "            f\"  **Failures by Suite**:\\n{detail}\\n\"\n",
    "            f\"  **Logs ZIP**: {logs_url}\\n\"\n",
    "            f\"  **Run URL**: {html_url}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "\n",
    "# Wrap it as a Haystack Tool\n",
    "ci_status_tool = create_tool_from_function(\n",
    "    get_failed_ci_runs_categorized,\n",
    "    name=\"ci_status_tool\",\n",
    "    description=(\n",
    "        \"Check the most recent GitHub Actions workflow runs for a given repository, \"\n",
    "        \"list any that failed, download their logs, and extract all failures, \"\n",
    "        \"grouped by test suite (inferred from log file paths).\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shell_tool`: Executes a local shell command with a configurable timeout, capturing stdout or returning detailed error output. Great for grepping, filtering, or tailing CI log files before passing only the relevant snippets to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_shell_command(\n",
    "    command: Annotated[\n",
    "        str,\n",
    "        \"The shell command to execute, e.g. 'grep -E \\\"ERROR|Exception\\\" build.log'\"\n",
    "    ],\n",
    "    timeout: Annotated[\n",
    "        int,\n",
    "        \"Maximum time in seconds to allow the command to run\"\n",
    "    ] = 30\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Executes a shell command with a timeout and returns stdout or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            timeout=timeout,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        return output\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"âŒ Command failed (exit code {e.returncode}):\\n{e.output}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"âŒ Command timed out after {timeout} seconds.\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Unexpected error: {str(e)}\"\n",
    "\n",
    "shell_tool = create_tool_from_function(\n",
    "    run_shell_command,\n",
    "    name=\"shell_tool\",\n",
    "    description=(\n",
    "        \"Execute a shell command (e.g., grep, awk) in a sandboxed environment with a timeout; \"\n",
    "        \"returns the commandâ€™s stdout or a detailed error message.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`human_in_loop_tool`: Prompts the user with a clarifying question via `input()` when the Agent encounters ambiguity or needs additional information. Ensures the Agent only interrupts for human feedback when strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_in_loop(\n",
    "    question: Annotated[\n",
    "        str,\n",
    "        \"A clarifying question to prompt the user for more information\"\n",
    "    ]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Prompts the user with the given question using Python's input() and returns their response.\n",
    "    \"\"\"\n",
    "    user_message = input(f\"[Agent needs your input] {question}\\n> \")\n",
    "    return user_message\n",
    "\n",
    "# Wrap it as a Haystack Tool\n",
    "human_in_loop_tool = create_tool_from_function(\n",
    "    human_in_loop,\n",
    "    name=\"human_in_loop\",\n",
    "    description=\"Ask the user a clarifying question and return their response via input().\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Configure the Agent\n",
    "Create a Haystack Agent instance and configure its behavior with a system prompt and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=OpenAIChatGenerator(),\n",
    "    tools=[git_list_commits_tool, git_diff_tool, ci_status_tool, shell_tool, human_in_loop_tool],\n",
    "    system_prompt=(\n",
    "        \"You are a DevOps support assistant with the following capabilities:\\n\"\n",
    "        \"1. Fetch commits and diffs from GitHub\\n\"\n",
    "        \"2. Check CI failures\\n\"\n",
    "        \"3. Analyze logs\\n\"\n",
    "        \"4. Ask users for clarification\\n\\n\"\n",
    "        \"IMPORTANT: Whenever you are unsure about any details or need clarification, \"\n",
    "        \"you MUST use the human_in_loop tool to ask the user questions. \"\n",
    "        \"For example, if the user's request is vague or missing crucial information, \"\n",
    "        \"use human_in_loop to get the necessary details.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the Agent\n",
    "agent.warm_up()\n",
    "#response = agent.run(messages=[ChatMessage.from_user(\"Did any CI fail deepset-ai/haystack repo?\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[ChatMessage.from_user(\"For the given repo, check for any failed CI workflows. Then debug and analyze the workflow by finding the root cause of the failure. Find the commit that caused the failure and list the changes in the commit. Ask the user for more information if needed. Call human in loop at the end\")]\n",
    "result = agent.run(messages=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've identified the failed CI workflows for the repository `deepset-ai/haystack-experimental`. Here are the details:\n",
      "\n",
      "### Failed Workflow Runs\n",
      "1. **Run ID**: **14824297966**\n",
      "   - **Branch**: `fix-init-params-in-state`\n",
      "   - **Created At**: 2025-05-04\n",
      "   - **Failures**:\n",
      "     - `0_Unit ubuntu-latest.txt`: Warnings about MIME type guessing failure.\n",
      "     - `Integration windows-latest`: Multiple failed tests due to `PipelineRuntimeError` with components.\n",
      "     - `Integration macos-latest`: Similar to the previous suite.\n",
      "     - `0_linting.txt`: No errors reported.\n",
      "     - `1_Unit windows-latest`: Warnings about MIME type guessing.\n",
      "   \n",
      "   - **Log Summary**:\n",
      "     - [Logs Zip](https://api.github.com/repos/deepset-ai/haystack-experimental/actions/runs/14824297966/logs)\n",
      "     - [Workflow URL](https://github.com/deepset-ai/haystack-experimental/actions/runs/14824297966)\n",
      "\n",
      "2. **Run ID**: **14824014070**\n",
      "   - **Branch**: `fix-init-params-in-state`\n",
      "   - **Created At**: 2025-05-04\n",
      "   - **Failures**:\n",
      "     - `0_Unit ubuntu-latest.txt`: Warnings about MIME type.\n",
      "     - `Integration windows-latest`: Same issues as above with different tests failing due to serialization problems.\n",
      "     - The tests seem to fail due to issues around API key handling and serialization failures.\n",
      "\n",
      "   - **Log Summary**:\n",
      "     - [Logs Zip](https://api.github.com/repos/deepset-ai/haystack-experimental/actions/runs/14824014070/logs)\n",
      "     - [Workflow URL](https://github.com/deepset-ai/haystack-experimental/actions/runs/14824014070)\n",
      "\n",
      "3. **Run ID**: **14823703864**\n",
      "   - **Branch**: `fix-init-params-in-state`\n",
      "   - **Created At**: 2025-05-04\n",
      "   - **Failures**: Similar to previous runs, focusing on serialization of token-based secrets and multiple unit/integration pipeline components failing due to API issues.\n",
      "\n",
      "   - **Log Summary**:\n",
      "     - [Logs Zip](https://api.github.com/repos/deepset-ai/haystack-experimental/actions/runs/14823703864/logs)\n",
      "     - [Workflow URL](https://github.com/deepset-ai/haystack-experimental/actions/runs/14823703864)\n",
      "\n",
      "### Recent Commits Leading to the Failures\n",
      "I have identified the most recent commits on the branch `fix-init-params-in-state`:\n",
      "- **e404e74**: Fix the error in tests\n",
      "- **4f3387c**: Fix other tests\n",
      "- **b0dda74**: Fix test\n",
      "- **1fcb78a**: Save init parameters in state (this is the base commit from which changes were made).\n",
      "\n",
      "### Changes in the Last Commit (e404e74)\n",
      "This commit involved modifications in:\n",
      "1. **`haystack_experimental/core/pipeline/pipeline.py`**  \n",
      "   - Added error handling improvements.\n",
      "   - Enhanced how components handle initializing parameters when running components.\n",
      "\n",
      "2. **`test/core/pipeline/test_pipeline_breakpoints_answer_joiner.py`**, **`test/core/pipeline/test_pipeline_breakpoints_branch_joiner.py`**, **`test/core/pipeline/test_pipeline_breakpoints_loops.py`**:\n",
      "   - Mock adjustments for error handling in API keys and methods related to the pipeline tests. Warnings for unhandled MIME types and mocked OpenAI components.\n",
      "\n",
      "### Next Steps\n",
      "To further analyze the specific changes, please confirm which of the following you would like to pursue:\n",
      "1. Detailed exploration of specific files modified.\n",
      "2. An investigation of specific tests that are failing.\n",
      "3. Additional testing suggestions or modifications to fix the issues.\n",
      "\n",
      "Please let me know how you would like to proceed!\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Trying Alternate Prompt ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent(\n",
    "    chat_generator=OpenAIChatGenerator(),\n",
    "    tools=[git_list_commits_tool, git_diff_tool, ci_status_tool, shell_tool, human_in_loop_tool],\n",
    "    system_prompt=(\n",
    "        \"You are DevOpsGPT, a specialized support assistant for diagnosing and fixing CI/CD failures.\\n\\n\"\n",
    "        \"Capabilities & Tools:\\n\"\n",
    "        \"- git_list_commits: list recent commits for a repo/branch\\n\"\n",
    "        \"- git_get_diff: show code changes between two commits/branches\\n\"\n",
    "        \"- ci_status_tool: detect failed GitHub Actions runs and extract the first failed test & error\\n\"\n",
    "        \"- shell_tool: run shell commands (e.g., grep logs) with timeout and safe sandboxing\\n\"\n",
    "        \"- human_in_loop: prompt the user for clarification when you lack critical details\\n\\n\"\n",
    "        \"Behavior Guidelines:\\n\"\n",
    "        \"- Tool-First: Always attempt to resolve the user's request by chaining tools.\\n\"\n",
    "        \"- Concise Reasoning: Before calling each tool, think (briefly) about why it's neededâ€”then call it.\\n\"\n",
    "        \"- Minimal Interruptions: Only use human_in_loop if:\\n\"\n",
    "        \"  * Required parameters are missing or ambiguous (e.g. repo name, run ID, branch)\\n\"\n",
    "        \"  * A tool returns an unexpected error that needs human insight\\n\"\n",
    "        \"- Structured Outputs: After your final tool call, summarize findings in Markdown:\\n\"\n",
    "        \"  * Run ID, Branch, Test, Error\\n\"\n",
    "        \"  * Next Steps (e.g., \\\"Add null-check to line 45\\\", \\\"Rerun tests after env fix\\\")\\n\"\n",
    "        \"- Error Handling: If a tool fails (e.g. 404, timeout), surface the error and decide whether to retry, choose another tool, or ask for clarification.\\n\"\n",
    "        \"- Respect Rate Limits: Don't over-fetchâ€”use per_page judiciously (default â‰¤ 20).\\n\\n\"\n",
    "        \"Exit Condition:\\n\"\n",
    "        \"Once you've provided a complete, actionable summary and next steps, stopâ€”do not call any more tools.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the Agent\n",
    "agent.warm_up()\n",
    "#response = agent.run(messages=[ChatMessage.from_user(\"Did any CI fail deepset-ai/haystack repo?\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[ChatMessage.from_user(\"For the given repo, check for any failed CI workflows. Then debug and analyze the workflow by finding the root cause of the failure. Find the commit that caused the failure and list the changes in the commit. Ask the user for more information if needed. Call human in loop at the end\")]\n",
    "result = agent.run(messages=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"messages\"][-1].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
