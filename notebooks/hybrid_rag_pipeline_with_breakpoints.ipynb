{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid RAG Pipeline with Breakpoints\n",
    "\n",
    "This notebook demonstrates how to setup breakpoints in a Haystack pipeline. In this case we will set up break points in a hybrid retrieval-augmented generation (RAG) pipeline. The pipeline combines BM25 and embedding-based retrieval methods, then uses a transformer-based reranker and an LLM to generate answers.\n",
    "\n",
    "> NOTE: this feature is a part of [`haystack-experimental`](https://github.com/deepset-ai/haystack-experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"haystack-experimental\"\n",
    "!pip install \"transformers[torch,sentencepiece]\"\n",
    "!pip install \"sentence-transformers>=3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary components from Haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.core.pipeline.pipeline import Pipeline # Note that we need to import the pipeline from haystack-experimental\n",
    "\n",
    "from haystack import Document\n",
    "from haystack.components.builders import AnswerBuilder, ChatPromptBuilder\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.document_stores.types import DuplicatePolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Store Initializations\n",
    "\n",
    "Let's create a simple document store with some sample documents and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing():\n",
    "    \"\"\"\n",
    "    Indexing documents in a DocumentStore.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Indexing documents...\")\n",
    "\n",
    "    # Create sample documents\n",
    "    documents = [\n",
    "        Document(content=\"My name is Jean and I live in Paris.\"),\n",
    "        Document(content=\"My name is Mark and I live in Berlin.\"),\n",
    "        Document(content=\"My name is Giorgio and I live in Rome.\"),\n",
    "    ]\n",
    "\n",
    "    # Initialize document store and components\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    doc_writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
    "    doc_embedder = SentenceTransformersDocumentEmbedder(model=\"intfloat/e5-base-v2\", progress_bar=False)\n",
    "\n",
    "    # Build and run the ingestion pipeline\n",
    "    ingestion_pipe = Pipeline()\n",
    "    ingestion_pipe.add_component(instance=doc_embedder, name=\"doc_embedder\")\n",
    "    ingestion_pipe.add_component(instance=doc_writer, name=\"doc_writer\")\n",
    "\n",
    "    ingestion_pipe.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n",
    "    ingestion_pipe.run({\"doc_embedder\": {\"documents\": documents}})\n",
    "\n",
    "    return document_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hybrid Retrieval Pipeline\n",
    "\n",
    "Now let's build a hybrid RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieval(doc_store):\n",
    "    \"\"\"\n",
    "    A simple pipeline for hybrid retrieval using BM25 and embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize query embedder\n",
    "    query_embedder = SentenceTransformersTextEmbedder(model=\"intfloat/e5-base-v2\", progress_bar=False)\n",
    "\n",
    "    # Define the prompt template for the LLM\n",
    "    template = [\n",
    "        ChatMessage.from_system(\n",
    "            \"You are a helpful AI assistant. Answer the following question based on the given context information only. If the context is empty or just a '\\n' answer with None, example: 'None'.\"\n",
    "        ),\n",
    "        ChatMessage.from_user(\n",
    "            \"\"\"\n",
    "            Context:\n",
    "            {% for document in documents %}\n",
    "                {{ document.content }}\n",
    "            {% endfor %}\n",
    "    \n",
    "            Question: {{question}}\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Build the RAG pipeline\n",
    "    rag_pipeline = Pipeline()\n",
    "    \n",
    "    # Add components to the pipeline\n",
    "    rag_pipeline.add_component(instance=InMemoryBM25Retriever(document_store=doc_store), name=\"bm25_retriever\")\n",
    "    rag_pipeline.add_component(instance=query_embedder, name=\"query_embedder\")\n",
    "    rag_pipeline.add_component(instance=InMemoryEmbeddingRetriever(document_store=doc_store), name=\"embedding_retriever\")\n",
    "    rag_pipeline.add_component(instance=DocumentJoiner(sort_by_score=False), name=\"doc_joiner\")\n",
    "    rag_pipeline.add_component(instance=TransformersSimilarityRanker(model=\"intfloat/simlm-msmarco-reranker\", top_k=5), name=\"ranker\")    \n",
    "    rag_pipeline.add_component(instance=ChatPromptBuilder(template=template, required_variables=[\"question\", \"documents\"]), name=\"prompt_builder\", )    \n",
    "    rag_pipeline.add_component(instance=OpenAIChatGenerator(), name=\"llm\")\n",
    "    rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "\n",
    "    # Connect the components\n",
    "    rag_pipeline.connect(\"query_embedder\", \"embedding_retriever.query_embedding\")\n",
    "    rag_pipeline.connect(\"embedding_retriever\", \"doc_joiner.documents\")\n",
    "    rag_pipeline.connect(\"bm25_retriever\", \"doc_joiner.documents\")\n",
    "    rag_pipeline.connect(\"doc_joiner\", \"ranker.documents\")\n",
    "    rag_pipeline.connect(\"ranker\", \"prompt_builder.documents\")\n",
    "    rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "    rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")    \n",
    "    rag_pipeline.connect(\"doc_joiner\", \"answer_builder.documents\")\n",
    "\n",
    "    return rag_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline with breakpoints\n",
    "\n",
    "This function shows how to run a pipeline using breakpoints to save its state at specific stages.\n",
    "In this example, we place a breakpoint at the query embedder and set the visit count to 0, breaking the execution in the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "ename": "PipelineBreakpointException",
     "evalue": "Breaking at component query_embedder visit count 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineBreakpointException\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere does Mark live?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_embedder\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: question},\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25_retriever\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_builder\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question},\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreakpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_embedder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/haystack-experimental/haystack_experimental/core/pipeline/pipeline.py:321\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, include_outputs_from, breakpoints, resume_state, debug_path)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# keep track of the original input to save it in case of a breakpoint when running the component\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_input_data \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 321\u001b[0m component_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_component\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_visits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidated_breakpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Updates global input state with component outputs and returns outputs that should go to\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# pipeline outputs.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m component_pipeline_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_component_outputs(\n\u001b[1;32m    332\u001b[0m     component_name\u001b[38;5;241m=\u001b[39mcomponent_name,\n\u001b[1;32m    333\u001b[0m     component_outputs\u001b[38;5;241m=\u001b[39mcomponent_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     include_outputs_from\u001b[38;5;241m=\u001b[39minclude_outputs_from,\n\u001b[1;32m    337\u001b[0m )\n",
      "File \u001b[0;32m~/haystack-experimental/haystack_experimental/core/pipeline/pipeline.py:90\u001b[0m, in \u001b[0;36mPipeline._run_component\u001b[0;34m(self, component, inputs, component_visits, breakpoints, parent_span)\u001b[0m\n\u001b[1;32m     88\u001b[0m breakpoint_inputs[component_name] \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39m_remove_unserializable_data(component_inputs)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m breakpoints \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_state:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_breakpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbreakpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_visits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreakpoint_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtrace(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaystack.component.run\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     tags\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# We deepcopy the inputs otherwise we might lose that information\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# when we delete them in case they're sent to other Components\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_content_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaystack.component.input\u001b[39m\u001b[38;5;124m\"\u001b[39m, deepcopy(component_inputs))\n",
      "File \u001b[0;32m~/haystack-experimental/haystack_experimental/core/pipeline/pipeline.py:425\u001b[0m, in \u001b[0;36mPipeline._check_breakpoints\u001b[0;34m(self, breakpoints, component_name, component_visits, inputs)\u001b[0m\n\u001b[1;32m    423\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(msg)\n\u001b[1;32m    424\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_state(inputs, \u001b[38;5;28mstr\u001b[39m(component_name), component_visits)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PipelineBreakpointException(msg, component\u001b[38;5;241m=\u001b[39mcomponent_name, state\u001b[38;5;241m=\u001b[39mstate)\n",
      "\u001b[0;31mPipelineBreakpointException\u001b[0m: Breaking at component query_embedder visit count 0"
     ]
    }
   ],
   "source": [
    "# Initialize document store and pipeline\n",
    "doc_store = indexing()\n",
    "pipeline = hybrid_retrieval(doc_store)\n",
    "\n",
    "# Define the query\n",
    "question = \"Where does Mark live?\"\n",
    "data = {\n",
    "    \"query_embedder\": {\"text\": question},\n",
    "    \"bm25_retriever\": {\"query\": question},\n",
    "    \"ranker\": {\"query\": question, \"top_k\": 10},\n",
    "    \"prompt_builder\": {\"question\": question},\n",
    "    \"answer_builder\": {\"query\": question},\n",
    "}\n",
    "\n",
    "\n",
    "pipeline.run(data, breakpoints={(\"query_embedder\", 0)}, debug_path=\"saved_states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run shall break with a `PipelineBreakpointException: Breaking at component query_embedder visit count 0` - and this will generate a JSON file with date and time stamp (e.g.: `query_embedder_2025_04_15_15_00_20.json`) in a new directory `saved_stated` containing the pipeline running states before running the component `query_embedder`.\n",
    "\n",
    "This file can be explored and used to inspect the pipeline at that execution point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming from a break point\n",
    "\n",
    "We can then resume a pipeline from a `saved_state` by passing it to the `Pipeline.run()` method. This will run the pipeline to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark lives in Berlin.\n",
      "{'model': 'gpt-4o-mini-2024-07-18', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 6, 'prompt_tokens': 74, 'total_tokens': 80, 'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0)}}\n"
     ]
    }
   ],
   "source": [
    " # Load the saved state and continue execution\n",
    "resume_state = pipeline.load_state(\"saved_states/query_embedder_2025_04_15_15_00_20.json\")\n",
    "result = pipeline.run(data={}, resume_state=resume_state)\n",
    "    \n",
    "# Print the results\n",
    "print(result['answer_builder']['answers'][0].data)\n",
    "print(result['answer_builder']['answers'][0].meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
