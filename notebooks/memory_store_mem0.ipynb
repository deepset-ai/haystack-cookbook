{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mem0 Memory Store with Haystack Agents\n",
    "\n",
    "[Mem0](https://mem0.ai/) is a managed memory layer for AI agents. Instead of passing entire conversation histories to an LLM on every turn, Mem0 intelligently extracts and compresses key facts from conversations into optimized memory representations.\n",
    "\n",
    "At a high level, Mem0 manages a cycle of **extraction**, **consolidation**, and **retrieval**. When new messages arrive, relevant facts are identified and stored. Over time, memories are merged, updated, or allowed to fade if they lose relevance. When the agent later needs context, Mem0 surfaces only the memories most relevant to the current query, helping to keep token usage and latency low.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Set up a `Mem0MemoryStore` and add memories about a user\n",
    "2. Inspect what Mem0 actually stored\n",
    "3. Create a Haystack Agent that uses the memory store\n",
    "4. Ask the Agent personalized questions and see how it leverages stored memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The Mem0 integration with Haystack lives in the [`haystack-experimental`](https://github.com/deepset-ai/haystack-experimental) package and is currently **experimental**. The API may change in future releases. For the latest status, check the [haystack-experimental repository](https://github.com/deepset-ai/haystack-experimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "We need `haystack-ai` for the core framework, `haystack-experimental` for the Mem0 memory store integration and the experimental Agent, and `mem0ai` as the underlying Mem0 client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install haystack-ai haystack-experimental mem0ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys\n",
    "\n",
    "This notebook requires two API keys:\n",
    "- **`OPENAI_API_KEY`**: Used by the [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) to power the Agent's LLM.\n",
    "- **`MEM0_API_KEY`**: Used by `Mem0MemoryStore` to connect to the [Mem0 Platform](https://app.mem0.ai/). You can get a free API key by signing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if not os.environ.get(\"MEM0_API_KEY\"):\n",
    "    os.environ[\"MEM0_API_KEY\"] = getpass(\"Enter your Mem0 API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Mem0 Memory Store\n",
    "\n",
    "The `Mem0MemoryStore` connects to the [Mem0 Platform](https://app.mem0.ai/) using the `MEM0_API_KEY` environment variable. Each memory is associated with a `user_id`, which allows Mem0 to maintain separate memory spaces for different users.\n",
    "\n",
    "We will add several facts about a user - their preferences, background, and work context. Mem0 will extract the key facts from these messages and store them as structured memories. When the Agent later queries the memory store, only the most relevant memories will be retrieved rather than replaying the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'memory_id': '31eb96d0-c679-4a2c-8b65-ec617c3d7e17',\n",
       "  'memory': 'User likes listening to Russian pop music.'},\n",
       " {'memory_id': '503f44b3-45ed-4a7d-bb0b-d5cfaf536e32',\n",
       "  'memory': 'User enjoyed a cold Spanish latte with oat milk.'},\n",
       " {'memory_id': '7f757c93-c00e-4972-a9b5-ce20a8f47efd',\n",
       "  'memory': 'User lives in Florence, Italy, and loves mountains.'},\n",
       " {'memory_id': 'f68058f0-93ef-471d-9092-f78c155c0f07',\n",
       "  'memory': 'User is a software engineer and CEO of a startup with 10 people, building a platform for small businesses to manage customers and sales, and prefers Python and the Haystack framework for NLP and LLM agent projects.'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_experimental.memory_stores.mem0 import Mem0MemoryStore\n",
    "\n",
    "memory_store = Mem0MemoryStore()\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_user(\"I like to listen to Russian pop music\"),\n",
    "    ChatMessage.from_user(\"I liked cold spanish latte with oat milk\"),\n",
    "    ChatMessage.from_user(\"I live in Florence Italy and I love mountains\"),\n",
    "    ChatMessage.from_user(\n",
    "        \"I am a software engineer and I like building application in python. \"\n",
    "        \"Most of my projects are related to NLP and LLM agents. \"\n",
    "        \"I find it easier to use Haystack framework to build my projects.\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"I work in a startup and I am the CEO of the company. \"\n",
    "        \"I have a team of 10 people and we are building a platform \"\n",
    "        \"for small businesses to manage their customers and sales.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory_store.add_memories(user_id=\"agent_example\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Stored Memories\n",
    "\n",
    "Before connecting the memory store to an Agent, let's see what Mem0 actually stored. The `search_memories` method lets us query the memory store and see which memories are retrieved for a given query. This is useful for understanding how Mem0 extracts and condenses information from raw messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is a software engineer and CEO of a startup with 10 people, building a platform for small businesses to manage customers and sales, and prefers Python and the Haystack framework for NLP and LLM agent projects.\n",
      "User likes listening to Russian pop music.\n",
      "User enjoyed a cold Spanish latte with oat milk.\n"
     ]
    }
   ],
   "source": [
    "results = memory_store.search_memories(\n",
    "    query=\"What programming tools does this person use?\",\n",
    "    user_id=\"agent_example\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for message in results:\n",
    "    print(message.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Mem0 has rephrased the original messages into facts. Instead of storing the full sentences, it extracts the key information and returns the memories most relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent with Memory\n",
    "\n",
    "Now we create a Haystack Agent that uses an `OpenAIChatGenerator` as its LLM and the `Mem0MemoryStore` as its memory.\n",
    "\n",
    "When the Agent runs, it will:\n",
    "1. Search the memory store using the user's message as a query\n",
    "2. Inject relevant memories into the conversation as context\n",
    "3. Generate a response that takes those memories into account\n",
    "4. Save new messages back to the memory store automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No tools provided to the Agent. The Agent will behave like a ChatGenerator and only return text responses. To enable tool usage, pass tools directly to the Agent, not to the chat_generator.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack_experimental.components.agents.agent import Agent\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-5-mini\")\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=chat_generator,\n",
    "    memory_store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Personalized Question\n",
    "\n",
    "Let's ask the Agent a question that it can only answer well if it remembers the user's preferences. Based on the stored memories, the Agent knows the user is a Python developer who uses Haystack and works on NLP/LLM projects - so it should tailor its recommendation accordingly.\n",
    "\n",
    "Note the `memory_store_kwargs` parameter: this is where we pass the `user_id` so the Agent knows which user's memories to search. Behind the scenes, the Agent will query Mem0 with the user's message, retrieve relevant memories, inject them into the prompt as additional context, and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: given that you're a Python-preferring engineer who already likes Haystack, use Haystack as the primary framework — and complement it with a vector DB (managed if you want to move fast), an LLM provider (OpenAI or a local LLM), and small orchestration pieces (FastAPI, background workers, connectors to booking APIs). Haystack maps directly to the retrieval + generation + multi‑turn needs of a travel agent and lets you build production pipelines in Python.\n",
      "\n",
      "Why Haystack fits you (based on what I know about you)\n",
      "- You prefer Python and already like Haystack for NLP/LLM projects — so lower ramp-up time and predictable productivity.\n",
      "- Haystack is built around retriever/reader/generation pipelines and conversational state, which matches travel agent needs (knowledge retrieval, multi‑turn booking dialogs, guided form filling).\n",
      "- It’s modular, so you can replace components (vector DB, LLM) as you scale.\n",
      "\n",
      "Recommended architecture and components\n",
      "- Frontend: React / mobile UI talking to a FastAPI backend.\n",
      "- API & orchestration (Python): FastAPI + Celery/RQ for long-running tasks (ticket search, booking).\n",
      "- Retrieval layer: Haystack DocumentStore + EmbeddingRetriever. DocumentStore options:\n",
      "  - Elasticsearch / OpenSearch (good for hybrid search + scale)\n",
      "  - Milvus / Pinecone / Weaviate (managed vector DB for fast iteration)\n",
      "  - FAISS (local, good for prototyping)\n",
      "- Embeddings/LLM:\n",
      "  - Managed (OpenAI, Anthropic) for fastest reliable quality.\n",
      "  - Or self‑hosted LLMs (Llama 2 / Mistral via Hugging Face or TGI) if you want cost/control.\n",
      "- Conversational pipeline: Haystack ConversationalPipeline / GenerativeQAPipeline for multi‑turn QA and context-aware responses.\n",
      "- Tooling & action execution:\n",
      "  - Implement booking/payment connectors as custom Haystack nodes or as separate microservices called by the orchestration layer.\n",
      "  - For tool-like behavior (calling flight/hotel APIs, calendar, payments), either use LLM function-calling (OpenAI) or orchestrate actions server-side based on parsed intents/entities.\n",
      "- NLU extras: use NER and slot-filling to extract traveler info (dates, origins, passport info), or integrate a light Rasa-style dialogue policy if you want explicit state machines.\n",
      "- Monitoring & safety: logging, prompt/response auditing, input validation, rate limiting, and consent/privacy handling for PII.\n",
      "\n",
      "How to prototype quickly (MVP path)\n",
      "1. Ingest relevant documents: FAQs, travel policies, vendor docs, sample itineraries into Haystack DocumentStore.\n",
      "2. Plug an EmbeddingRetriever and a generative LLM (OpenAI/gpt-4o or local model) via Haystack.\n",
      "3. Build a simple ConversationalPipeline that keeps context and can answer queries and ask clarifying questions.\n",
      "4. Add an action node or endpoint that converts validated slots into an API call to a test booking API (or a mocked service).\n",
      "5. Iterate on prompts, retrieval settings (top_k), and add a vector DB if you need scale.\n",
      "\n",
      "When to consider alternatives or complements\n",
      "- If you want heavy agentic tool chaining out of the box, LangChain has a more mature “agent” ecosystem — but you can integrate LangChain agents with your Python/Harstack stack or implement minimal agent behavior via Haystack nodes.\n",
      "- If you need rich dialogue policies across many flows, consider combining Haystack for knowledge/RAG with Rasa for fine-grained dialog management.\n",
      "- If document ingestion/metadata indexing is your main focus, LlamaIndex can simplify some ingestion tasks; you can still use Haystack for runtime pipelines.\n",
      "\n",
      "Scaling and production tips\n",
      "- Start with managed vector DB (Pinecone, Weaviate Cloud, Milvus Cloud) to avoid ops overhead.\n",
      "- Cache common queries/itineraries and batch embedding calls.\n",
      "- Use a hybrid retriever (sparse + dense) for precision and recall.\n",
      "- Separate “tell me” flows (informational) from “do” flows (bookings) and require stronger validation/consent for the latter.\n",
      "- Instrument for user feedback and fine-tune prompts/response reranking based on logs.\n",
      "\n",
      "If you want, I can:\n",
      "- Sketch a minimal Haystack-based project structure and sample pipeline code.\n",
      "- Recommend specific vector DB + LLM combos based on budget and latency requirements.\n",
      "- Provide a checklist for PII/booking compliance and testing.\n",
      "\n",
      "Which of those follow-ups would you like next?\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Based on what you know about me, which framework should I use to design an AI travel agent?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"last_message\"].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Follow-Up Question\n",
    "\n",
    "Let's try a different kind of question. This time about personal preferences rather than technical skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure — I can help. A few quick questions so I can make better suggestions:\n",
      "- How long is your trip (long weekend, 1 week, 2+ weeks)?\n",
      "- Where will you be traveling from / how far do you want to fly?\n",
      "- What's your budget range (economy, mid, luxury)?\n",
      "- What kind of trip do you prefer: beach, city & culture, nature/hiking, food/wine, relaxation, active/adventure, family-friendly?\n",
      "- Who are you traveling with (solo, couple, friends, kids)?\n",
      "- Any health/visa/other constraints I should know about?\n",
      "\n",
      "If you want ideas now, here are several good June options across tastes and budgets, with why June is a good time to go:\n",
      "\n",
      "- Santorini or Crete, Greece — warm weather, sunny beaches, great food and island vibes. Early–mid June is before peak late‑July crowds and still reliably sunny.\n",
      "- Dubrovnik & Hvar, Croatia — beautiful coast, historic Old Town, island hopping and clear seas. June has pleasant temperatures and long daylight.\n",
      "- Iceland (Ring Road / Golden Circle) — near‑24‑hour daylight, waterfalls, glaciers and dramatic landscapes; ideal for road trips and outdoor photography.\n",
      "- Norwegian fjords (Bergen, Aurland, Geiranger) — hiking and scenic drives with mild weather and the midnight sun; fewer mosquitoes than later summer.\n",
      "- Canadian Rockies (Banff / Jasper) — turquoise lakes, alpine hiking, wildlife viewing with snow mostly melted at higher trails by June.\n",
      "- Azores, Portugal — lush landscapes, whale/sea life watching, hiking and geothermal baths; cooler, green and less crowded than mainland Portugal.\n",
      "- Hokkaido, Japan — pleasant cool weather, great seafood and outdoor activities; avoids Japan’s main rainy season which hits Honshu in June.\n",
      "- Algarve (Portugal) or Amalfi Coast (Italy) — classic June sun/beach + coastal villages; book lodging early as tourism ramps up.\n",
      "\n",
      "If you tell me your answers to the questions above, I’ll recommend 2–3 tailored destinations and can suggest a sample itinerary, best neighborhoods, and packing tips. Which direction do you want to go?\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Can you suggest a vacation destination for me? \"\n",
    "            \"I plan to have some time off in June this year.\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"last_message\"].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent should reference the user's love of mountains and their location in Florence, Italy, demonstrating that Mem0 retrieves different memories depending on the query context. A question about frameworks surfaces technical memories, while a question about vacations surfaces lifestyle memories.\n",
    "\n",
    "Moreover, new messages are automatically passed to the memory store, so now it should also remember when the user plans their vacation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User plans to have time off in June 2026\n",
      "User is a software engineer and CEO of a startup with 10 people, building a platform for small businesses to manage customers and sales, and prefers Python and the Haystack framework for NLP and LLM agent projects.\n",
      "User lives in Florence, Italy, and loves mountains.\n"
     ]
    }
   ],
   "source": [
    "results = memory_store.search_memories(\n",
    "    query=\"What are the vacation plans?\",\n",
    "    user_id=\"agent_example\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for message in results:\n",
    "    print(message.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs. Retrieval\n",
    "\n",
    "If you have used Haystack for RAG, you might wonder how a memory store differs from a document retriever. While both supply additional context to the LLM, they serve different purposes:\n",
    "\n",
    "- **What is stored** - A retriever pulls chunks from a knowledge base built on top of your documents. A memory store holds distilled facts learned from conversations, like user preferences, past decisions, stated goals, but not raw documents.\n",
    "- **Who it belongs to** - Retrieved documents are typically shared across all users. Memories are scoped to a specific user, agent, or session, enabling personalization.\n",
    "- **How it evolves** - A document store changes only when you explicitly index new content. A memory store evolves automatically as the agent converses: new facts are extracted, conflicting ones are updated, and low-relevance memories decay over time.\n",
    "\n",
    "In practice, the two are complementary. A Haystack Agent can use a retriever to answer factual questions from your knowledge base while using a memory store to remember who it is talking to and what they care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Since memories are stored in your Mem0 account, let's clean up the memories we created in this notebook to avoid leaving orphaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store.delete_all_memories(user_id=\"agent_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
