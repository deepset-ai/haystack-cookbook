{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mem0 Memory Store with Haystack Agents\n",
    "\n",
    "[Mem0](https://mem0.ai/) is a managed memory layer for AI agents. Instead of passing entire conversation histories to an LLM on every turn, Mem0 intelligently extracts and compresses key facts from conversations into optimized memory representations. It handles the underlying infrastructure - vector store, graph services, and reranking - so you can focus on building your agent.\n",
    "\n",
    "At a high level, Mem0 manages a cycle of **extraction**, **consolidation**, and **retrieval**. When new messages arrive, relevant facts are identified and stored. Over time, memories are merged, updated, or allowed to fade if they lose relevance. When the agent later needs context, Mem0 surfaces only the memories most relevant to the current query - keeping token usage and latency low.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Set up a `Mem0MemoryStore` and add memories about a user\n",
    "2. Inspect what Mem0 actually stored\n",
    "3. Create a Haystack Agent that uses the memory store\n",
    "4. Ask the Agent personalized questions and see how it leverages stored memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The Mem0 integration with Haystack lives in the [`haystack-experimental`](https://github.com/deepset-ai/haystack-experimental) package and is currently **experimental**. The API may change in future releases. For the latest status, check the [haystack-experimental repository](https://github.com/deepset-ai/haystack-experimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "We need `haystack-ai` for the core framework, `haystack-experimental` for the Mem0 memory store integration and the experimental Agent, and `mem0ai` as the underlying Mem0 client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (2.23.0)\n",
      "Requirement already satisfied: haystack-experimental in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: mem0ai in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (1.0.3)\n",
      "Requirement already satisfied: docstring-parser in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (0.17.0)\n",
      "Requirement already satisfied: filetype in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (4.26.0)\n",
      "Requirement already satisfied: lazy-imports in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: more-itertools in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (10.8.0)\n",
      "Requirement already satisfied: networkx in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (3.6.1)\n",
      "Requirement already satisfied: numpy in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (2.4.2)\n",
      "Requirement already satisfied: openai>=1.99.2 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (2.20.0)\n",
      "Requirement already satisfied: posthog!=3.12.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (7.8.6)\n",
      "Requirement already satisfied: pydantic in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (2.12.5)\n",
      "Requirement already satisfied: python-dateutil in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (6.0.3)\n",
      "Requirement already satisfied: requests in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (9.1.4)\n",
      "Requirement already satisfied: tqdm in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from haystack-ai) (4.15.0)\n",
      "Requirement already satisfied: protobuf<6.0.0,>=5.29.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mem0ai) (5.29.6)\n",
      "Requirement already satisfied: pytz>=2024.1 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mem0ai) (2025.2)\n",
      "Requirement already satisfied: qdrant-client>=1.9.1 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mem0ai) (1.16.2)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.31 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mem0ai) (2.0.46)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai) (0.13.0)\n",
      "Requirement already satisfied: sniffio in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic->haystack-ai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic->haystack-ai) (0.4.2)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from qdrant-client>=1.9.1->mem0ai) (1.78.0)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from qdrant-client>=1.9.1->mem0ai) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from qdrant-client>=1.9.1->mem0ai) (2.6.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests->haystack-ai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests->haystack-ai) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests->haystack-ai) (2026.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jinja2->haystack-ai) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema->haystack-ai) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema->haystack-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema->haystack-ai) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema->haystack-ai) (0.30.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.99.2->haystack-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.99.2->haystack-ai) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/kacper.lukawski/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.9.1->mem0ai) (4.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install haystack-ai haystack-experimental mem0ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys\n",
    "\n",
    "This notebook requires two API keys:\n",
    "- **`OPENAI_API_KEY`**: Used by the [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) to power the Agent's LLM.\n",
    "- **`MEM0_API_KEY`**: Used by `Mem0MemoryStore` to connect to the [Mem0 Platform](https://app.mem0.ai/). You can get a free API key by signing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if not os.environ.get(\"MEM0_API_KEY\"):\n",
    "    os.environ[\"MEM0_API_KEY\"] = getpass(\"Enter your Mem0 API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Mem0 Memory Store\n",
    "\n",
    "The `Mem0MemoryStore` connects to the [Mem0 Platform](https://app.mem0.ai/) using the `MEM0_API_KEY` environment variable. Each memory is associated with a `user_id`, which allows Mem0 to maintain separate memory spaces for different users.\n",
    "\n",
    "We will add several facts about a user - their preferences, background, and work context. Mem0 will extract the key facts from these messages and store them as structured memories. When the Agent later queries the memory store, only the most relevant memories will be retrieved rather than replaying the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_experimental.memory_stores.mem0 import Mem0MemoryStore\n",
    "\n",
    "memory_store = Mem0MemoryStore()\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_user(\"I like to listen to Russian pop music\"),\n",
    "    ChatMessage.from_user(\"I liked cold spanish latte with oat milk\"),\n",
    "    ChatMessage.from_user(\"I live in Florence Italy and I love mountains\"),\n",
    "    ChatMessage.from_user(\n",
    "        \"I am a software engineer and I like building application in python. \"\n",
    "        \"Most of my projects are related to NLP and LLM agents. \"\n",
    "        \"I find it easier to use Haystack framework to build my projects.\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"I work in a startup and I am the CEO of the company. \"\n",
    "        \"I have a team of 10 people and we are building a platform \"\n",
    "        \"for small businesses to manage their customers and sales.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory_store.add_memories(user_id=\"agent_example\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Stored Memories\n",
    "\n",
    "Before connecting the memory store to an Agent, let's see what Mem0 actually stored. The `search_memories` method lets us query the memory store and see which memories are retrieved for a given query. This is useful for understanding how Mem0 extracts and condenses information from raw messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is a software engineer who builds Python applications, focuses on NLP and LLM agents, and prefers using the Haystack framework\n",
      "User works at a startup as CEO, leads a team of 10, and is building a platform for small businesses to manage customers and sales\n",
      "User likes listening to Russian pop music\n"
     ]
    }
   ],
   "source": [
    "results = memory_store.search_memories(\n",
    "    query=\"What programming tools does this person use?\",\n",
    "    user_id=\"agent_example\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for message in results:\n",
    "    print(message.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Mem0 has rephrased the original messages into facts. Instead of storing the full sentences, it extracts the key information and returns the memories most relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent with Memory\n",
    "\n",
    "Now we create a Haystack Agent that uses an `OpenAIChatGenerator` as its LLM and the `Mem0MemoryStore` as its memory.\n",
    "\n",
    "When the Agent runs, it will:\n",
    "1. Search the memory store using the user's message as a query\n",
    "2. Inject relevant memories into the conversation as context\n",
    "3. Generate a response that takes those memories into account\n",
    "4. Save new messages back to the memory store automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No tools provided to the Agent. The Agent will behave like a ChatGenerator and only return text responses. To enable tool usage, pass tools directly to the Agent, not to the chat_generator.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack_experimental.components.agents.agent import Agent\n",
    "\n",
    "chat_generator = OpenAIChatGenerator()\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=chat_generator,\n",
    "    memory_store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Personalized Question\n",
    "\n",
    "Let's ask the Agent a question that it can only answer well if it remembers the user's preferences. Based on the stored memories, the Agent knows the user is a Python developer who uses Haystack and works on NLP/LLM projects - so it should tailor its recommendation accordingly.\n",
    "\n",
    "Note the `memory_store_kwargs` parameter: this is where we pass the `user_id` so the Agent knows which user's memories to search. Behind the scenes, the Agent will query Mem0 with the user's message, retrieve relevant memories, inject them into the prompt as additional context, and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: Given your background (Python + NLP/LLM focus) and your stated preference for Haystack, Haystack is a very good primary choice for building an AI travel agent — supplemented where useful with a few complementary pieces (vector DB, lightweight dialog manager or Rasa for slot filling, and a tool-execution layer such as a small custom agent or LangChain tool wrappers).\n",
      "\n",
      "Why Haystack fits you\n",
      "- Python first and NLP/LLM oriented — matches your skillset.\n",
      "- Strong support for retrieval-augmented generation (RAG), document stores, retrievers and readers/generators (good for FAQs, travel guides, policy documents).\n",
      "- Pipelines make it easy to compose retrieval, re-ranking, generation and post-processing.\n",
      "- Integrations with common vector stores (FAISS, Milvus, Weaviate), Hugging Face/transformer models and OpenAI/Cohere, so you can pick hosted or local LLMs.\n",
      "- Good for provenance and confidence scores — important for bookings and customer trust.\n",
      "\n",
      "Suggested architecture and components\n",
      "- Ingestion & knowledge layer\n",
      "  - Sources: airline/hotel APIs, knowledge base pages (policies, destinations), saved itineraries, user preferences.\n",
      "  - Document store + vector index: Milvus or FAISS (Milvus if you want a production-ready cluster; FAISS for local prototyping).\n",
      "  - Metadata store for transactional data: PostgreSQL for bookings, Redis for sessions.\n",
      "- Retrieval + Response\n",
      "  - Haystack retriever -> reader/generator (HF LLM or OpenAI) for RAG responses (itineraries, Q&A).\n",
      "  - Use reranker or hybrid retriever (BM25 + dense embeddings) for accuracy.\n",
      "- Conversation state & slot management\n",
      "  - Simple state machine (FSM) for booking flows (dates, travelers, payment) or use Rasa if you want a full NLU + dialogue manager. Haystack handles retrieval/QA while Rasa or a small slot-filling module handles transactional dialogs.\n",
      "- Tool execution / actions\n",
      "  - A small tool layer that wraps external APIs (search flights, hold ticket, book hotel, payments). You can implement custom tool invocations from generated outputs or orchestrate deterministically from intent/slots.\n",
      "  - If you want LLM-driven tool selection, combine Haystack with LangChain tool wrappers or a small agent orchestrator that calls your tools.\n",
      "- Safety, provenance, UX\n",
      "  - Show provenance (source docs) for recommendations.\n",
      "  - Sanity checks for price/availability before confirming actions.\n",
      "  - GDPR/PII handling (you’re in EU) — ensure consent, data minimization, opt-outs.\n",
      "- Deployment\n",
      "  - FastAPI for endpoints, WebSocket for streaming responses.\n",
      "  - Docker + Kubernetes for scaling; Redis/Celery for async tasks (background booking, polling).\n",
      "  - Monitoring: logs, latency, conf intervals, user feedback loop.\n",
      "\n",
      "When to consider alternatives or complements\n",
      "- Pure agent/tool orchestration (LLM chooses and calls many tools automatically): LangChain has richer agent primitives. You can either:\n",
      "  - Use Haystack for RAG + LangChain agents for tool orchestration, or\n",
      "  - Build a small deterministic orchestrator for bookings and rely on Haystack for answers.\n",
      "- Conversational NLU + dialogue policies: Rasa is strong if you want a rule-based/NLU hybrid dialogue manager; otherwise a combination of Haystack + slot-filling code may suffice.\n",
      "\n",
      "Quick recommended starter stack (minimal viable)\n",
      "- Haystack for retrieval & RAG\n",
      "- Milvus (or FAISS) as vector store\n",
      "- Hugging Face hosted LLM or OpenAI for generator\n",
      "- FastAPI + Redis (session store) + PostgreSQL (bookings)\n",
      "- Small FSM module for booking flows; tool wrappers for flight/hotel APIs\n",
      "- Docker for dev, migrate to k8s for scale\n",
      "\n",
      "Next steps I can help with\n",
      "- Sketch a concrete Haystack pipeline for search + booking.\n",
      "- Example code snippets: ingestion -> vectorization -> conversational pipeline.\n",
      "- Evaluate model options (local LLMs vs OpenAI) given latency/cost/privacy.\n",
      "\n",
      "Which of these do you want to start with — a pipeline sketch, concrete code template, or a discussion of model hosting/privacy tradeoffs?\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Based on what you know about me, which framework should I use to design an AI travel agent?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Follow-Up Question\n",
    "\n",
    "Let's try a different kind of question. This time about personal preferences rather than technical skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice — since you live in Florence and love mountains, I’d recommend the Dolomites as a top choice.\n",
      "\n",
      "Why the Dolomites\n",
      "- Dramatic, photogenic peaks (UNESCO), great hiking and via ferrata routes, superb cable-car access and alpine huts (rifugi).\n",
      "- Variety: challenging multi-day treks, short panoramic hikes (Seceda, Tre Cime di Lavaredo), lakes (Lake Braies), mountain biking and climbing.\n",
      "- Food: hearty Tyrolean/Alpine dishes (speck, canederli, polenta) and cozy rifugi or spa hotels for recovery.\n",
      "\n",
      "Practical notes from Florence\n",
      "- Drive: ~3–4 hours to many Dolomite towns (Ortisei, Val Gardena, Cortina d’Ampezzo area).\n",
      "- Train+bus: possible but slower (train to Bolzano/Verona then regional connections).\n",
      "- Best season: June–September for hiking; December–March for skiing.\n",
      "\n",
      "Sample 4-day itinerary\n",
      "- Day 1: Drive to Val Gardena/Ortisei, short acclimatizing hike, overnight in a guesthouse or rifugio.\n",
      "- Day 2: Seceda or Alpe di Siusi cable-car day, panoramic photos and dinner at a mountain hut.\n",
      "- Day 3: Day trip to Tre Cime di Lavaredo and Lake Braies (early start), sunset views.\n",
      "- Day 4: Relaxing morning (spa or slow walk), drive back to Florence.\n",
      "\n",
      "Two alternatives if you want options\n",
      "- Aosta Valley / Gran Paradiso (northwest Italy): classic alpine villages, serious mountaineering, good for wildlife and less touristy trails.\n",
      "- Swiss Alps (Zermatt / Jungfrau): for higher-challenge mountaineering, impeccable infrastructure, and glacier views — pricier but spectacular.\n",
      "\n",
      "Quick questions to refine the suggestion\n",
      "- When do you want to travel and for how long?\n",
      "- Do you want an active trip (hiking/climbing) or a more relaxed spa/food-focused getaway?\n",
      "- Solo or with others? Any budget preferences?\n",
      "\n",
      "Tell me a bit more and I’ll tailor a detailed plan (routes, places to stay, and things to pack).\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Can you suggest a vacation destination for me?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent should reference the user's love of mountains and their location in Florence, Italy, demonstrating that Mem0 retrieves different memories depending on the query context. A question about frameworks surfaces technical memories, while a question about vacations surfaces lifestyle memories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs. Retrieval\n",
    "\n",
    "If you have used Haystack for RAG, you might wonder how a memory store differs from a document retriever. While both supply additional context to the LLM, they serve different purposes:\n",
    "\n",
    "- **What is stored** - A retriever pulls chunks from a knowledge base built on top of your documents. A memory store holds distilled facts learned from conversations, like user preferences, past decisions, stated goals, but not raw documents.\n",
    "- **Who it belongs to** - Retrieved documents are typically shared across all users. Memories are scoped to a specific user, agent, or session, enabling personalization.\n",
    "- **How it evolves** - A document store changes only when you explicitly index new content. A memory store evolves automatically as the agent converses: new facts are extracted, conflicting ones are updated, and low-relevance memories decay over time.\n",
    "\n",
    "In practice, the two are complementary. A Haystack Agent can use a retriever to answer factual questions from your knowledge base while using a memory store to remember who it is talking to and what they care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Since memories are stored in your Mem0 account, let's clean up the memories we created in this notebook to avoid leaving orphaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store.delete_all_memories(user_id=\"agent_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
