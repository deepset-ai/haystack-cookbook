{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mem0 Memory Store with Haystack Agents\n",
    "\n",
    "[Mem0](https://mem0.ai/) is a managed memory layer for AI agents. Instead of passing entire conversation histories to an LLM on every turn, Mem0 intelligently extracts and compresses key facts from conversations into optimized memory representations.\n",
    "\n",
    "At a high level, Mem0 manages a cycle of **extraction**, **consolidation**, and **retrieval**. When new messages arrive, relevant facts are identified and stored. Over time, memories are merged, updated, or allowed to fade if they lose relevance. When the agent later needs context, Mem0 surfaces only the memories most relevant to the current query, helping to keep token usage and latency low.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Set up a `Mem0MemoryStore` and add memories about a user\n",
    "2. Inspect what Mem0 actually stored\n",
    "3. Create a Haystack Agent that uses the memory store\n",
    "4. Ask the Agent personalized questions and see how it leverages stored memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The Mem0 integration with Haystack lives in the [`haystack-experimental`](https://github.com/deepset-ai/haystack-experimental) package and is currently **experimental**. The API may change in future releases. For the latest status, check the [haystack-experimental repository](https://github.com/deepset-ai/haystack-experimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "We need `haystack-ai` for the core framework, `haystack-experimental` for the Mem0 memory store integration and the experimental Agent, and `mem0ai` as the underlying Mem0 client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install haystack-ai haystack-experimental mem0ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys\n",
    "\n",
    "This notebook requires two API keys:\n",
    "- **`OPENAI_API_KEY`**: Used by the [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) to power the Agent's LLM.\n",
    "- **`MEM0_API_KEY`**: Used by `Mem0MemoryStore` to connect to the [Mem0 Platform](https://app.mem0.ai/). You can get a free API key by signing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if not os.environ.get(\"MEM0_API_KEY\"):\n",
    "    os.environ[\"MEM0_API_KEY\"] = getpass(\"Enter your Mem0 API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Mem0 Memory Store\n",
    "\n",
    "The `Mem0MemoryStore` connects to the [Mem0 Platform](https://app.mem0.ai/) using the `MEM0_API_KEY` environment variable. Each memory is associated with a `user_id`, which allows Mem0 to maintain separate memory spaces for different users.\n",
    "\n",
    "We will add several facts about a user - their preferences, background, and work context. Mem0 will extract the key facts from these messages and store them as structured memories. When the Agent later queries the memory store, only the most relevant memories will be retrieved rather than replaying the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'memory_id': '38b1d9b5-76f5-4828-ac81-7678ecc5d218',\n",
       "  'memory': 'User likes listening to Russian pop music.'},\n",
       " {'memory_id': '0b579981-fad1-4e34-82ac-ad3b5860adac',\n",
       "  'memory': 'User enjoys a cold Spanish latte with oat milk.'},\n",
       " {'memory_id': '14d1927a-9360-48c1-87be-e981fd2d9c97',\n",
       "  'memory': 'User lives in Florence, Italy, and loves mountains.'},\n",
       " {'memory_id': '0b8d9208-6186-45d9-8fef-4f8c55edfc82',\n",
       "  'memory': 'User is a software engineer and CEO of a startup with 10 people, building a platform for small businesses to manage customers and sales, and focuses on Python NLP and LLM agent projects using the Haystack framework.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_experimental.memory_stores.mem0 import Mem0MemoryStore\n",
    "\n",
    "memory_store = Mem0MemoryStore()\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_user(\"I like to listen to Russian pop music\"),\n",
    "    ChatMessage.from_user(\"I liked cold spanish latte with oat milk\"),\n",
    "    ChatMessage.from_user(\"I live in Florence Italy and I love mountains\"),\n",
    "    ChatMessage.from_user(\n",
    "        \"I am a software engineer and I like building application in python. \"\n",
    "        \"Most of my projects are related to NLP and LLM agents. \"\n",
    "        \"I find it easier to use Haystack framework to build my projects.\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"I work in a startup and I am the CEO of the company. \"\n",
    "        \"I have a team of 10 people and we are building a platform \"\n",
    "        \"for small businesses to manage their customers and sales.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory_store.add_memories(user_id=\"agent_example\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Stored Memories\n",
    "\n",
    "Before connecting the memory store to an Agent, let's see what Mem0 actually stored. The `search_memories` method lets us query the memory store and see which memories are retrieved for a given query. This is useful for understanding how Mem0 extracts and condenses information from raw messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is a software engineer and CEO of a startup with 10 people, building a platform for small businesses to manage customers and sales, and focuses on Python NLP and LLM agent projects using the Haystack framework.\n",
      "User likes listening to Russian pop music.\n",
      "User enjoys a cold Spanish latte with oat milk.\n"
     ]
    }
   ],
   "source": [
    "results = memory_store.search_memories(\n",
    "    query=\"What programming tools does this person use?\",\n",
    "    user_id=\"agent_example\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for message in results:\n",
    "    print(message.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Mem0 has rephrased the original messages into facts. Instead of storing the full sentences, it extracts the key information and returns the memories most relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent with Memory\n",
    "\n",
    "Now we create a Haystack Agent that uses an `OpenAIChatGenerator` as its LLM and the `Mem0MemoryStore` as its memory.\n",
    "\n",
    "When the Agent runs, it will:\n",
    "1. Search the memory store using the user's message as a query\n",
    "2. Inject relevant memories into the conversation as context\n",
    "3. Generate a response that takes those memories into account\n",
    "4. Save new messages back to the memory store automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No tools provided to the Agent. The Agent will behave like a ChatGenerator and only return text responses. To enable tool usage, pass tools directly to the Agent, not to the chat_generator.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack_experimental.components.agents.agent import Agent\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-5-mini\")\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=chat_generator,\n",
    "    memory_store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Personalized Question\n",
    "\n",
    "Let's ask the Agent a question that it can only answer well if it remembers the user's preferences. Based on the stored memories, the Agent knows the user is a Python developer who uses Haystack and works on NLP/LLM projects - so it should tailor its recommendation accordingly.\n",
    "\n",
    "Note the `memory_store_kwargs` parameter: this is where we pass the `user_id` so the Agent knows which user's memories to search. Behind the scenes, the Agent will query Mem0 with the user's message, retrieve relevant memories, inject them into the prompt as additional context, and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer — for a Python-first, production-ready AI travel agent I’d build around a combination of: Rasa (dialogue & NLU), a small custom orchestration layer (FastAPI) that calls LLM APIs for generative tasks, and a vector store (pgvector, Pinecone or Milvus) + your transactional DB for context and personalization. That gives you a Python-native, controllable architecture that fits a startup product and reuses your existing platform data.\n",
      "\n",
      "Why this fits you\n",
      "- You’re a Python engineer/CEO building a product for SMBs: Rasa and FastAPI are Python-first and easy to operate in-house.  \n",
      "- You already run a platform for customers/sales — you can reuse your CRM data to personalize itineraries and user profiles.  \n",
      "- The stack is modular: you control dialogue state and business logic (important for bookings, payments, SLA), while still using LLMs where they add most value (natural language understanding, generation, suggestions).\n",
      "\n",
      "Recommended stack (concise)\n",
      "- Dialog / NLU: Rasa (intent/entity extraction, dialogue policies, custom actions).  \n",
      "- Orchestration API: FastAPI (exposes endpoints, coordinates components, holds business logic).  \n",
      "- LLMs: OpenAI / Anthropic / local transformer endpoints for generation and complex Q&A (use function-calling / structured output to drive actions).  \n",
      "- Embeddings / similarity search: pgvector in Postgres (simple, cheap) or Pinecone / Milvus for scale.  \n",
      "- Persistent data: PostgreSQL for bookings, user profiles; Redis for session/state caching.  \n",
      "- Integrations: Amadeus / Skyscanner / Booking APIs for inventory; Stripe for payments; Google Maps for maps/routing.  \n",
      "- NLU + tooling: spaCy for NER/text pre-processing; Schema-driven validation (pydantic) for action inputs/outputs.  \n",
      "- Async tasks & orchestration: Celery or Dask for background jobs; Kubernetes + Docker for deployment.  \n",
      "- Observability & safety: Sentry, Prometheus/Grafana, logging + human-in-the-loop escalation for risky actions.\n",
      "\n",
      "How this architecture handles travel-agent requirements\n",
      "- Deterministic flows (booking, cancellations, payments): handled by Rasa policies + your action server (FastAPI).  \n",
      "- Open-ended conversation and rich responses (suggestions, itinerary prose): LLM generates text; restrict via templates and function-calls for safety.  \n",
      "- Knowledge and context (policies, local offers, FAQs): store documents/FAQs as embeddings and run similarity search; combine retrieval results with LLM prompts.  \n",
      "- Personalization: query your CRM for user preferences, past trips, loyalty status and feed as structured context to dialogue/actions.  \n",
      "- Compliance and privacy: keep PII in your DB, redact before sending to external LLMs, support opt-outs.\n",
      "\n",
      "MVP plan (6 steps)\n",
      "1. Define core flows: search flights, book flight/hotel, modify/cancel, itinerary summary, simple Q&A.  \n",
      "2. Implement intents/entities in Rasa and simple dialogue stories for each flow.  \n",
      "3. Build a FastAPI action server that executes Rasa actions, integrates with booking APIs, and calls LLMs for free-text generation where needed.  \n",
      "4. Add an embeddings index (pgvector for MVP) for FAQs, policies, and supplier docs.  \n",
      "5. Wire up payments, calendar invites, and email/SMS notifications.  \n",
      "6. Add monitoring, rollout a small beta, collect logs and refine policies and LLM prompts.\n",
      "\n",
      "Practical tips & pitfalls\n",
      "- Keep business logic and money flows out of black-box LLM decisions — LLMs should suggest text or structured payloads, not execute bookings directly without validation.  \n",
      "- Use strict schemas and function-style calls for any LLM output that triggers actions.  \n",
      "- Start with pgvector for cost-effective similarity search; move to a managed vector DB if you need throughput or multi-region replication.  \n",
      "- Design for explicable failures and clear fallbacks (human escalation, “I can’t do that” responses).  \n",
      "- Rate-limit and cache third-party API calls (supplier APIs can be slow or rate-limited).\n",
      "\n",
      "If you want, I can:\n",
      "- Sketch a concrete data model and Rasa intent/spec for the travel flows you care about.  \n",
      "- Draft sample FastAPI endpoints and example function-call schemas for the LLM integrations.\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Based on what you know about me, which framework should I use to design an AI travel agent?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"last_message\"].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Follow-Up Question\n",
    "\n",
    "Let's try a different kind of question. This time about personal preferences rather than technical skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice — since you live in Florence and love mountains, here are a few vacation ideas that fit that profile (and are all doable as a long weekend or week trip).\n",
      "\n",
      "1) Dolomites (Cortina d’Ampezzo / Alta Badia / Val Gardena)\n",
      "- Why: Dramatic jagged peaks, world-class hiking, via ferrata, mountain lakes (Tre Cime, Lago di Braies). Stunning views and great alpine food.\n",
      "- Travel: ~3–4 hr drive from Florence (public transport possible but slower).\n",
      "- Good for: active hiking, photography, a luxury or rustic mountain stay. Cortina has nicer cafés and shops if you want proper coffee + oat milk.\n",
      "- Tip: Rent a car for flexibility; book rifugios in high season.\n",
      "\n",
      "2) Aosta Valley / Courmayeur (Mont Blanc area)\n",
      "- Why: Big alpine vibe with Mont Blanc views, glacier cable cars, thermal spas nearby, quieter than some Dolomite spots.\n",
      "- Travel: ~3.5–4 hr drive (train + bus options).\n",
      "- Good for: combining serious mountain time with relaxation (spas), or a more secluded retreat. Great if you want alpine culture with French/Italian cuisine.\n",
      "- Tip: Consider a stay in Courmayeur or nearby Villages for easy access to cable cars and good coffee spots.\n",
      "\n",
      "3) Swiss Alps — Zermatt / Verbier\n",
      "- Why: Iconic Matterhorn scenery (Zermatt), excellent lifts, hiking, and top-tier mountain hospitality. Very polished infrastructure and cafés.\n",
      "- Travel: ~4–5 hr by train/car (a bit pricier, but very scenic).\n",
      "- Good for: a premium, postcard mountain experience; excellent for a stress-free stay with reliable services and well-connected transport.\n",
      "- Tip: Zermatt is car-free; take the train and enjoy the town.\n",
      "\n",
      "If you want something for a team offsite (you’re a CEO of a small startup): consider booking a chalet or mountain lodge close to Bolzano or Alta Badia — beautiful meeting spaces, hiking/teambuilding nearby, and good connectivity for a few focused work sessions and lots of outdoor time.\n",
      "\n",
      "Quick questions to narrow it down:\n",
      "- When are you planning to go (season)?\n",
      "- Solo trip or team/friends?\n",
      "- Prefer hiking/active trips or spa/relaxation?\n",
      "- Budget: modest vs luxury?\n",
      "\n",
      "Tell me a couple of answers and I’ll give a tailored itinerary or specific places to stay and eat.\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Can you suggest a vacation destination for me?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"last_message\"].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent should reference the user's love of mountains and their location in Florence, Italy, demonstrating that Mem0 retrieves different memories depending on the query context. A question about frameworks surfaces technical memories, while a question about vacations surfaces lifestyle memories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs. Retrieval\n",
    "\n",
    "If you have used Haystack for RAG, you might wonder how a memory store differs from a document retriever. While both supply additional context to the LLM, they serve different purposes:\n",
    "\n",
    "- **What is stored** - A retriever pulls chunks from a knowledge base built on top of your documents. A memory store holds distilled facts learned from conversations, like user preferences, past decisions, stated goals, but not raw documents.\n",
    "- **Who it belongs to** - Retrieved documents are typically shared across all users. Memories are scoped to a specific user, agent, or session, enabling personalization.\n",
    "- **How it evolves** - A document store changes only when you explicitly index new content. A memory store evolves automatically as the agent converses: new facts are extracted, conflicting ones are updated, and low-relevance memories decay over time.\n",
    "\n",
    "In practice, the two are complementary. A Haystack Agent can use a retriever to answer factual questions from your knowledge base while using a memory store to remember who it is talking to and what they care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Since memories are stored in your Mem0 account, let's clean up the memories we created in this notebook to avoid leaving orphaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store.delete_all_memories(user_id=\"agent_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
