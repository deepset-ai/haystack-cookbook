{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mem0 Memory Store with Haystack Agents\n",
    "\n",
    "[Mem0](https://mem0.ai/) is a managed memory layer for AI agents. Instead of passing entire conversation histories to an LLM on every turn, Mem0 intelligently extracts and compresses key facts from conversations into optimized memory representations.\n",
    "\n",
    "At a high level, Mem0 manages a cycle of **extraction**, **consolidation**, and **retrieval**. When new messages arrive, relevant facts are identified and stored. Over time, memories are merged, updated, or allowed to fade if they lose relevance. When the agent later needs context, Mem0 surfaces only the memories most relevant to the current query, helping to keep token usage and latency low.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Set up a `Mem0MemoryStore` and add memories about a user\n",
    "2. Inspect what Mem0 actually stored\n",
    "3. Create a Haystack Agent that uses the memory store\n",
    "4. Ask the Agent personalized questions and see how it leverages stored memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The Mem0 integration with Haystack lives in the [`haystack-experimental`](https://github.com/deepset-ai/haystack-experimental) package and is currently **experimental**. The API may change in future releases. For the latest status, check the [haystack-experimental repository](https://github.com/deepset-ai/haystack-experimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "We need `haystack-ai` for the core framework, `haystack-experimental` for the Mem0 memory store integration and the experimental Agent, and `mem0ai` as the underlying Mem0 client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install haystack-ai haystack-experimental mem0ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys\n",
    "\n",
    "This notebook requires two API keys:\n",
    "- **`OPENAI_API_KEY`**: Used by the [`OpenAIChatGenerator`](https://docs.haystack.deepset.ai/docs/openaichatgenerator) to power the Agent's LLM.\n",
    "- **`MEM0_API_KEY`**: Used by `Mem0MemoryStore` to connect to the [Mem0 Platform](https://app.mem0.ai/). You can get a free API key by signing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if not os.environ.get(\"MEM0_API_KEY\"):\n",
    "    os.environ[\"MEM0_API_KEY\"] = getpass(\"Enter your Mem0 API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Mem0 Memory Store\n",
    "\n",
    "The `Mem0MemoryStore` connects to the [Mem0 Platform](https://app.mem0.ai/) using the `MEM0_API_KEY` environment variable. Each memory is associated with a `user_id`, which allows Mem0 to maintain separate memory spaces for different users.\n",
    "\n",
    "We will add several facts about a user - their preferences, background, and work context. Mem0 will extract the key facts from these messages and store them as structured memories. When the Agent later queries the memory store, only the most relevant memories will be retrieved rather than replaying the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_experimental.memory_stores.mem0 import Mem0MemoryStore\n",
    "\n",
    "memory_store = Mem0MemoryStore()\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_user(\"I like to listen to Russian pop music\"),\n",
    "    ChatMessage.from_user(\"I liked cold spanish latte with oat milk\"),\n",
    "    ChatMessage.from_user(\"I live in Florence Italy and I love mountains\"),\n",
    "    ChatMessage.from_user(\n",
    "        \"I am a software engineer and I like building application in python. \"\n",
    "        \"Most of my projects are related to NLP and LLM agents. \"\n",
    "        \"I find it easier to use Haystack framework to build my projects.\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"I work in a startup and I am the CEO of the company. \"\n",
    "        \"I have a team of 10 people and we are building a platform \"\n",
    "        \"for small businesses to manage their customers and sales.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory_store.add_memories(user_id=\"agent_example\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Stored Memories\n",
    "\n",
    "Before connecting the memory store to an Agent, let's see what Mem0 actually stored. The `search_memories` method lets us query the memory store and see which memories are retrieved for a given query. This is useful for understanding how Mem0 extracts and condenses information from raw messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = memory_store.search_memories(\n",
    "    query=\"What programming tools does this person use?\",\n",
    "    user_id=\"agent_example\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "for message in results:\n",
    "    print(message.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Mem0 has rephrased the original messages into facts. Instead of storing the full sentences, it extracts the key information and returns the memories most relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent with Memory\n",
    "\n",
    "Now we create a Haystack Agent that uses an `OpenAIChatGenerator` as its LLM and the `Mem0MemoryStore` as its memory.\n",
    "\n",
    "When the Agent runs, it will:\n",
    "1. Search the memory store using the user's message as a query\n",
    "2. Inject relevant memories into the conversation as context\n",
    "3. Generate a response that takes those memories into account\n",
    "4. Save new messages back to the memory store automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack_experimental.components.agents.agent import Agent\n",
    "\n",
    "chat_generator = OpenAIChatGenerator()\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=chat_generator,\n",
    "    memory_store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Personalized Question\n",
    "\n",
    "Let's ask the Agent a question that it can only answer well if it remembers the user's preferences. Based on the stored memories, the Agent knows the user is a Python developer who uses Haystack and works on NLP/LLM projects - so it should tailor its recommendation accordingly.\n",
    "\n",
    "Note the `memory_store_kwargs` parameter: this is where we pass the `user_id` so the Agent knows which user's memories to search. Behind the scenes, the Agent will query Mem0 with the user's message, retrieve relevant memories, inject them into the prompt as additional context, and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Based on what you know about me, which framework should I use to design an AI travel agent?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Follow-Up Question\n",
    "\n",
    "Let's try a different kind of question. This time about personal preferences rather than technical skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(\n",
    "    messages=[\n",
    "        ChatMessage.from_user(\n",
    "            \"Can you suggest a vacation destination for me?\"\n",
    "        )\n",
    "    ],\n",
    "    memory_store_kwargs={\"user_id\": \"agent_example\"},\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent should reference the user's love of mountains and their location in Florence, Italy, demonstrating that Mem0 retrieves different memories depending on the query context. A question about frameworks surfaces technical memories, while a question about vacations surfaces lifestyle memories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs. Retrieval\n",
    "\n",
    "If you have used Haystack for RAG, you might wonder how a memory store differs from a document retriever. While both supply additional context to the LLM, they serve different purposes:\n",
    "\n",
    "- **What is stored** - A retriever pulls chunks from a knowledge base built on top of your documents. A memory store holds distilled facts learned from conversations, like user preferences, past decisions, stated goals, but not raw documents.\n",
    "- **Who it belongs to** - Retrieved documents are typically shared across all users. Memories are scoped to a specific user, agent, or session, enabling personalization.\n",
    "- **How it evolves** - A document store changes only when you explicitly index new content. A memory store evolves automatically as the agent converses: new facts are extracted, conflicting ones are updated, and low-relevance memories decay over time.\n",
    "\n",
    "In practice, the two are complementary. A Haystack Agent can use a retriever to answer factual questions from your knowledge base while using a memory store to remember who it is talking to and what they care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Since memories are stored in your Mem0 account, let's clean up the memories we created in this notebook to avoid leaving orphaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store.delete_all_memories(user_id=\"agent_example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
